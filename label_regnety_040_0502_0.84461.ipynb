{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "import torch.cuda.amp as amp\n",
    "from adamp import AdamP, SGDP\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  file_name       class state            label  label2\n",
      "0           0  10000.png  transistor  good  transistor-good      72\n",
      "1           1  10001.png     capsule  good     capsule-good      15\n",
      "2           2  10002.png  transistor  good  transistor-good      72\n",
      "3           3  10003.png        wood  good        wood-good      76\n",
      "4           4  10004.png      bottle  good      bottle-good       3\n",
      "   index  file_name\n",
      "0      0  20000.png\n",
      "1      1  20001.png\n",
      "2      2  20002.png\n",
      "3      3  20003.png\n",
      "4      4  20004.png\n",
      "(6221, 6)\n",
      "(2154, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './open'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df_add2.csv'))\n",
    "# train_df = train_df.drop('index', axis=1)\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['transistor-good', 'capsule-good', 'wood-good', 'bottle-good',\n",
       "       'screw-good', 'cable-bent_wire', 'carpet-hole', 'hazelnut-good',\n",
       "       'pill-pill_type', 'cable-good', 'metal_nut-scratch', 'pill-good',\n",
       "       'screw-thread_side', 'zipper-fabric_border', 'leather-good',\n",
       "       'pill-scratch', 'toothbrush-good', 'hazelnut-crack',\n",
       "       'screw-manipulated_front', 'zipper-good', 'tile-good',\n",
       "       'carpet-good', 'metal_nut-good', 'bottle-contamination',\n",
       "       'grid-good', 'zipper-split_teeth', 'pill-crack', 'wood-combined',\n",
       "       'pill-color', 'screw-thread_top', 'cable-missing_cable',\n",
       "       'capsule-squeeze', 'zipper-rough', 'capsule-crack', 'capsule-poke',\n",
       "       'metal_nut-flip', 'carpet-metal_contamination', 'metal_nut-color',\n",
       "       'transistor-bent_lead', 'zipper-fabric_interior', 'leather-fold',\n",
       "       'tile-glue_strip', 'screw-scratch_neck', 'screw-scratch_head',\n",
       "       'hazelnut-cut', 'bottle-broken_large', 'bottle-broken_small',\n",
       "       'leather-cut', 'cable-cut_outer_insulation',\n",
       "       'zipper-squeezed_teeth', 'toothbrush-defective',\n",
       "       'cable-cut_inner_insulation', 'pill-contamination',\n",
       "       'cable-missing_wire', 'carpet-thread', 'grid-broken',\n",
       "       'pill-faulty_imprint', 'hazelnut-hole', 'leather-glue',\n",
       "       'leather-poke', 'transistor-damaged_case', 'wood-scratch',\n",
       "       'tile-gray_stroke', 'capsule-faulty_imprint', 'grid-glue',\n",
       "       'zipper-combined', 'carpet-color', 'grid-bent', 'pill-combined',\n",
       "       'hazelnut-print', 'cable-combined', 'capsule-scratch',\n",
       "       'metal_nut-bent', 'zipper-broken_teeth', 'tile-oil',\n",
       "       'transistor-misplaced', 'grid-thread', 'grid-metal_contamination',\n",
       "       'carpet-cut', 'wood-color', 'cable-cable_swap', 'tile-crack',\n",
       "       'leather-color', 'cable-poke_insulation', 'transistor-cut_lead',\n",
       "       'wood-hole', 'tile-rough', 'wood-liquid'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(img, angle):\n",
    "    angle = int(random.uniform(-angle, angle))\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h)) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import imutils\n",
    "# oslabel = list(train_df['label'].unique())\n",
    "\n",
    "\n",
    "# for label in tqdm(oslabel):\n",
    "#     if 'good' not in label:\n",
    "#         print(label)\n",
    "#         idx = 0\n",
    "#         one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n",
    "#         images_list = natsorted(one_sample['file_name'])\n",
    "#         print(images_list)\n",
    "#         for _, image_name in enumerate(images_list):\n",
    "#             image = np.array(Image.open(opj('./open/train_add/', image_name)).convert('RGB'))\n",
    "            \n",
    "#             aug_img = rotation(image, 30)  \n",
    "#             aug_img = cv2.resize(aug_img, dsize=(1024, 1024))\n",
    "#             aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n",
    "#             save_path = opj('./open/train_add', f'{label}_{idx}.png')\n",
    "#             save_name = f'{label}_{idx}.png'\n",
    "#             idx += 1\n",
    "#             cv2.imwrite(save_path, aug_img)\n",
    "#             train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist()\n",
    "\n",
    "# for label in tqdm(oslabel):\n",
    "#     if 'good' not in label:\n",
    "#         print(label)\n",
    "#         idx = 0\n",
    "#         one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n",
    "#         images_list = natsorted(one_sample['file_name'])\n",
    "#         print(images_list)\n",
    "#         for _, image_name in enumerate(images_list):\n",
    "#             image = np.array(Image.open(opj('./open/train_add/', image_name)).convert('RGB'))\n",
    "            \n",
    "#             aug_img = cv2.flip(image, 1)\n",
    "#             aug_img = cv2.resize(aug_img, dsize=(1024, 1024))\n",
    "#             aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n",
    "#             save_path = opj('./open/train_add', f'{label}_{idx}_flip.png')\n",
    "#             save_name = f'{label}_{idx}_flip.png'\n",
    "#             idx += 1\n",
    "#             cv2.imwrite(save_path, aug_img)\n",
    "#             train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist()    \n",
    "            \n",
    "   \n",
    "            \n",
    "# train_df.to_csv('./open/train_df_add.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = pd.read_csv(\"./open/train_df_add.csv\")\n",
    "\n",
    "# train_labels = train_y[\"label\"]\n",
    "\n",
    "# label_unique = sorted(np.unique(train_labels))\n",
    "# label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "# train_labels = [label_unique[k] for k in train_labels]\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['label2'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./open/train_df_add2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 2 and 3 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(train_df.label2.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./open',\n",
    "     'Kfold':5,\n",
    "     'model_path':'label_results/',\n",
    "     'image_type':'train_1024', \n",
    "     'class_num' : class_num,\n",
    "\n",
    "     # Model parameter settings\n",
    "     'model_name':'regnety_040',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':512,\n",
    "     'batch_size':16,\n",
    "     'epochs':100,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-4,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'Reduce',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-5,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience': 7,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':True,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "     \n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, p: float, degrees: int):\n",
    "        super(RandomRotation, self).__init__(degrees)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            fill = self.fill\n",
    "            if isinstance(img, Tensor):\n",
    "                if isinstance(fill, (int, float)):\n",
    "                    fill = [float(fill)] * F.get_image_num_channels(img)\n",
    "                else:\n",
    "                    fill = [float(f) for f in fill]\n",
    "            angle = self.get_params(self.degrees)\n",
    "\n",
    "            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.target = df['label2'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         image = cv2.imread(opj('./open/train_add/', self.img_path[idx])).astype(np.float32)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        \n",
    "#         target = self.target[idx]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "        \n",
    "        image = Image.open(opj('./open/train_add/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "#         augmentation = random.randint(0,2)\n",
    "#             if augmentation==1:\n",
    "#                 img = img[::-1].copy()\n",
    "#             elif augmentation==2:\n",
    "#                 img = img[:,::-1].copy()\n",
    "#         img = transforms.ToTensor()(img)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "#         image = cv2.imread(opj('./open/test/', self.img_path[idx])).astype(np.float32)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "        image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \n",
    "                                 pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    if ver == 2:\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.3),\n",
    "                transforms.RandomVerticalFlip(p=0.3),\n",
    "#                 transforms.RandomCrop(500),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "                transforms.RandomAffine((-20,20)),\n",
    "                RandomRotation(0.5, degrees=5),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model(args.model_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        num_head = self.model_ft.head.fc.in_features\n",
    "        self.model_ft.head.fc = nn.Linear(num_head, 88)\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model(args.model_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        num_head = self.model_ft.head.fc.in_features\n",
    "        self.model_ft.head.fc = nn.Linear(num_head, 88)\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight: tensor([ 9.7750,  8.8864,  8.8864,  1.8708, 13.9643, 16.2917, 16.2917, 13.9643,\n",
      "        19.5500,  1.7455, 16.2917, 19.5500, 19.5500,  8.1458,  8.8864,  1.7854,\n",
      "         8.8864,  8.1458,  9.7750,  9.7750, 10.8611,  1.3964, 10.8611, 10.8611,\n",
      "         9.7750, 16.2917, 16.2917, 16.2917,  1.4811, 16.2917, 16.2917, 10.8611,\n",
      "        10.8611,  1.0000, 10.8611, 10.8611,  9.7750,  9.7750, 10.8611,  9.7750,\n",
      "         1.5959, 10.8611,  7.5192,  8.8864,  8.1458,  1.7773,  8.1458,  7.5192,\n",
      "        10.8611,  8.8864,  7.5192,  9.7750,  1.4644, 19.5500,  8.1458,  1.2219,\n",
      "         8.1458,  8.1458,  7.5192,  8.1458,  8.1458, 10.8611, 10.8611,  1.7000,\n",
      "        12.2188, 10.8611, 12.2188,  6.5167,  6.5167, 19.5500, 19.5500, 19.5500,\n",
      "         1.8357, 19.5500, 24.4375, 16.2917,  1.5830, 19.5500, 19.5500,  8.8864,\n",
      "         9.7750, 12.2188, 10.8611, 12.2188,  1.6292, 10.8611, 10.8611, 12.2188],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # weighted crossentropy loss를 위한 weight 계산 함수\n",
    "# def get_class_weight():\n",
    "#     return 1 / train_df['label2'].value_counts().sort_index().values\n",
    "\n",
    "# class_weight = get_class_weight()\n",
    "\n",
    "\n",
    "class_num = train_df.groupby([\"label2\"])[\"label2\"].count().tolist()\n",
    "class_weight = torch.tensor(np.max(class_num) / class_num).to(\"cuda\", dtype=torch.float)\n",
    "print(f\"class_weight: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://dacon.io/competitions/official/235585/codeshare/1796\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train_df_add2.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "#         if args.image_type is not None:\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label2'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "#         self.criterion = FocalLoss()\n",
    "#         self.criterion = CutMixCrossEntropyLoss(True)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "#         self.optimizer = Lookahead(torch.optim.Adam(self.model.parameters(), lr=args.initial_lr), k=5, alpha=0.5)\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5, mode=\"max\", verbose=True)\n",
    "            \n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        \n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "    \n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                    \n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./open/sample_submission.csv')\n",
    "df_train = pd.read_csv('./open/train_df_add2.csv')\n",
    "df_test = pd.read_csv('./open/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_040', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def make_pseudo_df(train_df, test_df, ensemble, step, threshold = 0.9, z_sample = 500): \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    test_df_copy['disease'] = np.nan\n",
    "    test_df_copy['disease_code'] = ensemble.argmax(axis=1)\n",
    "    pseudo_test_df = test_df_copy.iloc[np.where(ensemble > threshold)[0]].reset_index(drop=True)\n",
    "    z_idx  = pseudo_test_df[pseudo_test_df['disease_code'] == 0].sample(n=z_sample, random_state=42).index.tolist()\n",
    "    ot_idx = pseudo_test_df[pseudo_test_df['disease_code'].isin([*range(1,8)])].index.tolist()\n",
    "    pseudo_test_df = pseudo_test_df.iloc[z_idx + ot_idx]\n",
    "\n",
    "    train_df_copy = train_df_copy.append(pseudo_test_df, ignore_index=True).reset_index(drop=True) # reset_index\n",
    "    # print(f'Make train_{step}step.csv')\n",
    "    train_df_copy.to_csv(f'../data/train_{step}step.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 15:32:39,159 INFO: {'exp_num': '0', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:4976\n",
      "Dataset size:1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 15:32:39,553 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 311/311 [02:25<00:00,  2.14it/s]\n",
      "2022-05-02 15:35:04,812 INFO: Epoch:[001/100]\n",
      "2022-05-02 15:35:04,813 INFO: Train Loss:4.522 | Acc:0.0163 | F1:0.0073\n",
      "2022-05-02 15:35:22,923 INFO: val Loss:4.207 | Acc:0.0948 | F1:0.0273\n",
      "2022-05-02 15:35:23,603 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 311/311 [02:22<00:00,  2.19it/s]\n",
      "2022-05-02 15:37:45,948 INFO: Epoch:[002/100]\n",
      "2022-05-02 15:37:45,949 INFO: Train Loss:4.138 | Acc:0.1469 | F1:0.0637\n",
      "2022-05-02 15:38:02,706 INFO: val Loss:3.349 | Acc:0.3631 | F1:0.1334\n",
      "2022-05-02 15:38:03,361 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 311/311 [02:20<00:00,  2.21it/s]\n",
      "2022-05-02 15:40:23,955 INFO: Epoch:[003/100]\n",
      "2022-05-02 15:40:23,956 INFO: Train Loss:3.509 | Acc:0.3515 | F1:0.1526\n",
      "2022-05-02 15:40:40,402 INFO: val Loss:2.559 | Acc:0.4651 | F1:0.1621\n",
      "2022-05-02 15:40:41,036 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 311/311 [02:21<00:00,  2.20it/s]\n",
      "2022-05-02 15:43:02,106 INFO: Epoch:[004/100]\n",
      "2022-05-02 15:43:02,107 INFO: Train Loss:2.887 | Acc:0.3531 | F1:0.1807\n",
      "2022-05-02 15:43:18,610 INFO: val Loss:2.064 | Acc:0.3807 | F1:0.1630\n",
      "2022-05-02 15:43:19,240 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 311/311 [02:18<00:00,  2.24it/s]\n",
      "2022-05-02 15:45:38,150 INFO: Epoch:[005/100]\n",
      "2022-05-02 15:45:38,151 INFO: Train Loss:2.510 | Acc:0.3314 | F1:0.2112\n",
      "2022-05-02 15:45:56,061 INFO: val Loss:1.830 | Acc:0.3791 | F1:0.1807\n",
      "2022-05-02 15:45:56,756 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 311/311 [02:19<00:00,  2.24it/s]\n",
      "2022-05-02 15:48:15,905 INFO: Epoch:[006/100]\n",
      "2022-05-02 15:48:15,906 INFO: Train Loss:2.317 | Acc:0.3736 | F1:0.2442\n",
      "2022-05-02 15:48:32,193 INFO: val Loss:1.634 | Acc:0.4859 | F1:0.2256\n",
      "2022-05-02 15:48:32,818 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 311/311 [02:16<00:00,  2.27it/s]\n",
      "2022-05-02 15:50:49,669 INFO: Epoch:[007/100]\n",
      "2022-05-02 15:50:49,670 INFO: Train Loss:2.129 | Acc:0.4381 | F1:0.3017\n",
      "2022-05-02 15:51:05,654 INFO: val Loss:1.475 | Acc:0.5695 | F1:0.3170\n",
      "2022-05-02 15:51:06,276 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 311/311 [02:56<00:00,  1.76it/s]\n",
      "2022-05-02 15:54:02,593 INFO: Epoch:[008/100]\n",
      "2022-05-02 15:54:02,594 INFO: Train Loss:1.994 | Acc:0.4825 | F1:0.3296\n",
      "2022-05-02 15:54:20,914 INFO: val Loss:1.322 | Acc:0.5671 | F1:0.3570\n",
      "2022-05-02 15:54:21,634 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 311/311 [03:17<00:00,  1.58it/s]\n",
      "2022-05-02 15:57:38,735 INFO: Epoch:[009/100]\n",
      "2022-05-02 15:57:38,736 INFO: Train Loss:1.812 | Acc:0.5354 | F1:0.3898\n",
      "2022-05-02 15:57:57,631 INFO: val Loss:1.192 | Acc:0.6811 | F1:0.4636\n",
      "2022-05-02 15:57:58,344 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 16:01:19,254 INFO: Epoch:[010/100]\n",
      "2022-05-02 16:01:19,255 INFO: Train Loss:1.705 | Acc:0.5846 | F1:0.4416\n",
      "2022-05-02 16:01:38,074 INFO: val Loss:1.042 | Acc:0.7398 | F1:0.5432\n",
      "2022-05-02 16:01:38,796 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 311/311 [03:15<00:00,  1.59it/s]\n",
      "2022-05-02 16:04:54,403 INFO: Epoch:[011/100]\n",
      "2022-05-02 16:04:54,404 INFO: Train Loss:1.535 | Acc:0.6308 | F1:0.5027\n",
      "2022-05-02 16:05:13,213 INFO: val Loss:0.980 | Acc:0.6932 | F1:0.5530\n",
      "2022-05-02 16:05:13,932 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 311/311 [03:02<00:00,  1.71it/s]\n",
      "2022-05-02 16:08:16,128 INFO: Epoch:[012/100]\n",
      "2022-05-02 16:08:16,129 INFO: Train Loss:1.407 | Acc:0.6555 | F1:0.5462\n",
      "2022-05-02 16:08:34,892 INFO: val Loss:0.886 | Acc:0.7631 | F1:0.6094\n",
      "2022-05-02 16:08:35,665 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 311/311 [03:15<00:00,  1.59it/s]\n",
      "2022-05-02 16:11:51,617 INFO: Epoch:[013/100]\n",
      "2022-05-02 16:11:51,618 INFO: Train Loss:1.283 | Acc:0.6915 | F1:0.5907\n",
      "2022-05-02 16:12:09,673 INFO: val Loss:0.752 | Acc:0.7759 | F1:0.6524\n",
      "2022-05-02 16:12:10,317 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 16:15:32,106 INFO: Epoch:[014/100]\n",
      "2022-05-02 16:15:32,107 INFO: Train Loss:1.159 | Acc:0.7277 | F1:0.6408\n",
      "2022-05-02 16:15:50,553 INFO: val Loss:0.636 | Acc:0.8120 | F1:0.7057\n",
      "2022-05-02 16:15:51,193 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 16:19:11,641 INFO: Epoch:[015/100]\n",
      "2022-05-02 16:19:11,642 INFO: Train Loss:1.073 | Acc:0.7404 | F1:0.6527\n",
      "2022-05-02 16:19:30,455 INFO: val Loss:0.583 | Acc:0.8562 | F1:0.7553\n",
      "2022-05-02 16:19:31,090 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 311/311 [04:43<00:00,  1.10it/s]\n",
      "2022-05-02 16:24:14,271 INFO: Epoch:[016/100]\n",
      "2022-05-02 16:24:14,272 INFO: Train Loss:0.993 | Acc:0.7717 | F1:0.6965\n",
      "2022-05-02 16:24:34,262 INFO: val Loss:0.588 | Acc:0.8618 | F1:0.7623\n",
      "100%|██████████| 311/311 [03:24<00:00,  1.52it/s]\n",
      "2022-05-02 16:27:59,140 INFO: Epoch:[017/100]\n",
      "2022-05-02 16:27:59,141 INFO: Train Loss:0.879 | Acc:0.7888 | F1:0.7178\n",
      "2022-05-02 16:28:19,874 INFO: val Loss:0.479 | Acc:0.8884 | F1:0.8079\n",
      "2022-05-02 16:28:20,720 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 16:31:41,336 INFO: Epoch:[018/100]\n",
      "2022-05-02 16:31:41,337 INFO: Train Loss:0.818 | Acc:0.8191 | F1:0.7566\n",
      "2022-05-02 16:32:02,093 INFO: val Loss:0.413 | Acc:0.8394 | F1:0.8226\n",
      "2022-05-02 16:32:02,844 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 16:35:23,809 INFO: Epoch:[019/100]\n",
      "2022-05-02 16:35:23,810 INFO: Train Loss:0.691 | Acc:0.8374 | F1:0.7924\n",
      "2022-05-02 16:35:44,749 INFO: val Loss:0.361 | Acc:0.9044 | F1:0.8549\n",
      "2022-05-02 16:35:45,574 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 311/311 [03:18<00:00,  1.57it/s]\n",
      "2022-05-02 16:39:03,876 INFO: Epoch:[020/100]\n",
      "2022-05-02 16:39:03,877 INFO: Train Loss:0.636 | Acc:0.8615 | F1:0.8123\n",
      "2022-05-02 16:39:24,659 INFO: val Loss:0.348 | Acc:0.8763 | F1:0.8559\n",
      "2022-05-02 16:39:25,452 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 16:42:45,743 INFO: Epoch:[021/100]\n",
      "2022-05-02 16:42:45,744 INFO: Train Loss:0.589 | Acc:0.8637 | F1:0.8244\n",
      "2022-05-02 16:43:05,461 INFO: val Loss:0.237 | Acc:0.8964 | F1:0.8974\n",
      "2022-05-02 16:43:06,230 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 311/311 [03:15<00:00,  1.59it/s]\n",
      "2022-05-02 16:46:22,110 INFO: Epoch:[022/100]\n",
      "2022-05-02 16:46:22,111 INFO: Train Loss:0.517 | Acc:0.8873 | F1:0.8515\n",
      "2022-05-02 16:46:41,797 INFO: val Loss:0.215 | Acc:0.9454 | F1:0.9144\n",
      "2022-05-02 16:46:42,549 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 311/311 [03:23<00:00,  1.52it/s]\n",
      "2022-05-02 16:50:06,543 INFO: Epoch:[023/100]\n",
      "2022-05-02 16:50:06,544 INFO: Train Loss:0.469 | Acc:0.8889 | F1:0.8592\n",
      "2022-05-02 16:50:27,340 INFO: val Loss:0.254 | Acc:0.9165 | F1:0.8870\n",
      "100%|██████████| 311/311 [03:24<00:00,  1.52it/s]\n",
      "2022-05-02 16:53:51,466 INFO: Epoch:[024/100]\n",
      "2022-05-02 16:53:51,468 INFO: Train Loss:0.438 | Acc:0.8925 | F1:0.8703\n",
      "2022-05-02 16:54:12,915 INFO: val Loss:0.182 | Acc:0.9566 | F1:0.9332\n",
      "2022-05-02 16:54:13,709 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 16:57:34,937 INFO: Epoch:[025/100]\n",
      "2022-05-02 16:57:34,938 INFO: Train Loss:0.412 | Acc:0.9059 | F1:0.8807\n",
      "2022-05-02 16:57:55,477 INFO: val Loss:0.194 | Acc:0.9157 | F1:0.9087\n",
      "100%|██████████| 311/311 [03:18<00:00,  1.56it/s]\n",
      "2022-05-02 17:01:14,354 INFO: Epoch:[026/100]\n",
      "2022-05-02 17:01:14,355 INFO: Train Loss:0.375 | Acc:0.9138 | F1:0.8879\n",
      "2022-05-02 17:01:36,186 INFO: val Loss:0.190 | Acc:0.9373 | F1:0.9172\n",
      "100%|██████████| 311/311 [03:14<00:00,  1.60it/s]\n",
      "2022-05-02 17:04:50,671 INFO: Epoch:[027/100]\n",
      "2022-05-02 17:04:50,672 INFO: Train Loss:0.362 | Acc:0.9080 | F1:0.8874\n",
      "2022-05-02 17:05:12,017 INFO: val Loss:0.124 | Acc:0.9430 | F1:0.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 17:05:12,752 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 17:08:32,906 INFO: Epoch:[028/100]\n",
      "2022-05-02 17:08:32,907 INFO: Train Loss:0.332 | Acc:0.9186 | F1:0.9049\n",
      "2022-05-02 17:08:52,677 INFO: val Loss:0.270 | Acc:0.8627 | F1:0.8700\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 17:12:12,409 INFO: Epoch:[029/100]\n",
      "2022-05-02 17:12:12,410 INFO: Train Loss:0.316 | Acc:0.9118 | F1:0.8966\n",
      "2022-05-02 17:12:33,273 INFO: val Loss:0.123 | Acc:0.9502 | F1:0.9344\n",
      "2022-05-02 17:12:34,065 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 17:15:53,535 INFO: Epoch:[030/100]\n",
      "2022-05-02 17:15:53,536 INFO: Train Loss:0.288 | Acc:0.9244 | F1:0.9061\n",
      "2022-05-02 17:16:13,671 INFO: val Loss:0.170 | Acc:0.9703 | F1:0.9561\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 17:19:33,074 INFO: Epoch:[031/100]\n",
      "2022-05-02 17:19:33,075 INFO: Train Loss:0.264 | Acc:0.9319 | F1:0.9216\n",
      "2022-05-02 17:19:53,270 INFO: val Loss:0.112 | Acc:0.9518 | F1:0.9514\n",
      "2022-05-02 17:19:54,067 INFO: -----------------SAVE:31epoch----------------\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 17:23:15,443 INFO: Epoch:[032/100]\n",
      "2022-05-02 17:23:15,444 INFO: Train Loss:0.259 | Acc:0.9373 | F1:0.9188\n",
      "2022-05-02 17:23:36,464 INFO: val Loss:0.126 | Acc:0.9655 | F1:0.9563\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 17:26:58,165 INFO: Epoch:[033/100]\n",
      "2022-05-02 17:26:58,167 INFO: Train Loss:0.263 | Acc:0.9329 | F1:0.9141\n",
      "2022-05-02 17:27:18,045 INFO: val Loss:0.087 | Acc:0.9703 | F1:0.9550\n",
      "2022-05-02 17:27:18,791 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 311/311 [03:14<00:00,  1.60it/s]\n",
      "2022-05-02 17:30:33,719 INFO: Epoch:[034/100]\n",
      "2022-05-02 17:30:33,720 INFO: Train Loss:0.235 | Acc:0.9447 | F1:0.9312\n",
      "2022-05-02 17:30:54,690 INFO: val Loss:0.166 | Acc:0.9317 | F1:0.9442\n",
      "100%|██████████| 311/311 [03:17<00:00,  1.58it/s]\n",
      "2022-05-02 17:34:11,974 INFO: Epoch:[035/100]\n",
      "2022-05-02 17:34:11,975 INFO: Train Loss:0.220 | Acc:0.9415 | F1:0.9257\n",
      "2022-05-02 17:34:32,273 INFO: val Loss:0.063 | Acc:0.9759 | F1:0.9689\n",
      "2022-05-02 17:34:33,058 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 17:37:53,176 INFO: Epoch:[036/100]\n",
      "2022-05-02 17:37:53,177 INFO: Train Loss:0.208 | Acc:0.9429 | F1:0.9324\n",
      "2022-05-02 17:38:13,925 INFO: val Loss:0.099 | Acc:0.9735 | F1:0.9622\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 17:41:32,986 INFO: Epoch:[037/100]\n",
      "2022-05-02 17:41:32,987 INFO: Train Loss:0.185 | Acc:0.9522 | F1:0.9416\n",
      "2022-05-02 17:41:53,873 INFO: val Loss:0.107 | Acc:0.9823 | F1:0.9715\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 17:45:15,398 INFO: Epoch:[038/100]\n",
      "2022-05-02 17:45:15,399 INFO: Train Loss:0.195 | Acc:0.9498 | F1:0.9305\n",
      "2022-05-02 17:45:35,706 INFO: val Loss:0.068 | Acc:0.9735 | F1:0.9606\n",
      "100%|██████████| 311/311 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 17:48:57,738 INFO: Epoch:[039/100]\n",
      "2022-05-02 17:48:57,740 INFO: Train Loss:0.185 | Acc:0.9534 | F1:0.9398\n",
      "2022-05-02 17:49:18,197 INFO: val Loss:0.052 | Acc:0.9847 | F1:0.9780\n",
      "2022-05-02 17:49:18,958 INFO: -----------------SAVE:39epoch----------------\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 17:52:38,136 INFO: Epoch:[040/100]\n",
      "2022-05-02 17:52:38,137 INFO: Train Loss:0.171 | Acc:0.9588 | F1:0.9441\n",
      "2022-05-02 17:52:59,896 INFO: val Loss:0.196 | Acc:0.9357 | F1:0.9524\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 17:56:20,568 INFO: Epoch:[041/100]\n",
      "2022-05-02 17:56:20,569 INFO: Train Loss:0.165 | Acc:0.9550 | F1:0.9422\n",
      "2022-05-02 17:56:41,240 INFO: val Loss:0.123 | Acc:0.9566 | F1:0.9485\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 18:00:02,627 INFO: Epoch:[042/100]\n",
      "2022-05-02 18:00:02,629 INFO: Train Loss:0.162 | Acc:0.9570 | F1:0.9438\n",
      "2022-05-02 18:00:23,372 INFO: val Loss:0.081 | Acc:0.9751 | F1:0.9679\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 18:03:42,685 INFO: Epoch:[043/100]\n",
      "2022-05-02 18:03:42,686 INFO: Train Loss:0.150 | Acc:0.9596 | F1:0.9492\n",
      "2022-05-02 18:04:02,491 INFO: val Loss:0.050 | Acc:0.9839 | F1:0.9770\n",
      "2022-05-02 18:04:03,436 INFO: -----------------SAVE:43epoch----------------\n",
      "100%|██████████| 311/311 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 18:07:25,484 INFO: Epoch:[044/100]\n",
      "2022-05-02 18:07:25,485 INFO: Train Loss:0.161 | Acc:0.9558 | F1:0.9479\n",
      "2022-05-02 18:07:45,947 INFO: val Loss:0.040 | Acc:0.9896 | F1:0.9859\n",
      "2022-05-02 18:07:46,711 INFO: -----------------SAVE:44epoch----------------\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 18:11:07,251 INFO: Epoch:[045/100]\n",
      "2022-05-02 18:11:07,252 INFO: Train Loss:0.170 | Acc:0.9570 | F1:0.9455\n",
      "2022-05-02 18:11:27,694 INFO: val Loss:0.098 | Acc:0.9526 | F1:0.9512\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 18:14:49,152 INFO: Epoch:[046/100]\n",
      "2022-05-02 18:14:49,153 INFO: Train Loss:0.139 | Acc:0.9560 | F1:0.9502\n",
      "2022-05-02 18:15:09,875 INFO: val Loss:0.047 | Acc:0.9896 | F1:0.9827\n",
      "100%|██████████| 311/311 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 18:18:32,171 INFO: Epoch:[047/100]\n",
      "2022-05-02 18:18:32,172 INFO: Train Loss:0.176 | Acc:0.9624 | F1:0.9511\n",
      "2022-05-02 18:18:52,384 INFO: val Loss:0.096 | Acc:0.9863 | F1:0.9749\n",
      "100%|██████████| 311/311 [03:18<00:00,  1.57it/s]\n",
      "2022-05-02 18:22:10,958 INFO: Epoch:[048/100]\n",
      "2022-05-02 18:22:10,959 INFO: Train Loss:0.167 | Acc:0.9602 | F1:0.9497\n",
      "2022-05-02 18:22:31,471 INFO: val Loss:0.020 | Acc:0.9904 | F1:0.9909\n",
      "2022-05-02 18:22:32,256 INFO: -----------------SAVE:48epoch----------------\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 18:25:53,970 INFO: Epoch:[049/100]\n",
      "2022-05-02 18:25:53,971 INFO: Train Loss:0.119 | Acc:0.9682 | F1:0.9562\n",
      "2022-05-02 18:26:15,329 INFO: val Loss:0.037 | Acc:0.9799 | F1:0.9807\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 18:29:37,189 INFO: Epoch:[050/100]\n",
      "2022-05-02 18:29:37,190 INFO: Train Loss:0.123 | Acc:0.9654 | F1:0.9570\n",
      "2022-05-02 18:29:58,298 INFO: val Loss:0.053 | Acc:0.9775 | F1:0.9821\n",
      "100%|██████████| 311/311 [03:22<00:00,  1.53it/s]\n",
      "2022-05-02 18:33:21,304 INFO: Epoch:[051/100]\n",
      "2022-05-02 18:33:21,305 INFO: Train Loss:0.135 | Acc:0.9628 | F1:0.9553\n",
      "2022-05-02 18:33:42,939 INFO: val Loss:0.023 | Acc:0.9871 | F1:0.9862\n",
      "100%|██████████| 311/311 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 18:37:04,485 INFO: Epoch:[052/100]\n",
      "2022-05-02 18:37:04,486 INFO: Train Loss:0.115 | Acc:0.9689 | F1:0.9594\n",
      "2022-05-02 18:37:25,851 INFO: val Loss:0.043 | Acc:0.9904 | F1:0.9853\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 18:40:45,348 INFO: Epoch:[053/100]\n",
      "2022-05-02 18:40:45,349 INFO: Train Loss:0.114 | Acc:0.9666 | F1:0.9557\n",
      "2022-05-02 18:41:05,179 INFO: val Loss:0.030 | Acc:0.9847 | F1:0.9793\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 18:44:24,417 INFO: Epoch:[054/100]\n",
      "2022-05-02 18:44:24,418 INFO: Train Loss:0.101 | Acc:0.9713 | F1:0.9621\n",
      "2022-05-02 18:44:44,794 INFO: val Loss:0.058 | Acc:0.9912 | F1:0.9870\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 18:48:05,584 INFO: Epoch:[055/100]\n",
      "2022-05-02 18:48:05,585 INFO: Train Loss:0.121 | Acc:0.9697 | F1:0.9578\n",
      "2022-05-02 18:48:26,323 INFO: val Loss:0.018 | Acc:0.9912 | F1:0.9905\n",
      "2022-05-02 18:48:27,124 INFO: -----------------SAVE:55epoch----------------\n",
      "100%|██████████| 311/311 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 18:51:46,785 INFO: Epoch:[056/100]\n",
      "2022-05-02 18:51:46,786 INFO: Train Loss:0.104 | Acc:0.9733 | F1:0.9656\n",
      "2022-05-02 18:52:08,569 INFO: val Loss:0.034 | Acc:0.9823 | F1:0.9844\n",
      "100%|██████████| 311/311 [03:25<00:00,  1.51it/s]\n",
      "2022-05-02 18:55:34,097 INFO: Epoch:[057/100]\n",
      "2022-05-02 18:55:34,098 INFO: Train Loss:0.122 | Acc:0.9717 | F1:0.9617\n",
      "2022-05-02 18:55:54,302 INFO: val Loss:0.069 | Acc:0.9703 | F1:0.9653\n",
      "100%|██████████| 311/311 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 18:59:14,566 INFO: Epoch:[058/100]\n",
      "2022-05-02 18:59:14,568 INFO: Train Loss:0.111 | Acc:0.9721 | F1:0.9600\n",
      "2022-05-02 18:59:34,643 INFO: val Loss:0.027 | Acc:0.9855 | F1:0.9851\n",
      "100%|██████████| 311/311 [03:17<00:00,  1.57it/s]\n",
      "2022-05-02 19:02:52,294 INFO: Epoch:[059/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 19:02:52,295 INFO: Train Loss:0.111 | Acc:0.9711 | F1:0.9609\n",
      "2022-05-02 19:03:12,661 INFO: val Loss:0.042 | Acc:0.9847 | F1:0.9776\n",
      "100%|██████████| 311/311 [03:16<00:00,  1.59it/s]\n",
      "2022-05-02 19:06:28,714 INFO: Epoch:[060/100]\n",
      "2022-05-02 19:06:28,715 INFO: Train Loss:0.096 | Acc:0.9749 | F1:0.9668\n",
      "2022-05-02 19:06:49,548 INFO: val Loss:0.047 | Acc:0.9783 | F1:0.9796\n",
      "100%|██████████| 311/311 [03:18<00:00,  1.57it/s]\n",
      "2022-05-02 19:10:08,211 INFO: Epoch:[061/100]\n",
      "2022-05-02 19:10:08,212 INFO: Train Loss:0.111 | Acc:0.9662 | F1:0.9596\n",
      "2022-05-02 19:10:28,535 INFO: val Loss:0.136 | Acc:0.9703 | F1:0.9644\n",
      "100%|██████████| 311/311 [03:16<00:00,  1.58it/s]\n",
      "2022-05-02 19:13:45,199 INFO: Epoch:[062/100]\n",
      "2022-05-02 19:13:45,200 INFO: Train Loss:0.110 | Acc:0.9751 | F1:0.9686\n",
      "2022-05-02 19:14:05,015 INFO: val Loss:0.038 | Acc:0.9823 | F1:0.9825\n",
      "2022-05-02 19:14:05,016 INFO: \n",
      "Best Val Epoch:55 | Val Loss:0.0179 | Val Acc:0.9912 | Val F1:0.9905\n",
      "2022-05-02 19:14:05,017 INFO: Total Process time:221.421Minute\n",
      "2022-05-02 19:14:05,028 INFO: {'exp_num': '1', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:4977\n",
      "Dataset size:1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 19:14:05,505 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 19:17:29,890 INFO: Epoch:[001/100]\n",
      "2022-05-02 19:17:29,891 INFO: Train Loss:4.521 | Acc:0.0115 | F1:0.0047\n",
      "2022-05-02 19:17:51,807 INFO: val Loss:4.229 | Acc:0.0804 | F1:0.0293\n",
      "2022-05-02 19:17:52,622 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 19:21:13,581 INFO: Epoch:[002/100]\n",
      "2022-05-02 19:21:13,581 INFO: Train Loss:4.136 | Acc:0.1577 | F1:0.0743\n",
      "2022-05-02 19:21:34,561 INFO: val Loss:3.360 | Acc:0.4502 | F1:0.1409\n",
      "2022-05-02 19:21:35,330 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-02 19:24:59,045 INFO: Epoch:[003/100]\n",
      "2022-05-02 19:24:59,046 INFO: Train Loss:3.507 | Acc:0.3448 | F1:0.1503\n",
      "2022-05-02 19:25:19,494 INFO: val Loss:2.604 | Acc:0.4453 | F1:0.1433\n",
      "2022-05-02 19:25:20,296 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 19:28:43,176 INFO: Epoch:[004/100]\n",
      "2022-05-02 19:28:43,176 INFO: Train Loss:2.878 | Acc:0.3506 | F1:0.1893\n",
      "2022-05-02 19:29:03,863 INFO: val Loss:2.152 | Acc:0.3320 | F1:0.1752\n",
      "2022-05-02 19:29:04,707 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 19:32:26,486 INFO: Epoch:[005/100]\n",
      "2022-05-02 19:32:26,487 INFO: Train Loss:2.530 | Acc:0.3355 | F1:0.2111\n",
      "2022-05-02 19:32:46,870 INFO: val Loss:1.910 | Acc:0.3111 | F1:0.1902\n",
      "2022-05-02 19:32:47,719 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 19:36:10,326 INFO: Epoch:[006/100]\n",
      "2022-05-02 19:36:10,327 INFO: Train Loss:2.294 | Acc:0.3719 | F1:0.2537\n",
      "2022-05-02 19:36:30,820 INFO: val Loss:1.727 | Acc:0.3971 | F1:0.2287\n",
      "2022-05-02 19:36:31,621 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 19:39:53,379 INFO: Epoch:[007/100]\n",
      "2022-05-02 19:39:53,380 INFO: Train Loss:2.116 | Acc:0.4324 | F1:0.2990\n",
      "2022-05-02 19:40:15,271 INFO: val Loss:1.498 | Acc:0.5691 | F1:0.3479\n",
      "2022-05-02 19:40:16,078 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 19:43:36,537 INFO: Epoch:[008/100]\n",
      "2022-05-02 19:43:36,538 INFO: Train Loss:1.972 | Acc:0.4850 | F1:0.3425\n",
      "2022-05-02 19:43:58,761 INFO: val Loss:1.424 | Acc:0.5378 | F1:0.3717\n",
      "2022-05-02 19:43:59,598 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 19:47:21,283 INFO: Epoch:[009/100]\n",
      "2022-05-02 19:47:21,284 INFO: Train Loss:1.836 | Acc:0.5258 | F1:0.3890\n",
      "2022-05-02 19:47:43,187 INFO: val Loss:1.273 | Acc:0.6214 | F1:0.3999\n",
      "2022-05-02 19:47:43,995 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 312/312 [03:17<00:00,  1.58it/s]\n",
      "2022-05-02 19:51:01,788 INFO: Epoch:[010/100]\n",
      "2022-05-02 19:51:01,788 INFO: Train Loss:1.721 | Acc:0.5710 | F1:0.4325\n",
      "2022-05-02 19:51:21,827 INFO: val Loss:1.159 | Acc:0.6712 | F1:0.4610\n",
      "2022-05-02 19:51:22,621 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 19:54:44,340 INFO: Epoch:[011/100]\n",
      "2022-05-02 19:54:44,341 INFO: Train Loss:1.575 | Acc:0.6028 | F1:0.4789\n",
      "2022-05-02 19:55:05,136 INFO: val Loss:0.974 | Acc:0.7355 | F1:0.5461\n",
      "2022-05-02 19:55:05,874 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 19:58:30,201 INFO: Epoch:[012/100]\n",
      "2022-05-02 19:58:30,202 INFO: Train Loss:1.444 | Acc:0.6482 | F1:0.5286\n",
      "2022-05-02 19:58:51,425 INFO: val Loss:1.049 | Acc:0.6600 | F1:0.5285\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-02 20:02:14,738 INFO: Epoch:[013/100]\n",
      "2022-05-02 20:02:14,739 INFO: Train Loss:1.306 | Acc:0.6892 | F1:0.5773\n",
      "2022-05-02 20:02:36,432 INFO: val Loss:0.787 | Acc:0.7605 | F1:0.6161\n",
      "2022-05-02 20:02:37,270 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 20:06:01,326 INFO: Epoch:[014/100]\n",
      "2022-05-02 20:06:01,326 INFO: Train Loss:1.192 | Acc:0.7054 | F1:0.6161\n",
      "2022-05-02 20:06:22,177 INFO: val Loss:0.664 | Acc:0.8513 | F1:0.7620\n",
      "2022-05-02 20:06:22,999 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 20:09:44,938 INFO: Epoch:[015/100]\n",
      "2022-05-02 20:09:44,939 INFO: Train Loss:1.046 | Acc:0.7559 | F1:0.6596\n",
      "2022-05-02 20:10:05,199 INFO: val Loss:0.708 | Acc:0.8256 | F1:0.6774\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.57it/s]\n",
      "2022-05-02 20:13:24,305 INFO: Epoch:[016/100]\n",
      "2022-05-02 20:13:24,306 INFO: Train Loss:1.016 | Acc:0.7607 | F1:0.6833\n",
      "2022-05-02 20:13:45,383 INFO: val Loss:0.575 | Acc:0.8513 | F1:0.7383\n",
      "2022-05-02 20:13:46,188 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 312/312 [03:18<00:00,  1.57it/s]\n",
      "2022-05-02 20:17:04,934 INFO: Epoch:[017/100]\n",
      "2022-05-02 20:17:04,935 INFO: Train Loss:0.865 | Acc:0.7920 | F1:0.7301\n",
      "2022-05-02 20:17:25,438 INFO: val Loss:0.519 | Acc:0.8577 | F1:0.7811\n",
      "2022-05-02 20:17:26,219 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 20:20:48,468 INFO: Epoch:[018/100]\n",
      "2022-05-02 20:20:48,469 INFO: Train Loss:0.843 | Acc:0.8107 | F1:0.7475\n",
      "2022-05-02 20:21:08,615 INFO: val Loss:0.400 | Acc:0.8891 | F1:0.8447\n",
      "2022-05-02 20:21:09,365 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 20:24:29,874 INFO: Epoch:[019/100]\n",
      "2022-05-02 20:24:29,875 INFO: Train Loss:0.726 | Acc:0.8330 | F1:0.7789\n",
      "2022-05-02 20:24:50,851 INFO: val Loss:0.381 | Acc:0.8449 | F1:0.8050\n",
      "2022-05-02 20:24:51,665 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 20:28:14,568 INFO: Epoch:[020/100]\n",
      "2022-05-02 20:28:14,569 INFO: Train Loss:0.669 | Acc:0.8401 | F1:0.7940\n",
      "2022-05-02 20:28:35,679 INFO: val Loss:0.366 | Acc:0.7998 | F1:0.8047\n",
      "2022-05-02 20:28:36,494 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 20:31:56,428 INFO: Epoch:[021/100]\n",
      "2022-05-02 20:31:56,430 INFO: Train Loss:0.605 | Acc:0.8541 | F1:0.8134\n",
      "2022-05-02 20:32:16,154 INFO: val Loss:0.262 | Acc:0.9011 | F1:0.8742\n",
      "2022-05-02 20:32:16,930 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 312/312 [03:16<00:00,  1.59it/s]\n",
      "2022-05-02 20:35:33,748 INFO: Epoch:[022/100]\n",
      "2022-05-02 20:35:33,749 INFO: Train Loss:0.565 | Acc:0.8682 | F1:0.8326\n",
      "2022-05-02 20:35:54,432 INFO: val Loss:0.270 | Acc:0.9277 | F1:0.8995\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 20:39:15,842 INFO: Epoch:[023/100]\n",
      "2022-05-02 20:39:15,843 INFO: Train Loss:0.487 | Acc:0.8760 | F1:0.8495\n",
      "2022-05-02 20:39:35,955 INFO: val Loss:0.304 | Acc:0.9188 | F1:0.8677\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 20:42:57,397 INFO: Epoch:[024/100]\n",
      "2022-05-02 20:42:57,398 INFO: Train Loss:0.470 | Acc:0.8875 | F1:0.8580\n",
      "2022-05-02 20:43:18,723 INFO: val Loss:0.134 | Acc:0.9477 | F1:0.9335\n",
      "2022-05-02 20:43:19,526 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 20:46:39,079 INFO: Epoch:[025/100]\n",
      "2022-05-02 20:46:39,079 INFO: Train Loss:0.427 | Acc:0.8973 | F1:0.8717\n",
      "2022-05-02 20:47:01,206 INFO: val Loss:0.186 | Acc:0.9301 | F1:0.9071\n",
      "100%|██████████| 312/312 [03:15<00:00,  1.60it/s]\n",
      "2022-05-02 20:50:16,611 INFO: Epoch:[026/100]\n",
      "2022-05-02 20:50:16,612 INFO: Train Loss:0.411 | Acc:0.9054 | F1:0.8818\n",
      "2022-05-02 20:50:36,498 INFO: val Loss:0.139 | Acc:0.9542 | F1:0.9416\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 20:53:57,065 INFO: Epoch:[027/100]\n",
      "2022-05-02 20:53:57,066 INFO: Train Loss:0.354 | Acc:0.9138 | F1:0.8965\n",
      "2022-05-02 20:54:17,815 INFO: val Loss:0.177 | Acc:0.9204 | F1:0.9144\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 20:57:40,048 INFO: Epoch:[028/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 20:57:40,049 INFO: Train Loss:0.350 | Acc:0.9166 | F1:0.8966\n",
      "2022-05-02 20:58:00,727 INFO: val Loss:0.138 | Acc:0.9695 | F1:0.9574\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 21:01:22,227 INFO: Epoch:[029/100]\n",
      "2022-05-02 21:01:22,227 INFO: Train Loss:0.321 | Acc:0.9202 | F1:0.9036\n",
      "2022-05-02 21:01:42,700 INFO: val Loss:0.138 | Acc:0.9486 | F1:0.9358\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-02 21:05:06,299 INFO: Epoch:[030/100]\n",
      "2022-05-02 21:05:06,300 INFO: Train Loss:0.287 | Acc:0.9208 | F1:0.9041\n",
      "2022-05-02 21:05:27,712 INFO: val Loss:0.116 | Acc:0.9550 | F1:0.9524\n",
      "2022-05-02 21:05:28,536 INFO: -----------------SAVE:30epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 21:08:51,184 INFO: Epoch:[031/100]\n",
      "2022-05-02 21:08:51,185 INFO: Train Loss:0.274 | Acc:0.9277 | F1:0.9149\n",
      "2022-05-02 21:09:12,235 INFO: val Loss:0.156 | Acc:0.9630 | F1:0.9469\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 21:12:35,239 INFO: Epoch:[032/100]\n",
      "2022-05-02 21:12:35,240 INFO: Train Loss:0.257 | Acc:0.9301 | F1:0.9211\n",
      "2022-05-02 21:12:56,356 INFO: val Loss:0.177 | Acc:0.9614 | F1:0.9342\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 21:16:18,107 INFO: Epoch:[033/100]\n",
      "2022-05-02 21:16:18,108 INFO: Train Loss:0.255 | Acc:0.9301 | F1:0.9188\n",
      "2022-05-02 21:16:38,584 INFO: val Loss:0.150 | Acc:0.9381 | F1:0.9487\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 21:20:01,103 INFO: Epoch:[034/100]\n",
      "2022-05-02 21:20:01,105 INFO: Train Loss:0.264 | Acc:0.9271 | F1:0.9146\n",
      "2022-05-02 21:20:21,948 INFO: val Loss:0.061 | Acc:0.9839 | F1:0.9752\n",
      "2022-05-02 21:20:22,775 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 21:23:42,267 INFO: Epoch:[035/100]\n",
      "2022-05-02 21:23:42,268 INFO: Train Loss:0.225 | Acc:0.9383 | F1:0.9244\n",
      "2022-05-02 21:24:02,044 INFO: val Loss:0.062 | Acc:0.9879 | F1:0.9776\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 21:27:22,454 INFO: Epoch:[036/100]\n",
      "2022-05-02 21:27:22,455 INFO: Train Loss:0.249 | Acc:0.9357 | F1:0.9216\n",
      "2022-05-02 21:27:43,626 INFO: val Loss:0.090 | Acc:0.9815 | F1:0.9710\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-02 21:31:06,953 INFO: Epoch:[037/100]\n",
      "2022-05-02 21:31:06,954 INFO: Train Loss:0.216 | Acc:0.9427 | F1:0.9303\n",
      "2022-05-02 21:31:27,659 INFO: val Loss:0.049 | Acc:0.9791 | F1:0.9726\n",
      "2022-05-02 21:31:28,396 INFO: -----------------SAVE:37epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 21:34:51,246 INFO: Epoch:[038/100]\n",
      "2022-05-02 21:34:51,247 INFO: Train Loss:0.190 | Acc:0.9460 | F1:0.9332\n",
      "2022-05-02 21:35:12,086 INFO: val Loss:0.073 | Acc:0.9727 | F1:0.9686\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.54it/s]\n",
      "2022-05-02 21:38:35,159 INFO: Epoch:[039/100]\n",
      "2022-05-02 21:38:35,160 INFO: Train Loss:0.192 | Acc:0.9449 | F1:0.9377\n",
      "2022-05-02 21:38:55,921 INFO: val Loss:0.060 | Acc:0.9831 | F1:0.9735\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.54it/s]\n",
      "2022-05-02 21:42:17,927 INFO: Epoch:[040/100]\n",
      "2022-05-02 21:42:17,928 INFO: Train Loss:0.183 | Acc:0.9510 | F1:0.9403\n",
      "2022-05-02 21:42:38,854 INFO: val Loss:0.080 | Acc:0.9783 | F1:0.9546\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 21:46:00,981 INFO: Epoch:[041/100]\n",
      "2022-05-02 21:46:00,985 INFO: Train Loss:0.192 | Acc:0.9464 | F1:0.9329\n",
      "2022-05-02 21:46:23,730 INFO: val Loss:0.073 | Acc:0.9670 | F1:0.9663\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.52it/s]\n",
      "2022-05-02 21:49:48,717 INFO: Epoch:[042/100]\n",
      "2022-05-02 21:49:48,718 INFO: Train Loss:0.183 | Acc:0.9512 | F1:0.9432\n",
      "2022-05-02 21:50:10,388 INFO: val Loss:0.060 | Acc:0.9686 | F1:0.9694\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 21:53:32,799 INFO: Epoch:[043/100]\n",
      "2022-05-02 21:53:32,801 INFO: Train Loss:0.157 | Acc:0.9526 | F1:0.9443\n",
      "2022-05-02 21:53:53,268 INFO: val Loss:0.022 | Acc:0.9928 | F1:0.9916\n",
      "2022-05-02 21:53:54,088 INFO: -----------------SAVE:43epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 21:57:14,422 INFO: Epoch:[044/100]\n",
      "2022-05-02 21:57:14,423 INFO: Train Loss:0.164 | Acc:0.9574 | F1:0.9453\n",
      "2022-05-02 21:57:36,135 INFO: val Loss:0.063 | Acc:0.9847 | F1:0.9740\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 22:00:56,649 INFO: Epoch:[045/100]\n",
      "2022-05-02 22:00:56,650 INFO: Train Loss:0.163 | Acc:0.9578 | F1:0.9461\n",
      "2022-05-02 22:01:16,976 INFO: val Loss:0.155 | Acc:0.9783 | F1:0.9619\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 22:04:38,001 INFO: Epoch:[046/100]\n",
      "2022-05-02 22:04:38,002 INFO: Train Loss:0.157 | Acc:0.9576 | F1:0.9475\n",
      "2022-05-02 22:04:58,542 INFO: val Loss:0.051 | Acc:0.9855 | F1:0.9822\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 22:08:19,797 INFO: Epoch:[047/100]\n",
      "2022-05-02 22:08:19,798 INFO: Train Loss:0.143 | Acc:0.9616 | F1:0.9509\n",
      "2022-05-02 22:08:39,890 INFO: val Loss:0.113 | Acc:0.9735 | F1:0.9617\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 22:12:01,049 INFO: Epoch:[048/100]\n",
      "2022-05-02 22:12:01,051 INFO: Train Loss:0.133 | Acc:0.9624 | F1:0.9531\n",
      "2022-05-02 22:12:21,794 INFO: val Loss:0.040 | Acc:0.9855 | F1:0.9784\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-02 22:15:45,266 INFO: Epoch:[049/100]\n",
      "2022-05-02 22:15:45,267 INFO: Train Loss:0.117 | Acc:0.9695 | F1:0.9635\n",
      "2022-05-02 22:16:05,584 INFO: val Loss:0.058 | Acc:0.9775 | F1:0.9811\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 22:19:27,973 INFO: Epoch:[050/100]\n",
      "2022-05-02 22:19:27,974 INFO: Train Loss:0.141 | Acc:0.9670 | F1:0.9564\n",
      "2022-05-02 22:19:49,042 INFO: val Loss:0.033 | Acc:0.9847 | F1:0.9837\n",
      "2022-05-02 22:19:49,045 INFO: \n",
      "Best Val Epoch:43 | Val Loss:0.0215 | Val Acc:0.9928 | Val F1:0.9916\n",
      "2022-05-02 22:19:49,045 INFO: Total Process time:185.721Minute\n",
      "2022-05-02 22:19:49,056 INFO: {'exp_num': '2', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:4977\n",
      "Dataset size:1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 22:19:49,586 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 22:23:10,588 INFO: Epoch:[001/100]\n",
      "2022-05-02 22:23:10,589 INFO: Train Loss:4.519 | Acc:0.0137 | F1:0.0051\n",
      "2022-05-02 22:23:31,275 INFO: val Loss:4.229 | Acc:0.0997 | F1:0.0258\n",
      "2022-05-02 22:23:32,101 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 22:26:56,599 INFO: Epoch:[002/100]\n",
      "2022-05-02 22:26:56,600 INFO: Train Loss:4.136 | Acc:0.1579 | F1:0.0727\n",
      "2022-05-02 22:27:16,728 INFO: val Loss:3.364 | Acc:0.4035 | F1:0.1153\n",
      "2022-05-02 22:27:17,601 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.54it/s]\n",
      "2022-05-02 22:30:40,847 INFO: Epoch:[003/100]\n",
      "2022-05-02 22:30:40,848 INFO: Train Loss:3.504 | Acc:0.3516 | F1:0.1537\n",
      "2022-05-02 22:31:01,495 INFO: val Loss:2.622 | Acc:0.4228 | F1:0.1381\n",
      "2022-05-02 22:31:02,313 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 22:34:23,648 INFO: Epoch:[004/100]\n",
      "2022-05-02 22:34:23,649 INFO: Train Loss:2.891 | Acc:0.3390 | F1:0.1879\n",
      "2022-05-02 22:34:44,917 INFO: val Loss:2.144 | Acc:0.3931 | F1:0.1766\n",
      "2022-05-02 22:34:45,919 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.57it/s]\n",
      "2022-05-02 22:38:05,028 INFO: Epoch:[005/100]\n",
      "2022-05-02 22:38:05,029 INFO: Train Loss:2.537 | Acc:0.3400 | F1:0.2173\n",
      "2022-05-02 22:38:25,826 INFO: val Loss:1.884 | Acc:0.3907 | F1:0.2006\n",
      "2022-05-02 22:38:26,643 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 22:41:47,855 INFO: Epoch:[006/100]\n",
      "2022-05-02 22:41:47,856 INFO: Train Loss:2.303 | Acc:0.3822 | F1:0.2463\n",
      "2022-05-02 22:42:08,603 INFO: val Loss:1.656 | Acc:0.5169 | F1:0.2745\n",
      "2022-05-02 22:42:09,343 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 22:45:31,455 INFO: Epoch:[007/100]\n",
      "2022-05-02 22:45:31,456 INFO: Train Loss:2.110 | Acc:0.4438 | F1:0.3092\n",
      "2022-05-02 22:45:51,341 INFO: val Loss:1.488 | Acc:0.5338 | F1:0.3209\n",
      "2022-05-02 22:45:52,167 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 22:49:14,692 INFO: Epoch:[008/100]\n",
      "2022-05-02 22:49:14,693 INFO: Train Loss:1.972 | Acc:0.5061 | F1:0.3537\n",
      "2022-05-02 22:49:34,670 INFO: val Loss:1.367 | Acc:0.5434 | F1:0.3882\n",
      "2022-05-02 22:49:35,463 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 22:52:57,295 INFO: Epoch:[009/100]\n",
      "2022-05-02 22:52:57,296 INFO: Train Loss:1.835 | Acc:0.5322 | F1:0.3836\n",
      "2022-05-02 22:53:17,982 INFO: val Loss:1.229 | Acc:0.6986 | F1:0.4566\n",
      "2022-05-02 22:53:18,837 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 22:56:43,189 INFO: Epoch:[010/100]\n",
      "2022-05-02 22:56:43,190 INFO: Train Loss:1.714 | Acc:0.5805 | F1:0.4377\n",
      "2022-05-02 22:57:03,205 INFO: val Loss:1.092 | Acc:0.6841 | F1:0.4933\n",
      "2022-05-02 22:57:04,029 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 23:00:25,239 INFO: Epoch:[011/100]\n",
      "2022-05-02 23:00:25,240 INFO: Train Loss:1.573 | Acc:0.6150 | F1:0.4781\n",
      "2022-05-02 23:00:46,350 INFO: val Loss:1.021 | Acc:0.7106 | F1:0.5619\n",
      "2022-05-02 23:00:47,232 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 23:04:11,452 INFO: Epoch:[012/100]\n",
      "2022-05-02 23:04:11,453 INFO: Train Loss:1.446 | Acc:0.6490 | F1:0.5304\n",
      "2022-05-02 23:04:31,534 INFO: val Loss:0.883 | Acc:0.7773 | F1:0.6079\n",
      "2022-05-02 23:04:32,295 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 312/312 [03:16<00:00,  1.59it/s]\n",
      "2022-05-02 23:07:49,090 INFO: Epoch:[013/100]\n",
      "2022-05-02 23:07:49,091 INFO: Train Loss:1.317 | Acc:0.6886 | F1:0.5769\n",
      "2022-05-02 23:08:08,549 INFO: val Loss:0.788 | Acc:0.7846 | F1:0.6280\n",
      "2022-05-02 23:08:09,358 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-02 23:11:29,846 INFO: Epoch:[014/100]\n",
      "2022-05-02 23:11:29,847 INFO: Train Loss:1.226 | Acc:0.7111 | F1:0.6084\n",
      "2022-05-02 23:11:50,127 INFO: val Loss:0.777 | Acc:0.6849 | F1:0.6417\n",
      "2022-05-02 23:11:51,013 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-02 23:15:12,741 INFO: Epoch:[015/100]\n",
      "2022-05-02 23:15:12,741 INFO: Train Loss:1.106 | Acc:0.7408 | F1:0.6531\n",
      "2022-05-02 23:15:34,141 INFO: val Loss:0.635 | Acc:0.8240 | F1:0.7246\n",
      "2022-05-02 23:15:34,918 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 312/312 [03:26<00:00,  1.51it/s]\n",
      "2022-05-02 23:19:01,801 INFO: Epoch:[016/100]\n",
      "2022-05-02 23:19:01,802 INFO: Train Loss:1.035 | Acc:0.7545 | F1:0.6723\n",
      "2022-05-02 23:19:23,756 INFO: val Loss:0.599 | Acc:0.7982 | F1:0.7114\n",
      "2022-05-02 23:19:24,575 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.52it/s]\n",
      "2022-05-02 23:22:49,488 INFO: Epoch:[017/100]\n",
      "2022-05-02 23:22:49,489 INFO: Train Loss:0.906 | Acc:0.7798 | F1:0.7047\n",
      "2022-05-02 23:23:10,347 INFO: val Loss:0.487 | Acc:0.8834 | F1:0.7835\n",
      "2022-05-02 23:23:11,188 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-02 23:26:33,726 INFO: Epoch:[018/100]\n",
      "2022-05-02 23:26:33,727 INFO: Train Loss:0.859 | Acc:0.8027 | F1:0.7344\n",
      "2022-05-02 23:26:54,077 INFO: val Loss:0.418 | Acc:0.8674 | F1:0.8109\n",
      "2022-05-02 23:26:54,941 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.55it/s]\n",
      "2022-05-02 23:30:15,610 INFO: Epoch:[019/100]\n",
      "2022-05-02 23:30:15,611 INFO: Train Loss:0.758 | Acc:0.8174 | F1:0.7639\n",
      "2022-05-02 23:30:36,034 INFO: val Loss:0.499 | Acc:0.7781 | F1:0.7669\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-02 23:34:01,610 INFO: Epoch:[020/100]\n",
      "2022-05-02 23:34:01,611 INFO: Train Loss:0.707 | Acc:0.8176 | F1:0.7748\n",
      "2022-05-02 23:34:22,193 INFO: val Loss:0.392 | Acc:0.8561 | F1:0.8211\n",
      "2022-05-02 23:34:23,010 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-02 23:37:47,246 INFO: Epoch:[021/100]\n",
      "2022-05-02 23:37:47,247 INFO: Train Loss:0.646 | Acc:0.8499 | F1:0.8029\n",
      "2022-05-02 23:38:08,639 INFO: val Loss:0.254 | Acc:0.9486 | F1:0.9164\n",
      "2022-05-02 23:38:09,440 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 312/312 [03:18<00:00,  1.58it/s]\n",
      "2022-05-02 23:41:27,479 INFO: Epoch:[022/100]\n",
      "2022-05-02 23:41:27,480 INFO: Train Loss:0.590 | Acc:0.8583 | F1:0.8167\n",
      "2022-05-02 23:41:48,583 INFO: val Loss:0.269 | Acc:0.9381 | F1:0.9047\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-02 23:45:14,269 INFO: Epoch:[023/100]\n",
      "2022-05-02 23:45:14,270 INFO: Train Loss:0.512 | Acc:0.8815 | F1:0.8476\n",
      "2022-05-02 23:45:34,725 INFO: val Loss:0.325 | Acc:0.8971 | F1:0.8624\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.52it/s]\n",
      "2022-05-02 23:48:59,720 INFO: Epoch:[024/100]\n",
      "2022-05-02 23:48:59,721 INFO: Train Loss:0.489 | Acc:0.8881 | F1:0.8557\n",
      "2022-05-02 23:49:20,284 INFO: val Loss:0.200 | Acc:0.9381 | F1:0.9190\n",
      "2022-05-02 23:49:21,490 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-02 23:52:46,679 INFO: Epoch:[025/100]\n",
      "2022-05-02 23:52:46,680 INFO: Train Loss:0.439 | Acc:0.8969 | F1:0.8697\n",
      "2022-05-02 23:53:06,939 INFO: val Loss:0.149 | Acc:0.9542 | F1:0.9350\n",
      "2022-05-02 23:53:07,846 INFO: -----------------SAVE:25epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-02 23:56:27,520 INFO: Epoch:[026/100]\n",
      "2022-05-02 23:56:27,521 INFO: Train Loss:0.395 | Acc:0.9040 | F1:0.8810\n",
      "2022-05-02 23:56:48,053 INFO: val Loss:0.215 | Acc:0.9124 | F1:0.9136\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.55it/s]\n",
      "2022-05-03 00:00:08,883 INFO: Epoch:[027/100]\n",
      "2022-05-03 00:00:08,883 INFO: Train Loss:0.366 | Acc:0.9148 | F1:0.8926\n",
      "2022-05-03 00:00:29,251 INFO: val Loss:0.142 | Acc:0.9582 | F1:0.9429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 00:00:30,101 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 312/312 [03:18<00:00,  1.57it/s]\n",
      "2022-05-03 00:03:48,585 INFO: Epoch:[028/100]\n",
      "2022-05-03 00:03:48,586 INFO: Train Loss:0.355 | Acc:0.9190 | F1:0.8977\n",
      "2022-05-03 00:04:08,523 INFO: val Loss:0.179 | Acc:0.9268 | F1:0.9294\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.55it/s]\n",
      "2022-05-03 00:07:29,357 INFO: Epoch:[029/100]\n",
      "2022-05-03 00:07:29,358 INFO: Train Loss:0.351 | Acc:0.9158 | F1:0.8986\n",
      "2022-05-03 00:07:49,498 INFO: val Loss:0.091 | Acc:0.9727 | F1:0.9522\n",
      "2022-05-03 00:07:50,278 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-03 00:11:15,665 INFO: Epoch:[030/100]\n",
      "2022-05-03 00:11:15,666 INFO: Train Loss:0.297 | Acc:0.9255 | F1:0.9091\n",
      "2022-05-03 00:11:35,684 INFO: val Loss:0.093 | Acc:0.9711 | F1:0.9610\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-03 00:14:59,908 INFO: Epoch:[031/100]\n",
      "2022-05-03 00:14:59,909 INFO: Train Loss:0.286 | Acc:0.9267 | F1:0.9104\n",
      "2022-05-03 00:15:20,646 INFO: val Loss:0.109 | Acc:0.9686 | F1:0.9465\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-03 00:18:45,750 INFO: Epoch:[032/100]\n",
      "2022-05-03 00:18:45,751 INFO: Train Loss:0.240 | Acc:0.9381 | F1:0.9271\n",
      "2022-05-03 00:19:06,271 INFO: val Loss:0.150 | Acc:0.9429 | F1:0.9371\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-03 00:22:29,587 INFO: Epoch:[033/100]\n",
      "2022-05-03 00:22:29,587 INFO: Train Loss:0.268 | Acc:0.9427 | F1:0.9257\n",
      "2022-05-03 00:22:49,664 INFO: val Loss:0.064 | Acc:0.9703 | F1:0.9712\n",
      "2022-05-03 00:22:50,441 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 312/312 [03:17<00:00,  1.58it/s]\n",
      "2022-05-03 00:26:08,215 INFO: Epoch:[034/100]\n",
      "2022-05-03 00:26:08,217 INFO: Train Loss:0.247 | Acc:0.9359 | F1:0.9253\n",
      "2022-05-03 00:26:29,477 INFO: val Loss:0.179 | Acc:0.9494 | F1:0.9599\n",
      "100%|██████████| 312/312 [03:18<00:00,  1.57it/s]\n",
      "2022-05-03 00:29:48,473 INFO: Epoch:[035/100]\n",
      "2022-05-03 00:29:48,476 INFO: Train Loss:0.218 | Acc:0.9441 | F1:0.9319\n",
      "2022-05-03 00:30:08,976 INFO: val Loss:0.037 | Acc:0.9879 | F1:0.9838\n",
      "2022-05-03 00:30:09,773 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-03 00:33:29,755 INFO: Epoch:[036/100]\n",
      "2022-05-03 00:33:29,756 INFO: Train Loss:0.210 | Acc:0.9494 | F1:0.9370\n",
      "2022-05-03 00:33:51,654 INFO: val Loss:0.096 | Acc:0.9598 | F1:0.9614\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.54it/s]\n",
      "2022-05-03 00:37:14,773 INFO: Epoch:[037/100]\n",
      "2022-05-03 00:37:14,774 INFO: Train Loss:0.210 | Acc:0.9458 | F1:0.9292\n",
      "2022-05-03 00:37:35,276 INFO: val Loss:0.207 | Acc:0.9236 | F1:0.9375\n",
      "100%|██████████| 312/312 [03:24<00:00,  1.53it/s]\n",
      "2022-05-03 00:40:59,653 INFO: Epoch:[038/100]\n",
      "2022-05-03 00:40:59,654 INFO: Train Loss:0.191 | Acc:0.9492 | F1:0.9372\n",
      "2022-05-03 00:41:19,741 INFO: val Loss:0.035 | Acc:0.9871 | F1:0.9830\n",
      "2022-05-03 00:41:20,542 INFO: -----------------SAVE:38epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-03 00:44:41,028 INFO: Epoch:[039/100]\n",
      "2022-05-03 00:44:41,029 INFO: Train Loss:0.186 | Acc:0.9490 | F1:0.9339\n",
      "2022-05-03 00:45:01,033 INFO: val Loss:0.078 | Acc:0.9775 | F1:0.9741\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-03 00:48:26,107 INFO: Epoch:[040/100]\n",
      "2022-05-03 00:48:26,108 INFO: Train Loss:0.205 | Acc:0.9514 | F1:0.9389\n",
      "2022-05-03 00:48:46,312 INFO: val Loss:0.073 | Acc:0.9855 | F1:0.9790\n",
      "100%|██████████| 312/312 [03:26<00:00,  1.51it/s]\n",
      "2022-05-03 00:52:13,105 INFO: Epoch:[041/100]\n",
      "2022-05-03 00:52:13,106 INFO: Train Loss:0.176 | Acc:0.9476 | F1:0.9385\n",
      "2022-05-03 00:52:33,556 INFO: val Loss:0.019 | Acc:0.9904 | F1:0.9891\n",
      "2022-05-03 00:52:34,447 INFO: -----------------SAVE:41epoch----------------\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-03 00:55:58,371 INFO: Epoch:[042/100]\n",
      "2022-05-03 00:55:58,372 INFO: Train Loss:0.184 | Acc:0.9560 | F1:0.9419\n",
      "2022-05-03 00:56:19,792 INFO: val Loss:0.084 | Acc:0.9727 | F1:0.9666\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-03 00:59:45,306 INFO: Epoch:[043/100]\n",
      "2022-05-03 00:59:45,307 INFO: Train Loss:0.176 | Acc:0.9516 | F1:0.9422\n",
      "2022-05-03 01:00:06,282 INFO: val Loss:0.101 | Acc:0.9791 | F1:0.9646\n",
      "100%|██████████| 312/312 [03:25<00:00,  1.52it/s]\n",
      "2022-05-03 01:03:31,543 INFO: Epoch:[044/100]\n",
      "2022-05-03 01:03:31,547 INFO: Train Loss:0.160 | Acc:0.9518 | F1:0.9436\n",
      "2022-05-03 01:03:52,883 INFO: val Loss:0.025 | Acc:0.9912 | F1:0.9894\n",
      "100%|██████████| 312/312 [03:28<00:00,  1.50it/s]\n",
      "2022-05-03 01:07:21,006 INFO: Epoch:[045/100]\n",
      "2022-05-03 01:07:21,007 INFO: Train Loss:0.143 | Acc:0.9594 | F1:0.9504\n",
      "2022-05-03 01:07:42,098 INFO: val Loss:0.035 | Acc:0.9839 | F1:0.9844\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-03 01:11:04,009 INFO: Epoch:[046/100]\n",
      "2022-05-03 01:11:04,011 INFO: Train Loss:0.152 | Acc:0.9630 | F1:0.9532\n",
      "2022-05-03 01:11:25,458 INFO: val Loss:0.060 | Acc:0.9719 | F1:0.9704\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-03 01:14:45,271 INFO: Epoch:[047/100]\n",
      "2022-05-03 01:14:45,272 INFO: Train Loss:0.131 | Acc:0.9666 | F1:0.9573\n",
      "2022-05-03 01:15:05,986 INFO: val Loss:0.042 | Acc:0.9807 | F1:0.9781\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-03 01:18:28,245 INFO: Epoch:[048/100]\n",
      "2022-05-03 01:18:28,246 INFO: Train Loss:0.135 | Acc:0.9685 | F1:0.9587\n",
      "2022-05-03 01:18:48,531 INFO: val Loss:0.047 | Acc:0.9823 | F1:0.9819\n",
      "2022-05-03 01:18:48,533 INFO: \n",
      "Best Val Epoch:41 | Val Loss:0.0195 | Val Acc:0.9904 | Val F1:0.9891\n",
      "2022-05-03 01:18:48,534 INFO: Total Process time:178.975Minute\n",
      "2022-05-03 01:18:48,547 INFO: {'exp_num': '3', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:4977\n",
      "Dataset size:1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 01:18:49,059 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.57it/s]\n",
      "2022-05-03 01:22:08,734 INFO: Epoch:[001/100]\n",
      "2022-05-03 01:22:08,735 INFO: Train Loss:4.523 | Acc:0.0157 | F1:0.0053\n",
      "2022-05-03 01:22:29,413 INFO: val Loss:4.261 | Acc:0.0563 | F1:0.0211\n",
      "2022-05-03 01:22:30,189 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.55it/s]\n",
      "2022-05-03 01:25:50,959 INFO: Epoch:[002/100]\n",
      "2022-05-03 01:25:50,961 INFO: Train Loss:4.129 | Acc:0.1316 | F1:0.0667\n",
      "2022-05-03 01:26:12,097 INFO: val Loss:3.460 | Acc:0.4212 | F1:0.1267\n",
      "2022-05-03 01:26:12,922 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-03 01:29:34,702 INFO: Epoch:[003/100]\n",
      "2022-05-03 01:29:34,703 INFO: Train Loss:3.518 | Acc:0.3167 | F1:0.1453\n",
      "2022-05-03 01:29:54,700 INFO: val Loss:2.620 | Acc:0.4952 | F1:0.1416\n",
      "2022-05-03 01:29:55,517 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-03 01:33:19,309 INFO: Epoch:[004/100]\n",
      "2022-05-03 01:33:19,310 INFO: Train Loss:2.900 | Acc:0.3643 | F1:0.1846\n",
      "2022-05-03 01:33:40,288 INFO: val Loss:2.194 | Acc:0.3625 | F1:0.1625\n",
      "2022-05-03 01:33:41,095 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-03 01:37:03,952 INFO: Epoch:[005/100]\n",
      "2022-05-03 01:37:03,953 INFO: Train Loss:2.527 | Acc:0.3205 | F1:0.2120\n",
      "2022-05-03 01:37:24,903 INFO: val Loss:1.877 | Acc:0.3987 | F1:0.2081\n",
      "2022-05-03 01:37:25,768 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 312/312 [03:19<00:00,  1.56it/s]\n",
      "2022-05-03 01:40:45,517 INFO: Epoch:[006/100]\n",
      "2022-05-03 01:40:45,518 INFO: Train Loss:2.286 | Acc:0.3623 | F1:0.2487\n",
      "2022-05-03 01:41:05,765 INFO: val Loss:1.676 | Acc:0.4711 | F1:0.2409\n",
      "2022-05-03 01:41:06,517 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.53it/s]\n",
      "2022-05-03 01:44:29,794 INFO: Epoch:[007/100]\n",
      "2022-05-03 01:44:29,795 INFO: Train Loss:2.119 | Acc:0.4262 | F1:0.2960\n",
      "2022-05-03 01:44:51,091 INFO: val Loss:1.531 | Acc:0.4952 | F1:0.3233\n",
      "2022-05-03 01:44:51,821 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-03 01:48:14,268 INFO: Epoch:[008/100]\n",
      "2022-05-03 01:48:14,268 INFO: Train Loss:1.978 | Acc:0.4808 | F1:0.3331\n",
      "2022-05-03 01:48:35,402 INFO: val Loss:1.395 | Acc:0.5812 | F1:0.3944\n",
      "2022-05-03 01:48:36,135 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 312/312 [03:23<00:00,  1.54it/s]\n",
      "2022-05-03 01:51:59,305 INFO: Epoch:[009/100]\n",
      "2022-05-03 01:51:59,306 INFO: Train Loss:1.819 | Acc:0.5202 | F1:0.3843\n",
      "2022-05-03 01:52:19,787 INFO: val Loss:1.255 | Acc:0.7082 | F1:0.4494\n",
      "2022-05-03 01:52:20,589 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-03 01:55:43,366 INFO: Epoch:[010/100]\n",
      "2022-05-03 01:55:43,367 INFO: Train Loss:1.704 | Acc:0.5694 | F1:0.4315\n",
      "2022-05-03 01:56:03,708 INFO: val Loss:1.118 | Acc:0.6986 | F1:0.4948\n",
      "2022-05-03 01:56:04,503 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 312/312 [03:20<00:00,  1.56it/s]\n",
      "2022-05-03 01:59:24,710 INFO: Epoch:[011/100]\n",
      "2022-05-03 01:59:24,711 INFO: Train Loss:1.563 | Acc:0.6118 | F1:0.4832\n",
      "2022-05-03 01:59:45,088 INFO: val Loss:0.925 | Acc:0.7942 | F1:0.5968\n",
      "2022-05-03 01:59:45,846 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.54it/s]\n",
      "2022-05-03 02:03:07,831 INFO: Epoch:[012/100]\n",
      "2022-05-03 02:03:07,833 INFO: Train Loss:1.454 | Acc:0.6594 | F1:0.5368\n",
      "2022-05-03 02:03:29,071 INFO: val Loss:0.898 | Acc:0.7524 | F1:0.6297\n",
      "2022-05-03 02:03:29,937 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-03 02:06:52,107 INFO: Epoch:[013/100]\n",
      "2022-05-03 02:06:52,108 INFO: Train Loss:1.290 | Acc:0.6960 | F1:0.5772\n",
      "2022-05-03 02:07:12,665 INFO: val Loss:0.768 | Acc:0.8191 | F1:0.6756\n",
      "2022-05-03 02:07:13,442 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 312/312 [03:22<00:00,  1.54it/s]\n",
      "2022-05-03 02:10:35,780 INFO: Epoch:[014/100]\n",
      "2022-05-03 02:10:35,781 INFO: Train Loss:1.176 | Acc:0.7259 | F1:0.6300\n",
      "2022-05-03 02:10:56,595 INFO: val Loss:0.722 | Acc:0.8095 | F1:0.7219\n",
      "2022-05-03 02:10:57,414 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-03 02:14:18,677 INFO: Epoch:[015/100]\n",
      "2022-05-03 02:14:18,678 INFO: Train Loss:1.084 | Acc:0.7444 | F1:0.6542\n",
      "2022-05-03 02:14:38,897 INFO: val Loss:0.645 | Acc:0.8014 | F1:0.7170\n",
      "2022-05-03 02:14:39,639 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-03 02:18:01,142 INFO: Epoch:[016/100]\n",
      "2022-05-03 02:18:01,143 INFO: Train Loss:1.028 | Acc:0.7685 | F1:0.6905\n",
      "2022-05-03 02:18:22,519 INFO: val Loss:0.509 | Acc:0.8730 | F1:0.7810\n",
      "2022-05-03 02:18:23,438 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 312/312 [03:21<00:00,  1.55it/s]\n",
      "2022-05-03 02:21:44,914 INFO: Epoch:[017/100]\n",
      "2022-05-03 02:21:44,916 INFO: Train Loss:0.898 | Acc:0.7932 | F1:0.7180\n",
      "2022-05-03 02:22:05,983 INFO: val Loss:0.453 | Acc:0.8400 | F1:0.7902\n",
      "2022-05-03 02:22:06,799 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 312/312 [02:39<00:00,  1.96it/s]\n",
      "2022-05-03 02:24:46,035 INFO: Epoch:[018/100]\n",
      "2022-05-03 02:24:46,035 INFO: Train Loss:0.812 | Acc:0.8170 | F1:0.7558\n",
      "2022-05-03 02:25:05,961 INFO: val Loss:0.415 | Acc:0.8473 | F1:0.8239\n",
      "2022-05-03 02:25:06,739 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 312/312 [02:40<00:00,  1.95it/s]\n",
      "2022-05-03 02:27:46,879 INFO: Epoch:[019/100]\n",
      "2022-05-03 02:27:46,880 INFO: Train Loss:0.731 | Acc:0.8190 | F1:0.7738\n",
      "2022-05-03 02:28:06,905 INFO: val Loss:0.311 | Acc:0.9076 | F1:0.8767\n",
      "2022-05-03 02:28:07,698 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 312/312 [02:29<00:00,  2.08it/s]\n",
      "2022-05-03 02:30:37,633 INFO: Epoch:[020/100]\n",
      "2022-05-03 02:30:37,633 INFO: Train Loss:0.656 | Acc:0.8441 | F1:0.8072\n",
      "2022-05-03 02:30:55,893 INFO: val Loss:0.289 | Acc:0.9252 | F1:0.8777\n",
      "2022-05-03 02:30:56,605 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 02:33:23,162 INFO: Epoch:[021/100]\n",
      "2022-05-03 02:33:23,163 INFO: Train Loss:0.593 | Acc:0.8640 | F1:0.8244\n",
      "2022-05-03 02:33:41,760 INFO: val Loss:0.267 | Acc:0.9317 | F1:0.9048\n",
      "2022-05-03 02:33:42,472 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 02:36:10,728 INFO: Epoch:[022/100]\n",
      "2022-05-03 02:36:10,729 INFO: Train Loss:0.532 | Acc:0.8740 | F1:0.8481\n",
      "2022-05-03 02:36:29,098 INFO: val Loss:0.306 | Acc:0.9116 | F1:0.8782\n",
      "100%|██████████| 312/312 [02:30<00:00,  2.07it/s]\n",
      "2022-05-03 02:38:59,818 INFO: Epoch:[023/100]\n",
      "2022-05-03 02:38:59,819 INFO: Train Loss:0.470 | Acc:0.8835 | F1:0.8541\n",
      "2022-05-03 02:39:18,735 INFO: val Loss:0.230 | Acc:0.9204 | F1:0.9183\n",
      "2022-05-03 02:39:19,684 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 312/312 [02:31<00:00,  2.06it/s]\n",
      "2022-05-03 02:41:51,208 INFO: Epoch:[024/100]\n",
      "2022-05-03 02:41:51,209 INFO: Train Loss:0.455 | Acc:0.8897 | F1:0.8665\n",
      "2022-05-03 02:42:10,113 INFO: val Loss:0.127 | Acc:0.9622 | F1:0.9413\n",
      "2022-05-03 02:42:10,838 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.15it/s]\n",
      "2022-05-03 02:44:36,158 INFO: Epoch:[025/100]\n",
      "2022-05-03 02:44:36,159 INFO: Train Loss:0.392 | Acc:0.8981 | F1:0.8796\n",
      "2022-05-03 02:44:54,696 INFO: val Loss:0.164 | Acc:0.9477 | F1:0.9283\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 02:47:22,112 INFO: Epoch:[026/100]\n",
      "2022-05-03 02:47:22,113 INFO: Train Loss:0.376 | Acc:0.9038 | F1:0.8852\n",
      "2022-05-03 02:47:40,280 INFO: val Loss:0.148 | Acc:0.9325 | F1:0.9288\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 02:50:06,530 INFO: Epoch:[027/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 02:50:06,532 INFO: Train Loss:0.362 | Acc:0.9126 | F1:0.8902\n",
      "2022-05-03 02:50:26,127 INFO: val Loss:0.130 | Acc:0.9646 | F1:0.9440\n",
      "100%|██████████| 312/312 [02:24<00:00,  2.16it/s]\n",
      "2022-05-03 02:52:50,548 INFO: Epoch:[028/100]\n",
      "2022-05-03 02:52:50,549 INFO: Train Loss:0.346 | Acc:0.9116 | F1:0.8976\n",
      "2022-05-03 02:53:10,140 INFO: val Loss:0.096 | Acc:0.9614 | F1:0.9569\n",
      "2022-05-03 02:53:10,854 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 02:55:38,446 INFO: Epoch:[029/100]\n",
      "2022-05-03 02:55:38,447 INFO: Train Loss:0.311 | Acc:0.9216 | F1:0.9069\n",
      "2022-05-03 02:55:57,166 INFO: val Loss:0.133 | Acc:0.9622 | F1:0.9475\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 02:58:24,737 INFO: Epoch:[030/100]\n",
      "2022-05-03 02:58:24,738 INFO: Train Loss:0.273 | Acc:0.9253 | F1:0.9180\n",
      "2022-05-03 02:58:43,487 INFO: val Loss:0.101 | Acc:0.9526 | F1:0.9568\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 03:01:11,265 INFO: Epoch:[031/100]\n",
      "2022-05-03 03:01:11,265 INFO: Train Loss:0.252 | Acc:0.9395 | F1:0.9279\n",
      "2022-05-03 03:01:30,841 INFO: val Loss:0.077 | Acc:0.9783 | F1:0.9664\n",
      "2022-05-03 03:01:31,669 INFO: -----------------SAVE:31epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 03:03:58,369 INFO: Epoch:[032/100]\n",
      "2022-05-03 03:03:58,369 INFO: Train Loss:0.242 | Acc:0.9337 | F1:0.9230\n",
      "2022-05-03 03:04:16,608 INFO: val Loss:0.088 | Acc:0.9759 | F1:0.9676\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.15it/s]\n",
      "2022-05-03 03:06:41,926 INFO: Epoch:[033/100]\n",
      "2022-05-03 03:06:41,927 INFO: Train Loss:0.259 | Acc:0.9335 | F1:0.9189\n",
      "2022-05-03 03:07:01,108 INFO: val Loss:0.081 | Acc:0.9662 | F1:0.9532\n",
      "100%|██████████| 312/312 [02:24<00:00,  2.16it/s]\n",
      "2022-05-03 03:09:25,824 INFO: Epoch:[034/100]\n",
      "2022-05-03 03:09:25,825 INFO: Train Loss:0.215 | Acc:0.9451 | F1:0.9361\n",
      "2022-05-03 03:09:44,647 INFO: val Loss:0.066 | Acc:0.9815 | F1:0.9754\n",
      "2022-05-03 03:09:45,382 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 03:12:10,963 INFO: Epoch:[035/100]\n",
      "2022-05-03 03:12:10,964 INFO: Train Loss:0.215 | Acc:0.9377 | F1:0.9260\n",
      "2022-05-03 03:12:30,218 INFO: val Loss:0.115 | Acc:0.9678 | F1:0.9514\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 03:14:56,667 INFO: Epoch:[036/100]\n",
      "2022-05-03 03:14:56,668 INFO: Train Loss:0.235 | Acc:0.9466 | F1:0.9325\n",
      "2022-05-03 03:15:15,223 INFO: val Loss:0.095 | Acc:0.9518 | F1:0.9626\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 03:17:40,894 INFO: Epoch:[037/100]\n",
      "2022-05-03 03:17:40,895 INFO: Train Loss:0.187 | Acc:0.9494 | F1:0.9422\n",
      "2022-05-03 03:17:58,911 INFO: val Loss:0.133 | Acc:0.9518 | F1:0.9616\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.15it/s]\n",
      "2022-05-03 03:20:24,262 INFO: Epoch:[038/100]\n",
      "2022-05-03 03:20:24,263 INFO: Train Loss:0.197 | Acc:0.9401 | F1:0.9296\n",
      "2022-05-03 03:20:43,058 INFO: val Loss:0.089 | Acc:0.9751 | F1:0.9675\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 03:23:08,616 INFO: Epoch:[039/100]\n",
      "2022-05-03 03:23:08,617 INFO: Train Loss:0.185 | Acc:0.9494 | F1:0.9347\n",
      "2022-05-03 03:23:28,076 INFO: val Loss:0.058 | Acc:0.9783 | F1:0.9689\n",
      "2022-05-03 03:23:28,871 INFO: -----------------SAVE:39epoch----------------\n",
      "100%|██████████| 312/312 [02:22<00:00,  2.19it/s]\n",
      "2022-05-03 03:25:51,114 INFO: Epoch:[040/100]\n",
      "2022-05-03 03:25:51,115 INFO: Train Loss:0.186 | Acc:0.9516 | F1:0.9399\n",
      "2022-05-03 03:26:09,122 INFO: val Loss:0.054 | Acc:0.9847 | F1:0.9762\n",
      "2022-05-03 03:26:09,820 INFO: -----------------SAVE:40epoch----------------\n",
      "100%|██████████| 312/312 [02:29<00:00,  2.09it/s]\n",
      "2022-05-03 03:28:39,000 INFO: Epoch:[041/100]\n",
      "2022-05-03 03:28:39,001 INFO: Train Loss:0.180 | Acc:0.9490 | F1:0.9369\n",
      "2022-05-03 03:28:57,923 INFO: val Loss:0.068 | Acc:0.9815 | F1:0.9705\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 03:31:24,118 INFO: Epoch:[042/100]\n",
      "2022-05-03 03:31:24,119 INFO: Train Loss:0.169 | Acc:0.9582 | F1:0.9491\n",
      "2022-05-03 03:31:42,402 INFO: val Loss:0.119 | Acc:0.9799 | F1:0.9686\n",
      "100%|██████████| 312/312 [02:20<00:00,  2.22it/s]\n",
      "2022-05-03 03:34:02,901 INFO: Epoch:[043/100]\n",
      "2022-05-03 03:34:02,902 INFO: Train Loss:0.166 | Acc:0.9568 | F1:0.9442\n",
      "2022-05-03 03:34:22,310 INFO: val Loss:0.060 | Acc:0.9783 | F1:0.9724\n",
      "100%|██████████| 312/312 [02:24<00:00,  2.16it/s]\n",
      "2022-05-03 03:36:46,443 INFO: Epoch:[044/100]\n",
      "2022-05-03 03:36:46,443 INFO: Train Loss:0.148 | Acc:0.9528 | F1:0.9478\n",
      "2022-05-03 03:37:04,583 INFO: val Loss:0.107 | Acc:0.9542 | F1:0.9561\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 03:39:32,301 INFO: Epoch:[045/100]\n",
      "2022-05-03 03:39:32,302 INFO: Train Loss:0.139 | Acc:0.9566 | F1:0.9500\n",
      "2022-05-03 03:39:51,263 INFO: val Loss:0.077 | Acc:0.9405 | F1:0.9693\n",
      "100%|██████████| 312/312 [02:22<00:00,  2.19it/s]\n",
      "2022-05-03 03:42:13,705 INFO: Epoch:[046/100]\n",
      "2022-05-03 03:42:13,706 INFO: Train Loss:0.156 | Acc:0.9616 | F1:0.9500\n",
      "2022-05-03 03:42:32,637 INFO: val Loss:0.018 | Acc:0.9936 | F1:0.9926\n",
      "2022-05-03 03:42:33,426 INFO: -----------------SAVE:46epoch----------------\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 03:45:01,693 INFO: Epoch:[047/100]\n",
      "2022-05-03 03:45:01,693 INFO: Train Loss:0.121 | Acc:0.9650 | F1:0.9537\n",
      "2022-05-03 03:45:20,484 INFO: val Loss:0.040 | Acc:0.9879 | F1:0.9804\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 03:47:46,448 INFO: Epoch:[048/100]\n",
      "2022-05-03 03:47:46,449 INFO: Train Loss:0.131 | Acc:0.9672 | F1:0.9570\n",
      "2022-05-03 03:48:04,922 INFO: val Loss:0.122 | Acc:0.9791 | F1:0.9656\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 03:50:31,205 INFO: Epoch:[049/100]\n",
      "2022-05-03 03:50:31,206 INFO: Train Loss:0.130 | Acc:0.9650 | F1:0.9553\n",
      "2022-05-03 03:50:50,438 INFO: val Loss:0.069 | Acc:0.9678 | F1:0.9697\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.14it/s]\n",
      "2022-05-03 03:53:16,526 INFO: Epoch:[050/100]\n",
      "2022-05-03 03:53:16,527 INFO: Train Loss:0.132 | Acc:0.9596 | F1:0.9473\n",
      "2022-05-03 03:53:35,291 INFO: val Loss:0.034 | Acc:0.9895 | F1:0.9844\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 03:56:03,182 INFO: Epoch:[051/100]\n",
      "2022-05-03 03:56:03,183 INFO: Train Loss:0.136 | Acc:0.9672 | F1:0.9576\n",
      "2022-05-03 03:56:22,224 INFO: val Loss:0.040 | Acc:0.9904 | F1:0.9910\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 03:58:48,901 INFO: Epoch:[052/100]\n",
      "2022-05-03 03:58:48,902 INFO: Train Loss:0.130 | Acc:0.9668 | F1:0.9539\n",
      "2022-05-03 03:59:07,100 INFO: val Loss:0.092 | Acc:0.9783 | F1:0.9720\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:01:34,430 INFO: Epoch:[053/100]\n",
      "2022-05-03 04:01:34,431 INFO: Train Loss:0.124 | Acc:0.9620 | F1:0.9528\n",
      "2022-05-03 04:01:52,916 INFO: val Loss:0.055 | Acc:0.9751 | F1:0.9738\n",
      "2022-05-03 04:01:52,918 INFO: \n",
      "Best Val Epoch:46 | Val Loss:0.0183 | Val Acc:0.9936 | Val F1:0.9926\n",
      "2022-05-03 04:01:52,919 INFO: Total Process time:163.059Minute\n",
      "2022-05-03 04:01:52,939 INFO: {'exp_num': '4', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:4977\n",
      "Dataset size:1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 04:01:53,388 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:04:20,991 INFO: Epoch:[001/100]\n",
      "2022-05-03 04:04:20,992 INFO: Train Loss:4.527 | Acc:0.0141 | F1:0.0060\n",
      "2022-05-03 04:04:39,283 INFO: val Loss:4.264 | Acc:0.0844 | F1:0.0201\n",
      "2022-05-03 04:04:40,047 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 04:07:08,522 INFO: Epoch:[002/100]\n",
      "2022-05-03 04:07:08,523 INFO: Train Loss:4.135 | Acc:0.1336 | F1:0.0630\n",
      "2022-05-03 04:07:27,213 INFO: val Loss:3.418 | Acc:0.3923 | F1:0.1586\n",
      "2022-05-03 04:07:27,927 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 04:09:53,622 INFO: Epoch:[003/100]\n",
      "2022-05-03 04:09:53,623 INFO: Train Loss:3.497 | Acc:0.3518 | F1:0.1543\n",
      "2022-05-03 04:10:12,808 INFO: val Loss:2.604 | Acc:0.5072 | F1:0.1775\n",
      "2022-05-03 04:10:13,570 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 04:12:40,153 INFO: Epoch:[004/100]\n",
      "2022-05-03 04:12:40,154 INFO: Train Loss:2.890 | Acc:0.3685 | F1:0.1901\n",
      "2022-05-03 04:12:59,632 INFO: val Loss:2.117 | Acc:0.4084 | F1:0.1893\n",
      "2022-05-03 04:13:00,345 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 04:15:26,892 INFO: Epoch:[005/100]\n",
      "2022-05-03 04:15:26,893 INFO: Train Loss:2.525 | Acc:0.3446 | F1:0.2221\n",
      "2022-05-03 04:15:44,322 INFO: val Loss:1.886 | Acc:0.3690 | F1:0.2021\n",
      "2022-05-03 04:15:45,132 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:18:12,439 INFO: Epoch:[006/100]\n",
      "2022-05-03 04:18:12,440 INFO: Train Loss:2.297 | Acc:0.3956 | F1:0.2568\n",
      "2022-05-03 04:18:31,107 INFO: val Loss:1.684 | Acc:0.4164 | F1:0.2303\n",
      "2022-05-03 04:18:31,925 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 312/312 [02:30<00:00,  2.07it/s]\n",
      "2022-05-03 04:21:02,668 INFO: Epoch:[007/100]\n",
      "2022-05-03 04:21:02,669 INFO: Train Loss:2.124 | Acc:0.4298 | F1:0.2979\n",
      "2022-05-03 04:21:22,483 INFO: val Loss:1.487 | Acc:0.5707 | F1:0.3459\n",
      "2022-05-03 04:21:23,192 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 312/312 [02:22<00:00,  2.19it/s]\n",
      "2022-05-03 04:23:45,866 INFO: Epoch:[008/100]\n",
      "2022-05-03 04:23:45,867 INFO: Train Loss:1.969 | Acc:0.4919 | F1:0.3439\n",
      "2022-05-03 04:24:04,202 INFO: val Loss:1.395 | Acc:0.6061 | F1:0.3411\n",
      "2022-05-03 04:24:04,893 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:26:32,379 INFO: Epoch:[009/100]\n",
      "2022-05-03 04:26:32,381 INFO: Train Loss:1.830 | Acc:0.5473 | F1:0.3918\n",
      "2022-05-03 04:26:51,242 INFO: val Loss:1.265 | Acc:0.6736 | F1:0.4280\n",
      "2022-05-03 04:26:51,955 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:29:19,329 INFO: Epoch:[010/100]\n",
      "2022-05-03 04:29:19,330 INFO: Train Loss:1.736 | Acc:0.5724 | F1:0.4401\n",
      "2022-05-03 04:29:38,318 INFO: val Loss:1.114 | Acc:0.7307 | F1:0.5088\n",
      "2022-05-03 04:29:39,061 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.11it/s]\n",
      "2022-05-03 04:32:07,114 INFO: Epoch:[011/100]\n",
      "2022-05-03 04:32:07,115 INFO: Train Loss:1.532 | Acc:0.6180 | F1:0.4952\n",
      "2022-05-03 04:32:26,127 INFO: val Loss:0.993 | Acc:0.7363 | F1:0.5367\n",
      "2022-05-03 04:32:26,854 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:34:54,243 INFO: Epoch:[012/100]\n",
      "2022-05-03 04:34:54,244 INFO: Train Loss:1.443 | Acc:0.6693 | F1:0.5503\n",
      "2022-05-03 04:35:12,807 INFO: val Loss:0.887 | Acc:0.7588 | F1:0.5921\n",
      "2022-05-03 04:35:13,516 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.12it/s]\n",
      "2022-05-03 04:37:40,465 INFO: Epoch:[013/100]\n",
      "2022-05-03 04:37:40,466 INFO: Train Loss:1.277 | Acc:0.7050 | F1:0.5904\n",
      "2022-05-03 04:37:58,814 INFO: val Loss:0.757 | Acc:0.7428 | F1:0.6483\n",
      "2022-05-03 04:37:59,515 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 04:40:27,402 INFO: Epoch:[014/100]\n",
      "2022-05-03 04:40:27,402 INFO: Train Loss:1.209 | Acc:0.7177 | F1:0.6234\n",
      "2022-05-03 04:40:46,287 INFO: val Loss:0.718 | Acc:0.8055 | F1:0.7048\n",
      "2022-05-03 04:40:46,995 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 04:43:15,765 INFO: Epoch:[015/100]\n",
      "2022-05-03 04:43:15,767 INFO: Train Loss:1.091 | Acc:0.7482 | F1:0.6592\n",
      "2022-05-03 04:43:34,744 INFO: val Loss:0.621 | Acc:0.7942 | F1:0.7388\n",
      "2022-05-03 04:43:35,443 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 312/312 [02:23<00:00,  2.17it/s]\n",
      "2022-05-03 04:45:59,014 INFO: Epoch:[016/100]\n",
      "2022-05-03 04:45:59,015 INFO: Train Loss:1.002 | Acc:0.7707 | F1:0.6981\n",
      "2022-05-03 04:46:17,501 INFO: val Loss:0.543 | Acc:0.7814 | F1:0.7573\n",
      "2022-05-03 04:46:18,189 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 04:48:45,733 INFO: Epoch:[017/100]\n",
      "2022-05-03 04:48:45,734 INFO: Train Loss:0.876 | Acc:0.7880 | F1:0.7240\n",
      "2022-05-03 04:49:04,346 INFO: val Loss:0.459 | Acc:0.8818 | F1:0.8003\n",
      "2022-05-03 04:49:05,076 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 04:51:32,186 INFO: Epoch:[018/100]\n",
      "2022-05-03 04:51:32,187 INFO: Train Loss:0.838 | Acc:0.8164 | F1:0.7469\n",
      "2022-05-03 04:51:50,843 INFO: val Loss:0.413 | Acc:0.8915 | F1:0.8460\n",
      "2022-05-03 04:51:51,552 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 04:54:17,390 INFO: Epoch:[019/100]\n",
      "2022-05-03 04:54:17,391 INFO: Train Loss:0.717 | Acc:0.8192 | F1:0.7705\n",
      "2022-05-03 04:54:35,787 INFO: val Loss:0.370 | Acc:0.8915 | F1:0.8483\n",
      "2022-05-03 04:54:36,476 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 312/312 [02:24<00:00,  2.15it/s]\n",
      "2022-05-03 04:57:01,320 INFO: Epoch:[020/100]\n",
      "2022-05-03 04:57:01,320 INFO: Train Loss:0.682 | Acc:0.8441 | F1:0.7966\n",
      "2022-05-03 04:57:20,252 INFO: val Loss:0.310 | Acc:0.9124 | F1:0.8561\n",
      "2022-05-03 04:57:20,913 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 04:59:47,399 INFO: Epoch:[021/100]\n",
      "2022-05-03 04:59:47,400 INFO: Train Loss:0.621 | Acc:0.8583 | F1:0.8141\n",
      "2022-05-03 05:00:06,327 INFO: val Loss:0.263 | Acc:0.9252 | F1:0.8838\n",
      "2022-05-03 05:00:07,041 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 05:02:34,553 INFO: Epoch:[022/100]\n",
      "2022-05-03 05:02:34,553 INFO: Train Loss:0.555 | Acc:0.8708 | F1:0.8311\n",
      "2022-05-03 05:02:53,208 INFO: val Loss:0.229 | Acc:0.9293 | F1:0.9001\n",
      "2022-05-03 05:02:53,924 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 312/312 [02:20<00:00,  2.21it/s]\n",
      "2022-05-03 05:05:14,860 INFO: Epoch:[023/100]\n",
      "2022-05-03 05:05:14,861 INFO: Train Loss:0.480 | Acc:0.8859 | F1:0.8532\n",
      "2022-05-03 05:05:33,592 INFO: val Loss:0.230 | Acc:0.9268 | F1:0.9055\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.14it/s]\n",
      "2022-05-03 05:07:59,725 INFO: Epoch:[024/100]\n",
      "2022-05-03 05:07:59,725 INFO: Train Loss:0.465 | Acc:0.8883 | F1:0.8599\n",
      "2022-05-03 05:08:18,762 INFO: val Loss:0.275 | Acc:0.8722 | F1:0.8844\n",
      "100%|██████████| 312/312 [02:29<00:00,  2.09it/s]\n",
      "2022-05-03 05:10:48,201 INFO: Epoch:[025/100]\n",
      "2022-05-03 05:10:48,202 INFO: Train Loss:0.439 | Acc:0.8915 | F1:0.8669\n",
      "2022-05-03 05:11:06,838 INFO: val Loss:0.162 | Acc:0.9413 | F1:0.9268\n",
      "2022-05-03 05:11:07,577 INFO: -----------------SAVE:25epoch----------------\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 05:13:36,374 INFO: Epoch:[026/100]\n",
      "2022-05-03 05:13:36,375 INFO: Train Loss:0.393 | Acc:0.9100 | F1:0.8851\n",
      "2022-05-03 05:13:56,126 INFO: val Loss:0.144 | Acc:0.9349 | F1:0.9496\n",
      "2022-05-03 05:13:56,876 INFO: -----------------SAVE:26epoch----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 05:16:22,611 INFO: Epoch:[027/100]\n",
      "2022-05-03 05:16:22,612 INFO: Train Loss:0.371 | Acc:0.9009 | F1:0.8792\n",
      "2022-05-03 05:16:41,192 INFO: val Loss:0.090 | Acc:0.9751 | F1:0.9682\n",
      "2022-05-03 05:16:41,943 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.15it/s]\n",
      "2022-05-03 05:19:07,216 INFO: Epoch:[028/100]\n",
      "2022-05-03 05:19:07,216 INFO: Train Loss:0.357 | Acc:0.9170 | F1:0.8985\n",
      "2022-05-03 05:19:24,940 INFO: val Loss:0.160 | Acc:0.9333 | F1:0.9435\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.11it/s]\n",
      "2022-05-03 05:21:53,164 INFO: Epoch:[029/100]\n",
      "2022-05-03 05:21:53,165 INFO: Train Loss:0.313 | Acc:0.9108 | F1:0.8997\n",
      "2022-05-03 05:22:11,781 INFO: val Loss:0.078 | Acc:0.9767 | F1:0.9682\n",
      "2022-05-03 05:22:12,491 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 05:24:38,679 INFO: Epoch:[030/100]\n",
      "2022-05-03 05:24:38,679 INFO: Train Loss:0.287 | Acc:0.9265 | F1:0.9065\n",
      "2022-05-03 05:24:56,580 INFO: val Loss:0.243 | Acc:0.9196 | F1:0.9027\n",
      "100%|██████████| 312/312 [02:23<00:00,  2.17it/s]\n",
      "2022-05-03 05:27:20,454 INFO: Epoch:[031/100]\n",
      "2022-05-03 05:27:20,455 INFO: Train Loss:0.272 | Acc:0.9257 | F1:0.9120\n",
      "2022-05-03 05:27:38,605 INFO: val Loss:0.075 | Acc:0.9823 | F1:0.9754\n",
      "2022-05-03 05:27:39,311 INFO: -----------------SAVE:31epoch----------------\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 05:30:06,910 INFO: Epoch:[032/100]\n",
      "2022-05-03 05:30:06,910 INFO: Train Loss:0.264 | Acc:0.9236 | F1:0.9168\n",
      "2022-05-03 05:30:25,176 INFO: val Loss:0.089 | Acc:0.9727 | F1:0.9665\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 05:32:52,297 INFO: Epoch:[033/100]\n",
      "2022-05-03 05:32:52,297 INFO: Train Loss:0.256 | Acc:0.9305 | F1:0.9202\n",
      "2022-05-03 05:33:10,869 INFO: val Loss:0.085 | Acc:0.9775 | F1:0.9726\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 05:35:39,253 INFO: Epoch:[034/100]\n",
      "2022-05-03 05:35:39,254 INFO: Train Loss:0.238 | Acc:0.9359 | F1:0.9259\n",
      "2022-05-03 05:35:57,765 INFO: val Loss:0.077 | Acc:0.9686 | F1:0.9629\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.12it/s]\n",
      "2022-05-03 05:38:24,674 INFO: Epoch:[035/100]\n",
      "2022-05-03 05:38:24,675 INFO: Train Loss:0.238 | Acc:0.9359 | F1:0.9235\n",
      "2022-05-03 05:38:42,999 INFO: val Loss:0.053 | Acc:0.9799 | F1:0.9695\n",
      "2022-05-03 05:38:43,728 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 05:41:10,069 INFO: Epoch:[036/100]\n",
      "2022-05-03 05:41:10,070 INFO: Train Loss:0.225 | Acc:0.9407 | F1:0.9256\n",
      "2022-05-03 05:41:28,428 INFO: val Loss:0.055 | Acc:0.9711 | F1:0.9682\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 05:43:54,855 INFO: Epoch:[037/100]\n",
      "2022-05-03 05:43:54,856 INFO: Train Loss:0.190 | Acc:0.9478 | F1:0.9398\n",
      "2022-05-03 05:44:14,399 INFO: val Loss:0.075 | Acc:0.9759 | F1:0.9714\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.11it/s]\n",
      "2022-05-03 05:46:41,965 INFO: Epoch:[038/100]\n",
      "2022-05-03 05:46:41,966 INFO: Train Loss:0.196 | Acc:0.9363 | F1:0.9294\n",
      "2022-05-03 05:47:00,853 INFO: val Loss:0.054 | Acc:0.9646 | F1:0.9703\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 05:49:27,225 INFO: Epoch:[039/100]\n",
      "2022-05-03 05:49:27,228 INFO: Train Loss:0.178 | Acc:0.9512 | F1:0.9417\n",
      "2022-05-03 05:49:45,385 INFO: val Loss:0.044 | Acc:0.9831 | F1:0.9767\n",
      "2022-05-03 05:49:46,076 INFO: -----------------SAVE:39epoch----------------\n",
      "100%|██████████| 312/312 [02:23<00:00,  2.18it/s]\n",
      "2022-05-03 05:52:09,380 INFO: Epoch:[040/100]\n",
      "2022-05-03 05:52:09,381 INFO: Train Loss:0.180 | Acc:0.9494 | F1:0.9423\n",
      "2022-05-03 05:52:27,522 INFO: val Loss:0.027 | Acc:0.9920 | F1:0.9919\n",
      "2022-05-03 05:52:28,223 INFO: -----------------SAVE:40epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.15it/s]\n",
      "2022-05-03 05:54:53,482 INFO: Epoch:[041/100]\n",
      "2022-05-03 05:54:53,483 INFO: Train Loss:0.182 | Acc:0.9548 | F1:0.9381\n",
      "2022-05-03 05:55:12,214 INFO: val Loss:0.054 | Acc:0.9735 | F1:0.9770\n",
      "100%|██████████| 312/312 [02:28<00:00,  2.10it/s]\n",
      "2022-05-03 05:57:40,603 INFO: Epoch:[042/100]\n",
      "2022-05-03 05:57:40,604 INFO: Train Loss:0.182 | Acc:0.9556 | F1:0.9406\n",
      "2022-05-03 05:57:59,059 INFO: val Loss:0.056 | Acc:0.9823 | F1:0.9781\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.15it/s]\n",
      "2022-05-03 06:00:24,267 INFO: Epoch:[043/100]\n",
      "2022-05-03 06:00:24,268 INFO: Train Loss:0.159 | Acc:0.9550 | F1:0.9437\n",
      "2022-05-03 06:00:42,958 INFO: val Loss:0.064 | Acc:0.9799 | F1:0.9725\n",
      "100%|██████████| 312/312 [02:23<00:00,  2.17it/s]\n",
      "2022-05-03 06:03:06,499 INFO: Epoch:[044/100]\n",
      "2022-05-03 06:03:06,500 INFO: Train Loss:0.141 | Acc:0.9636 | F1:0.9552\n",
      "2022-05-03 06:03:25,928 INFO: val Loss:0.047 | Acc:0.9807 | F1:0.9778\n",
      "100%|██████████| 312/312 [02:23<00:00,  2.18it/s]\n",
      "2022-05-03 06:05:49,210 INFO: Epoch:[045/100]\n",
      "2022-05-03 06:05:49,211 INFO: Train Loss:0.149 | Acc:0.9636 | F1:0.9539\n",
      "2022-05-03 06:06:07,315 INFO: val Loss:0.024 | Acc:0.9928 | F1:0.9931\n",
      "2022-05-03 06:06:07,992 INFO: -----------------SAVE:45epoch----------------\n",
      "100%|██████████| 312/312 [02:25<00:00,  2.14it/s]\n",
      "2022-05-03 06:08:33,781 INFO: Epoch:[046/100]\n",
      "2022-05-03 06:08:33,782 INFO: Train Loss:0.154 | Acc:0.9604 | F1:0.9490\n",
      "2022-05-03 06:08:52,302 INFO: val Loss:0.046 | Acc:0.9831 | F1:0.9844\n",
      "100%|██████████| 312/312 [02:23<00:00,  2.17it/s]\n",
      "2022-05-03 06:11:16,274 INFO: Epoch:[047/100]\n",
      "2022-05-03 06:11:16,275 INFO: Train Loss:0.142 | Acc:0.9626 | F1:0.9504\n",
      "2022-05-03 06:11:35,313 INFO: val Loss:0.045 | Acc:0.9791 | F1:0.9798\n",
      "100%|██████████| 312/312 [02:24<00:00,  2.15it/s]\n",
      "2022-05-03 06:14:00,286 INFO: Epoch:[048/100]\n",
      "2022-05-03 06:14:00,287 INFO: Train Loss:0.144 | Acc:0.9642 | F1:0.9539\n",
      "2022-05-03 06:14:18,371 INFO: val Loss:0.041 | Acc:0.9895 | F1:0.9860\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 06:16:45,433 INFO: Epoch:[049/100]\n",
      "2022-05-03 06:16:45,433 INFO: Train Loss:0.148 | Acc:0.9628 | F1:0.9523\n",
      "2022-05-03 06:17:03,295 INFO: val Loss:0.032 | Acc:0.9871 | F1:0.9852\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 06:19:29,810 INFO: Epoch:[050/100]\n",
      "2022-05-03 06:19:29,811 INFO: Train Loss:0.117 | Acc:0.9666 | F1:0.9565\n",
      "2022-05-03 06:19:47,932 INFO: val Loss:0.040 | Acc:0.9887 | F1:0.9855\n",
      "100%|██████████| 312/312 [02:26<00:00,  2.13it/s]\n",
      "2022-05-03 06:22:14,288 INFO: Epoch:[051/100]\n",
      "2022-05-03 06:22:14,289 INFO: Train Loss:0.114 | Acc:0.9681 | F1:0.9601\n",
      "2022-05-03 06:22:33,323 INFO: val Loss:0.097 | Acc:0.9807 | F1:0.9707\n",
      "100%|██████████| 312/312 [02:27<00:00,  2.12it/s]\n",
      "2022-05-03 06:25:00,558 INFO: Epoch:[052/100]\n",
      "2022-05-03 06:25:00,558 INFO: Train Loss:0.110 | Acc:0.9668 | F1:0.9563\n",
      "2022-05-03 06:25:19,016 INFO: val Loss:0.062 | Acc:0.9839 | F1:0.9827\n",
      "2022-05-03 06:25:19,017 INFO: \n",
      "Best Val Epoch:45 | Val Loss:0.0243 | Val Acc:0.9928 | Val F1:0.9931\n",
      "2022-05-03 06:25:19,018 INFO: Total Process time:143.422Minute\n"
     ]
    }
   ],
   "source": [
    "args.step = 0\n",
    "models_path = []\n",
    "for s_fold in range(5): # 5fold\n",
    "    args.fold = s_fold\n",
    "    args.exp_num = str(s_fold)\n",
    "    save_path = main(args)\n",
    "    models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size:2154\n"
     ]
    }
   ],
   "source": [
    "img_size = 512\n",
    "\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = ['./label_results/000', './label_results/001', './label_results/002', './label_results/003', './label_results/004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 06:25:19,486 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:53<00:00,  3.34s/it]\n",
      "2022-05-03 06:27:14,634 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:45<00:00,  3.11s/it]\n",
      "2022-05-03 06:29:01,988 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:49<00:00,  3.23s/it]\n",
      "2022-05-03 06:30:52,837 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:46<00:00,  3.13s/it]\n",
      "2022-05-03 06:32:40,406 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:51<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble = ensemble_5fold(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62,\n",
       " 28,\n",
       " 72,\n",
       " 64,\n",
       " 63,\n",
       " 50,\n",
       " 14,\n",
       " 32,\n",
       " 40,\n",
       " 39,\n",
       " 0,\n",
       " 76,\n",
       " 85,\n",
       " 60,\n",
       " 62,\n",
       " 2,\n",
       " 25,\n",
       " 22,\n",
       " 21,\n",
       " 47,\n",
       " 66,\n",
       " 84,\n",
       " 0,\n",
       " 58,\n",
       " 51,\n",
       " 55,\n",
       " 51,\n",
       " 60,\n",
       " 40,\n",
       " 78,\n",
       " 55,\n",
       " 9,\n",
       " 59,\n",
       " 83,\n",
       " 84,\n",
       " 82,\n",
       " 40,\n",
       " 9,\n",
       " 37,\n",
       " 44,\n",
       " 45,\n",
       " 10,\n",
       " 60,\n",
       " 14,\n",
       " 44,\n",
       " 63,\n",
       " 68,\n",
       " 13,\n",
       " 84,\n",
       " 55,\n",
       " 9,\n",
       " 72,\n",
       " 31,\n",
       " 35,\n",
       " 84,\n",
       " 46,\n",
       " 20,\n",
       " 21,\n",
       " 38,\n",
       " 15,\n",
       " 69,\n",
       " 48,\n",
       " 18,\n",
       " 87,\n",
       " 13,\n",
       " 36,\n",
       " 72,\n",
       " 13,\n",
       " 53,\n",
       " 43,\n",
       " 25,\n",
       " 76,\n",
       " 32,\n",
       " 63,\n",
       " 41,\n",
       " 84,\n",
       " 9,\n",
       " 40,\n",
       " 5,\n",
       " 9,\n",
       " 50,\n",
       " 76,\n",
       " 57,\n",
       " 75,\n",
       " 72,\n",
       " 1,\n",
       " 84,\n",
       " 22,\n",
       " 40,\n",
       " 83,\n",
       " 36,\n",
       " 84,\n",
       " 9,\n",
       " 42,\n",
       " 55,\n",
       " 65,\n",
       " 72,\n",
       " 72,\n",
       " 45,\n",
       " 45,\n",
       " 9,\n",
       " 24,\n",
       " 52,\n",
       " 17,\n",
       " 76,\n",
       " 2,\n",
       " 86,\n",
       " 49,\n",
       " 72,\n",
       " 34,\n",
       " 36,\n",
       " 58,\n",
       " 72,\n",
       " 72,\n",
       " 84,\n",
       " 15,\n",
       " 28,\n",
       " 52,\n",
       " 15,\n",
       " 41,\n",
       " 55,\n",
       " 19,\n",
       " 45,\n",
       " 72,\n",
       " 77,\n",
       " 72,\n",
       " 61,\n",
       " 72,\n",
       " 54,\n",
       " 55,\n",
       " 28,\n",
       " 46,\n",
       " 50,\n",
       " 42,\n",
       " 15,\n",
       " 25,\n",
       " 57,\n",
       " 2,\n",
       " 52,\n",
       " 78,\n",
       " 73,\n",
       " 33,\n",
       " 7,\n",
       " 9,\n",
       " 84,\n",
       " 35,\n",
       " 23,\n",
       " 84,\n",
       " 85,\n",
       " 76,\n",
       " 84,\n",
       " 79,\n",
       " 72,\n",
       " 52,\n",
       " 83,\n",
       " 63,\n",
       " 68,\n",
       " 63,\n",
       " 33,\n",
       " 45,\n",
       " 57,\n",
       " 60,\n",
       " 57,\n",
       " 6,\n",
       " 21,\n",
       " 38,\n",
       " 49,\n",
       " 74,\n",
       " 83,\n",
       " 10,\n",
       " 21,\n",
       " 24,\n",
       " 56,\n",
       " 39,\n",
       " 9,\n",
       " 52,\n",
       " 55,\n",
       " 86,\n",
       " 15,\n",
       " 84,\n",
       " 40,\n",
       " 27,\n",
       " 3,\n",
       " 11,\n",
       " 51,\n",
       " 20,\n",
       " 62,\n",
       " 45,\n",
       " 79,\n",
       " 70,\n",
       " 52,\n",
       " 18,\n",
       " 84,\n",
       " 32,\n",
       " 52,\n",
       " 57,\n",
       " 83,\n",
       " 85,\n",
       " 84,\n",
       " 33,\n",
       " 72,\n",
       " 12,\n",
       " 58,\n",
       " 1,\n",
       " 52,\n",
       " 40,\n",
       " 47,\n",
       " 52,\n",
       " 33,\n",
       " 25,\n",
       " 41,\n",
       " 15,\n",
       " 55,\n",
       " 76,\n",
       " 48,\n",
       " 20,\n",
       " 18,\n",
       " 49,\n",
       " 72,\n",
       " 1,\n",
       " 65,\n",
       " 28,\n",
       " 63,\n",
       " 1,\n",
       " 16,\n",
       " 43,\n",
       " 51,\n",
       " 36,\n",
       " 9,\n",
       " 72,\n",
       " 52,\n",
       " 55,\n",
       " 81,\n",
       " 49,\n",
       " 80,\n",
       " 17,\n",
       " 72,\n",
       " 45,\n",
       " 31,\n",
       " 14,\n",
       " 45,\n",
       " 34,\n",
       " 85,\n",
       " 60,\n",
       " 77,\n",
       " 41,\n",
       " 63,\n",
       " 64,\n",
       " 79,\n",
       " 9,\n",
       " 72,\n",
       " 64,\n",
       " 4,\n",
       " 15,\n",
       " 44,\n",
       " 45,\n",
       " 45,\n",
       " 13,\n",
       " 86,\n",
       " 33,\n",
       " 31,\n",
       " 55,\n",
       " 33,\n",
       " 32,\n",
       " 41,\n",
       " 40,\n",
       " 76,\n",
       " 55,\n",
       " 44,\n",
       " 9,\n",
       " 24,\n",
       " 55,\n",
       " 77,\n",
       " 72,\n",
       " 9,\n",
       " 16,\n",
       " 47,\n",
       " 15,\n",
       " 73,\n",
       " 87,\n",
       " 48,\n",
       " 36,\n",
       " 72,\n",
       " 53,\n",
       " 52,\n",
       " 55,\n",
       " 30,\n",
       " 63,\n",
       " 63,\n",
       " 0,\n",
       " 22,\n",
       " 1,\n",
       " 38,\n",
       " 37,\n",
       " 9,\n",
       " 87,\n",
       " 63,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 58,\n",
       " 41,\n",
       " 80,\n",
       " 72,\n",
       " 57,\n",
       " 51,\n",
       " 68,\n",
       " 28,\n",
       " 36,\n",
       " 58,\n",
       " 55,\n",
       " 72,\n",
       " 3,\n",
       " 46,\n",
       " 76,\n",
       " 73,\n",
       " 59,\n",
       " 76,\n",
       " 76,\n",
       " 63,\n",
       " 9,\n",
       " 58,\n",
       " 46,\n",
       " 7,\n",
       " 40,\n",
       " 15,\n",
       " 55,\n",
       " 56,\n",
       " 50,\n",
       " 72,\n",
       " 58,\n",
       " 15,\n",
       " 55,\n",
       " 21,\n",
       " 19,\n",
       " 3,\n",
       " 63,\n",
       " 43,\n",
       " 58,\n",
       " 4,\n",
       " 76,\n",
       " 55,\n",
       " 42,\n",
       " 63,\n",
       " 55,\n",
       " 39,\n",
       " 44,\n",
       " 42,\n",
       " 33,\n",
       " 53,\n",
       " 24,\n",
       " 5,\n",
       " 9,\n",
       " 17,\n",
       " 21,\n",
       " 16,\n",
       " 9,\n",
       " 52,\n",
       " 3,\n",
       " 67,\n",
       " 70,\n",
       " 13,\n",
       " 38,\n",
       " 19,\n",
       " 72,\n",
       " 80,\n",
       " 77,\n",
       " 7,\n",
       " 9,\n",
       " 37,\n",
       " 33,\n",
       " 55,\n",
       " 9,\n",
       " 3,\n",
       " 33,\n",
       " 70,\n",
       " 77,\n",
       " 55,\n",
       " 14,\n",
       " 28,\n",
       " 46,\n",
       " 55,\n",
       " 74,\n",
       " 62,\n",
       " 15,\n",
       " 28,\n",
       " 42,\n",
       " 15,\n",
       " 32,\n",
       " 55,\n",
       " 40,\n",
       " 43,\n",
       " 1,\n",
       " 37,\n",
       " 13,\n",
       " 69,\n",
       " 9,\n",
       " 2,\n",
       " 35,\n",
       " 63,\n",
       " 78,\n",
       " 64,\n",
       " 72,\n",
       " 81,\n",
       " 2,\n",
       " 63,\n",
       " 50,\n",
       " 17,\n",
       " 40,\n",
       " 52,\n",
       " 9,\n",
       " 16,\n",
       " 52,\n",
       " 9,\n",
       " 1,\n",
       " 72,\n",
       " 40,\n",
       " 72,\n",
       " 32,\n",
       " 70,\n",
       " 46,\n",
       " 84,\n",
       " 42,\n",
       " 21,\n",
       " 19,\n",
       " 50,\n",
       " 84,\n",
       " 84,\n",
       " 41,\n",
       " 71,\n",
       " 1,\n",
       " 84,\n",
       " 9,\n",
       " 1,\n",
       " 63,\n",
       " 31,\n",
       " 79,\n",
       " 51,\n",
       " 21,\n",
       " 27,\n",
       " 47,\n",
       " 65,\n",
       " 72,\n",
       " 55,\n",
       " 73,\n",
       " 70,\n",
       " 24,\n",
       " 86,\n",
       " 62,\n",
       " 38,\n",
       " 77,\n",
       " 84,\n",
       " 1,\n",
       " 21,\n",
       " 63,\n",
       " 33,\n",
       " 59,\n",
       " 33,\n",
       " 22,\n",
       " 44,\n",
       " 9,\n",
       " 21,\n",
       " 60,\n",
       " 41,\n",
       " 24,\n",
       " 14,\n",
       " 13,\n",
       " 44,\n",
       " 59,\n",
       " 42,\n",
       " 53,\n",
       " 42,\n",
       " 63,\n",
       " 3,\n",
       " 56,\n",
       " 9,\n",
       " 76,\n",
       " 72,\n",
       " 60,\n",
       " 60,\n",
       " 50,\n",
       " 55,\n",
       " 22,\n",
       " 9,\n",
       " 55,\n",
       " 55,\n",
       " 9,\n",
       " 28,\n",
       " 34,\n",
       " 51,\n",
       " 20,\n",
       " 6,\n",
       " 7,\n",
       " 55,\n",
       " 33,\n",
       " 15,\n",
       " 70,\n",
       " 67,\n",
       " 7,\n",
       " 86,\n",
       " 55,\n",
       " 65,\n",
       " 46,\n",
       " 72,\n",
       " 4,\n",
       " 63,\n",
       " 38,\n",
       " 78,\n",
       " 68,\n",
       " 41,\n",
       " 16,\n",
       " 56,\n",
       " 73,\n",
       " 72,\n",
       " 72,\n",
       " 84,\n",
       " 9,\n",
       " 46,\n",
       " 40,\n",
       " 9,\n",
       " 64,\n",
       " 53,\n",
       " 76,\n",
       " 45,\n",
       " 52,\n",
       " 82,\n",
       " 11,\n",
       " 76,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 34,\n",
       " 33,\n",
       " 34,\n",
       " 28,\n",
       " 55,\n",
       " 86,\n",
       " 79,\n",
       " 49,\n",
       " 67,\n",
       " 9,\n",
       " 76,\n",
       " 26,\n",
       " 55,\n",
       " 28,\n",
       " 87,\n",
       " 72,\n",
       " 63,\n",
       " 33,\n",
       " 54,\n",
       " 9,\n",
       " 13,\n",
       " 47,\n",
       " 52,\n",
       " 81,\n",
       " 62,\n",
       " 55,\n",
       " 17,\n",
       " 81,\n",
       " 55,\n",
       " 39,\n",
       " 14,\n",
       " 40,\n",
       " 23,\n",
       " 9,\n",
       " 33,\n",
       " 28,\n",
       " 28,\n",
       " 33,\n",
       " 59,\n",
       " 33,\n",
       " 79,\n",
       " 33,\n",
       " 54,\n",
       " 11,\n",
       " 77,\n",
       " 86,\n",
       " 21,\n",
       " 27,\n",
       " 82,\n",
       " 49,\n",
       " 42,\n",
       " 63,\n",
       " 20,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 21,\n",
       " 40,\n",
       " 72,\n",
       " 55,\n",
       " 32,\n",
       " 9,\n",
       " 86,\n",
       " 84,\n",
       " 42,\n",
       " 23,\n",
       " 63,\n",
       " 86,\n",
       " 33,\n",
       " 40,\n",
       " 33,\n",
       " 34,\n",
       " 7,\n",
       " 50,\n",
       " 9,\n",
       " 52,\n",
       " 9,\n",
       " 70,\n",
       " 51,\n",
       " 42,\n",
       " 27,\n",
       " 9,\n",
       " 21,\n",
       " 33,\n",
       " 16,\n",
       " 52,\n",
       " 86,\n",
       " 24,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 84,\n",
       " 62,\n",
       " 0,\n",
       " 22,\n",
       " 42,\n",
       " 68,\n",
       " 33,\n",
       " 1,\n",
       " 40,\n",
       " 67,\n",
       " 55,\n",
       " 58,\n",
       " 86,\n",
       " 40,\n",
       " 11,\n",
       " 15,\n",
       " 23,\n",
       " 41,\n",
       " 52,\n",
       " 37,\n",
       " 72,\n",
       " 0,\n",
       " 21,\n",
       " 83,\n",
       " 9,\n",
       " 37,\n",
       " 15,\n",
       " 86,\n",
       " 75,\n",
       " 35,\n",
       " 21,\n",
       " 55,\n",
       " 29,\n",
       " 79,\n",
       " 58,\n",
       " 13,\n",
       " 15,\n",
       " 40,\n",
       " 72,\n",
       " 8,\n",
       " 21,\n",
       " 70,\n",
       " 15,\n",
       " 28,\n",
       " 70,\n",
       " 34,\n",
       " 66,\n",
       " 9,\n",
       " 33,\n",
       " 50,\n",
       " 3,\n",
       " 54,\n",
       " 51,\n",
       " 0,\n",
       " 76,\n",
       " 54,\n",
       " 9,\n",
       " 63,\n",
       " 40,\n",
       " 38,\n",
       " 65,\n",
       " 52,\n",
       " 9,\n",
       " 63,\n",
       " 63,\n",
       " 33,\n",
       " 28,\n",
       " 40,\n",
       " 42,\n",
       " 10,\n",
       " 56,\n",
       " 47,\n",
       " 28,\n",
       " 9,\n",
       " 33,\n",
       " 85,\n",
       " 63,\n",
       " 2,\n",
       " 10,\n",
       " 41,\n",
       " 32,\n",
       " 59,\n",
       " 1,\n",
       " 19,\n",
       " 55,\n",
       " 71,\n",
       " 8,\n",
       " 76,\n",
       " 50,\n",
       " 52,\n",
       " 58,\n",
       " 30,\n",
       " 24,\n",
       " 79,\n",
       " 19,\n",
       " 41,\n",
       " 33,\n",
       " 84,\n",
       " 15,\n",
       " 72,\n",
       " 56,\n",
       " 73,\n",
       " 82,\n",
       " 72,\n",
       " 50,\n",
       " 33,\n",
       " 54,\n",
       " 4,\n",
       " 33,\n",
       " 3,\n",
       " 52,\n",
       " 52,\n",
       " 33,\n",
       " 42,\n",
       " 24,\n",
       " 52,\n",
       " 14,\n",
       " 79,\n",
       " 33,\n",
       " 24,\n",
       " 47,\n",
       " 52,\n",
       " 63,\n",
       " 21,\n",
       " 3,\n",
       " 50,\n",
       " 48,\n",
       " 13,\n",
       " 45,\n",
       " 9,\n",
       " 44,\n",
       " 72,\n",
       " 17,\n",
       " 65,\n",
       " 84,\n",
       " 3,\n",
       " 52,\n",
       " 15,\n",
       " 55,\n",
       " 34,\n",
       " 10,\n",
       " 54,\n",
       " 3,\n",
       " 15,\n",
       " 25,\n",
       " 82,\n",
       " 45,\n",
       " 47,\n",
       " 74,\n",
       " 23,\n",
       " 75,\n",
       " 83,\n",
       " 34,\n",
       " 59,\n",
       " 48,\n",
       " 55,\n",
       " 0,\n",
       " 13,\n",
       " 70,\n",
       " 36,\n",
       " 55,\n",
       " 75,\n",
       " 9,\n",
       " 40,\n",
       " 68,\n",
       " 68,\n",
       " 57,\n",
       " 8,\n",
       " 85,\n",
       " 21,\n",
       " 67,\n",
       " 68,\n",
       " 84,\n",
       " 31,\n",
       " 72,\n",
       " 85,\n",
       " 55,\n",
       " 52,\n",
       " 87,\n",
       " 31,\n",
       " 66,\n",
       " 43,\n",
       " 64,\n",
       " 76,\n",
       " 52,\n",
       " 0,\n",
       " 39,\n",
       " 86,\n",
       " 9,\n",
       " 14,\n",
       " 17,\n",
       " 25,\n",
       " 68,\n",
       " 76,\n",
       " 55,\n",
       " 2,\n",
       " 61,\n",
       " 55,\n",
       " 54,\n",
       " 35,\n",
       " 9,\n",
       " 45,\n",
       " 55,\n",
       " 9,\n",
       " 15,\n",
       " 9,\n",
       " 31,\n",
       " 72,\n",
       " 19,\n",
       " 25,\n",
       " 9,\n",
       " 14,\n",
       " 84,\n",
       " 63,\n",
       " 16,\n",
       " 57,\n",
       " 21,\n",
       " 52,\n",
       " 34,\n",
       " 0,\n",
       " 55,\n",
       " 56,\n",
       " 61,\n",
       " 69,\n",
       " 36,\n",
       " 2,\n",
       " 65,\n",
       " 15,\n",
       " 65,\n",
       " 78,\n",
       " 55,\n",
       " 14,\n",
       " 72,\n",
       " 16,\n",
       " 84,\n",
       " 31,\n",
       " 72,\n",
       " 85,\n",
       " 5,\n",
       " 33,\n",
       " 45,\n",
       " 55,\n",
       " 20,\n",
       " 72,\n",
       " 67,\n",
       " 9,\n",
       " 84,\n",
       " 9,\n",
       " 21,\n",
       " 41,\n",
       " 52,\n",
       " 51,\n",
       " 47,\n",
       " 21,\n",
       " 60,\n",
       " 77,\n",
       " 40,\n",
       " 40,\n",
       " 33,\n",
       " 25,\n",
       " 15,\n",
       " 16,\n",
       " 72,\n",
       " 63,\n",
       " 57,\n",
       " 47,\n",
       " 40,\n",
       " 44,\n",
       " 72,\n",
       " 84,\n",
       " 40,\n",
       " 12,\n",
       " 37,\n",
       " 55,\n",
       " 54,\n",
       " 55,\n",
       " 12,\n",
       " 21,\n",
       " 71,\n",
       " 40,\n",
       " 55,\n",
       " 52,\n",
       " 14,\n",
       " 80,\n",
       " 38,\n",
       " 13,\n",
       " 58,\n",
       " 46,\n",
       " 62,\n",
       " 52,\n",
       " 81,\n",
       " 18,\n",
       " 39,\n",
       " 60,\n",
       " 44,\n",
       " 60,\n",
       " 65,\n",
       " 70,\n",
       " 33,\n",
       " 55,\n",
       " 66,\n",
       " 84,\n",
       " 44,\n",
       " 39,\n",
       " 4,\n",
       " 68,\n",
       " 9,\n",
       " 68,\n",
       " 63,\n",
       " 5,\n",
       " 18,\n",
       " 42,\n",
       " 16,\n",
       " 52,\n",
       " 65,\n",
       " 37,\n",
       " 19,\n",
       " 40,\n",
       " 24,\n",
       " 38,\n",
       " 58,\n",
       " 56,\n",
       " 2,\n",
       " 40,\n",
       " 39,\n",
       " 17,\n",
       " 87,\n",
       " 34,\n",
       " 17,\n",
       " 46,\n",
       " 63,\n",
       " 30,\n",
       " 55,\n",
       " 24,\n",
       " 13,\n",
       " 69,\n",
       " 15,\n",
       " 77,\n",
       " 84,\n",
       " 58,\n",
       " 50,\n",
       " 34,\n",
       " 33,\n",
       " 6,\n",
       " 58,\n",
       " 35,\n",
       " 9,\n",
       " 33,\n",
       " 21,\n",
       " 19,\n",
       " 1,\n",
       " 9,\n",
       " 33,\n",
       " 62,\n",
       " 53,\n",
       " 40,\n",
       " 57,\n",
       " 55,\n",
       " 40,\n",
       " 21,\n",
       " 59,\n",
       " 72,\n",
       " 54,\n",
       " 18,\n",
       " 28,\n",
       " 45,\n",
       " 55,\n",
       " 55,\n",
       " 47,\n",
       " 58,\n",
       " 15,\n",
       " 11,\n",
       " 9,\n",
       " 3,\n",
       " 82,\n",
       " 3,\n",
       " 38,\n",
       " 55,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pred = ensemble.argmax(axis=1).tolist()\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./open/train_df_add2.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tile-glue_strip',\n",
       " 'grid-good',\n",
       " 'transistor-good',\n",
       " 'tile-gray_stroke',\n",
       " 'tile-good',\n",
       " 'pill-crack',\n",
       " 'capsule-faulty_imprint',\n",
       " 'hazelnut-cut',\n",
       " 'leather-good',\n",
       " 'leather-glue',\n",
       " 'bottle-broken_large',\n",
       " 'wood-good',\n",
       " 'zipper-rough',\n",
       " 'screw-thread_top',\n",
       " 'tile-glue_strip',\n",
       " 'bottle-contamination',\n",
       " 'grid-bent',\n",
       " 'carpet-hole',\n",
       " 'carpet-good',\n",
       " 'pill-color',\n",
       " 'tile-rough',\n",
       " 'zipper-good',\n",
       " 'bottle-broken_large',\n",
       " 'screw-scratch_neck',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-thread_top',\n",
       " 'leather-good',\n",
       " 'wood-liquid',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'screw-thread_side',\n",
       " 'zipper-fabric_interior',\n",
       " 'zipper-good',\n",
       " 'zipper-fabric_border',\n",
       " 'leather-good',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-good',\n",
       " 'cable-missing_cable',\n",
       " 'screw-thread_top',\n",
       " 'capsule-faulty_imprint',\n",
       " 'metal_nut-flip',\n",
       " 'tile-good',\n",
       " 'toothbrush-good',\n",
       " 'capsule-crack',\n",
       " 'zipper-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'transistor-good',\n",
       " 'hazelnut-crack',\n",
       " 'hazelnut-print',\n",
       " 'zipper-good',\n",
       " 'metal_nut-scratch',\n",
       " 'carpet-cut',\n",
       " 'carpet-good',\n",
       " 'leather-fold',\n",
       " 'capsule-good',\n",
       " 'transistor-bent_lead',\n",
       " 'pill-combined',\n",
       " 'capsule-squeeze',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'capsule-crack',\n",
       " 'leather-color',\n",
       " 'transistor-good',\n",
       " 'capsule-crack',\n",
       " 'pill-pill_type',\n",
       " 'metal_nut-color',\n",
       " 'grid-bent',\n",
       " 'wood-good',\n",
       " 'hazelnut-cut',\n",
       " 'tile-good',\n",
       " 'leather-poke',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'leather-good',\n",
       " 'cable-cable_swap',\n",
       " 'cable-good',\n",
       " 'pill-crack',\n",
       " 'wood-good',\n",
       " 'screw-scratch_head',\n",
       " 'wood-combined',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-good',\n",
       " 'carpet-hole',\n",
       " 'leather-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'leather-color',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-bent',\n",
       " 'screw-good',\n",
       " 'tile-oil',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'metal_nut-good',\n",
       " 'metal_nut-good',\n",
       " 'cable-good',\n",
       " 'carpet-thread',\n",
       " 'pill-good',\n",
       " 'capsule-scratch',\n",
       " 'wood-good',\n",
       " 'bottle-contamination',\n",
       " 'zipper-split_teeth',\n",
       " 'pill-contamination',\n",
       " 'transistor-good',\n",
       " 'hazelnut-hole',\n",
       " 'leather-color',\n",
       " 'screw-scratch_neck',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'pill-good',\n",
       " 'capsule-good',\n",
       " 'leather-poke',\n",
       " 'screw-good',\n",
       " 'carpet-color',\n",
       " 'metal_nut-good',\n",
       " 'transistor-good',\n",
       " 'wood-hole',\n",
       " 'transistor-good',\n",
       " 'tile-crack',\n",
       " 'transistor-good',\n",
       " 'pill-scratch',\n",
       " 'screw-good',\n",
       " 'grid-good',\n",
       " 'metal_nut-scratch',\n",
       " 'pill-crack',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'grid-bent',\n",
       " 'screw-scratch_head',\n",
       " 'bottle-contamination',\n",
       " 'pill-good',\n",
       " 'wood-liquid',\n",
       " 'transistor-misplaced',\n",
       " 'hazelnut-good',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'cable-good',\n",
       " 'zipper-good',\n",
       " 'hazelnut-print',\n",
       " 'carpet-metal_contamination',\n",
       " 'zipper-good',\n",
       " 'zipper-rough',\n",
       " 'wood-good',\n",
       " 'zipper-good',\n",
       " 'wood-scratch',\n",
       " 'transistor-good',\n",
       " 'pill-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'tile-good',\n",
       " 'toothbrush-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-scratch_head',\n",
       " 'screw-thread_top',\n",
       " 'screw-scratch_head',\n",
       " 'cable-combined',\n",
       " 'carpet-good',\n",
       " 'leather-fold',\n",
       " 'pill-contamination',\n",
       " 'wood-color',\n",
       " 'zipper-fabric_interior',\n",
       " 'cable-missing_cable',\n",
       " 'carpet-good',\n",
       " 'carpet-thread',\n",
       " 'screw-manipulated_front',\n",
       " 'leather-glue',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'zipper-split_teeth',\n",
       " 'capsule-good',\n",
       " 'zipper-good',\n",
       " 'leather-good',\n",
       " 'grid-glue',\n",
       " 'bottle-good',\n",
       " 'cable-missing_wire',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-cut',\n",
       " 'tile-glue_strip',\n",
       " 'metal_nut-good',\n",
       " 'wood-scratch',\n",
       " 'transistor-cut_lead',\n",
       " 'pill-good',\n",
       " 'capsule-squeeze',\n",
       " 'zipper-good',\n",
       " 'hazelnut-cut',\n",
       " 'pill-good',\n",
       " 'screw-scratch_head',\n",
       " 'zipper-fabric_interior',\n",
       " 'zipper-rough',\n",
       " 'zipper-good',\n",
       " 'hazelnut-good',\n",
       " 'transistor-good',\n",
       " 'cable-poke_insulation',\n",
       " 'screw-scratch_neck',\n",
       " 'bottle-broken_small',\n",
       " 'pill-good',\n",
       " 'leather-good',\n",
       " 'pill-color',\n",
       " 'pill-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-bent',\n",
       " 'leather-poke',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'wood-good',\n",
       " 'pill-combined',\n",
       " 'carpet-cut',\n",
       " 'capsule-squeeze',\n",
       " 'pill-contamination',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_small',\n",
       " 'tile-oil',\n",
       " 'grid-good',\n",
       " 'tile-good',\n",
       " 'bottle-broken_small',\n",
       " 'capsule-poke',\n",
       " 'metal_nut-color',\n",
       " 'pill-faulty_imprint',\n",
       " 'leather-color',\n",
       " 'cable-good',\n",
       " 'transistor-good',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'zipper-combined',\n",
       " 'pill-contamination',\n",
       " 'zipper-broken_teeth',\n",
       " 'capsule-scratch',\n",
       " 'transistor-good',\n",
       " 'metal_nut-good',\n",
       " 'hazelnut-crack',\n",
       " 'capsule-faulty_imprint',\n",
       " 'metal_nut-good',\n",
       " 'hazelnut-hole',\n",
       " 'zipper-rough',\n",
       " 'screw-thread_top',\n",
       " 'wood-hole',\n",
       " 'leather-poke',\n",
       " 'tile-good',\n",
       " 'tile-gray_stroke',\n",
       " 'wood-scratch',\n",
       " 'cable-good',\n",
       " 'transistor-good',\n",
       " 'tile-gray_stroke',\n",
       " 'cable-bent_wire',\n",
       " 'capsule-good',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-good',\n",
       " 'metal_nut-good',\n",
       " 'capsule-crack',\n",
       " 'zipper-split_teeth',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-crack',\n",
       " 'screw-good',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-cut',\n",
       " 'leather-poke',\n",
       " 'leather-good',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'metal_nut-flip',\n",
       " 'cable-good',\n",
       " 'carpet-thread',\n",
       " 'screw-good',\n",
       " 'wood-hole',\n",
       " 'transistor-good',\n",
       " 'cable-good',\n",
       " 'capsule-poke',\n",
       " 'pill-color',\n",
       " 'capsule-good',\n",
       " 'transistor-misplaced',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'pill-combined',\n",
       " 'leather-color',\n",
       " 'transistor-good',\n",
       " 'pill-pill_type',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'grid-thread',\n",
       " 'tile-good',\n",
       " 'tile-good',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-hole',\n",
       " 'bottle-broken_small',\n",
       " 'leather-fold',\n",
       " 'leather-cut',\n",
       " 'cable-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'tile-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_neck',\n",
       " 'leather-poke',\n",
       " 'zipper-broken_teeth',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-faulty_imprint',\n",
       " 'toothbrush-good',\n",
       " 'grid-good',\n",
       " 'leather-color',\n",
       " 'screw-scratch_neck',\n",
       " 'screw-good',\n",
       " 'transistor-good',\n",
       " 'bottle-good',\n",
       " 'metal_nut-scratch',\n",
       " 'wood-good',\n",
       " 'transistor-misplaced',\n",
       " 'screw-thread_side',\n",
       " 'wood-good',\n",
       " 'wood-good',\n",
       " 'tile-good',\n",
       " 'cable-good',\n",
       " 'screw-scratch_neck',\n",
       " 'metal_nut-scratch',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'leather-good',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'screw-manipulated_front',\n",
       " 'pill-crack',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'bottle-good',\n",
       " 'tile-good',\n",
       " 'metal_nut-color',\n",
       " 'screw-scratch_neck',\n",
       " 'cable-bent_wire',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'screw-good',\n",
       " 'leather-glue',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-bent',\n",
       " 'hazelnut-good',\n",
       " 'pill-pill_type',\n",
       " 'carpet-thread',\n",
       " 'cable-cable_swap',\n",
       " 'cable-good',\n",
       " 'capsule-scratch',\n",
       " 'carpet-good',\n",
       " 'capsule-poke',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'bottle-good',\n",
       " 'toothbrush-defective',\n",
       " 'transistor-cut_lead',\n",
       " 'capsule-crack',\n",
       " 'leather-fold',\n",
       " 'carpet-color',\n",
       " 'transistor-good',\n",
       " 'zipper-broken_teeth',\n",
       " 'wood-hole',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'hazelnut-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'bottle-good',\n",
       " 'hazelnut-good',\n",
       " 'transistor-cut_lead',\n",
       " 'wood-hole',\n",
       " 'screw-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'grid-good',\n",
       " 'metal_nut-scratch',\n",
       " 'screw-good',\n",
       " 'wood-color',\n",
       " 'tile-glue_strip',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'hazelnut-cut',\n",
       " 'screw-good',\n",
       " 'leather-good',\n",
       " 'metal_nut-color',\n",
       " 'bottle-broken_small',\n",
       " 'leather-cut',\n",
       " 'capsule-crack',\n",
       " 'transistor-bent_lead',\n",
       " 'cable-good',\n",
       " 'bottle-contamination',\n",
       " 'hazelnut-print',\n",
       " 'tile-good',\n",
       " 'wood-liquid',\n",
       " 'tile-gray_stroke',\n",
       " 'transistor-good',\n",
       " 'zipper-combined',\n",
       " 'bottle-contamination',\n",
       " 'tile-good',\n",
       " 'pill-crack',\n",
       " 'capsule-scratch',\n",
       " 'leather-good',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'bottle-broken_small',\n",
       " 'transistor-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'hazelnut-cut',\n",
       " 'transistor-cut_lead',\n",
       " 'metal_nut-scratch',\n",
       " 'zipper-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'pill-crack',\n",
       " 'zipper-good',\n",
       " 'zipper-good',\n",
       " 'leather-poke',\n",
       " 'transistor-damaged_case',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'bottle-broken_small',\n",
       " 'tile-good',\n",
       " 'hazelnut-crack',\n",
       " 'wood-scratch',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-good',\n",
       " 'grid-glue',\n",
       " 'pill-color',\n",
       " 'tile-oil',\n",
       " 'transistor-good',\n",
       " 'screw-good',\n",
       " 'transistor-misplaced',\n",
       " 'transistor-cut_lead',\n",
       " 'carpet-thread',\n",
       " 'zipper-split_teeth',\n",
       " 'tile-glue_strip',\n",
       " 'leather-fold',\n",
       " 'wood-hole',\n",
       " 'zipper-good',\n",
       " 'bottle-broken_small',\n",
       " 'carpet-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'carpet-hole',\n",
       " 'metal_nut-flip',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'screw-thread_top',\n",
       " 'leather-poke',\n",
       " 'carpet-thread',\n",
       " 'capsule-faulty_imprint',\n",
       " 'capsule-crack',\n",
       " 'metal_nut-flip',\n",
       " 'screw-thread_side',\n",
       " 'metal_nut-bent',\n",
       " 'pill-pill_type',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'bottle-good',\n",
       " 'screw-manipulated_front',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'transistor-good',\n",
       " 'screw-thread_top',\n",
       " 'screw-thread_top',\n",
       " 'pill-crack',\n",
       " 'screw-good',\n",
       " 'carpet-hole',\n",
       " 'cable-good',\n",
       " 'screw-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'grid-good',\n",
       " 'hazelnut-hole',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-cut',\n",
       " 'cable-combined',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'screw-good',\n",
       " 'hazelnut-good',\n",
       " 'capsule-good',\n",
       " 'transistor-cut_lead',\n",
       " 'toothbrush-defective',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'zipper-split_teeth',\n",
       " 'screw-good',\n",
       " 'tile-oil',\n",
       " 'metal_nut-scratch',\n",
       " 'transistor-good',\n",
       " 'cable-bent_wire',\n",
       " 'tile-good',\n",
       " 'leather-fold',\n",
       " 'wood-liquid',\n",
       " 'toothbrush-good',\n",
       " 'leather-poke',\n",
       " 'capsule-poke',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-misplaced',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-scratch',\n",
       " 'leather-good',\n",
       " 'cable-good',\n",
       " 'tile-gray_stroke',\n",
       " 'pill-pill_type',\n",
       " 'wood-good',\n",
       " 'metal_nut-good',\n",
       " 'pill-good',\n",
       " 'zipper-fabric_border',\n",
       " 'cable-missing_wire',\n",
       " 'wood-good',\n",
       " 'capsule-good',\n",
       " 'carpet-cut',\n",
       " 'capsule-good',\n",
       " 'hazelnut-hole',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-hole',\n",
       " 'grid-good',\n",
       " 'screw-good',\n",
       " 'zipper-split_teeth',\n",
       " 'wood-scratch',\n",
       " 'pill-contamination',\n",
       " 'toothbrush-defective',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'grid-broken',\n",
       " 'screw-good',\n",
       " 'grid-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'transistor-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'pill-scratch',\n",
       " 'cable-good',\n",
       " 'capsule-crack',\n",
       " 'pill-color',\n",
       " 'pill-good',\n",
       " 'zipper-combined',\n",
       " 'tile-glue_strip',\n",
       " 'screw-good',\n",
       " 'capsule-scratch',\n",
       " 'zipper-combined',\n",
       " 'screw-good',\n",
       " 'leather-glue',\n",
       " 'capsule-faulty_imprint',\n",
       " 'leather-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-good',\n",
       " 'grid-good',\n",
       " 'hazelnut-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'wood-scratch',\n",
       " 'hazelnut-good',\n",
       " 'pill-scratch',\n",
       " 'cable-missing_wire',\n",
       " 'wood-hole',\n",
       " 'zipper-split_teeth',\n",
       " 'carpet-good',\n",
       " 'grid-glue',\n",
       " 'zipper-fabric_border',\n",
       " 'pill-contamination',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'carpet-cut',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'carpet-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'screw-good',\n",
       " 'hazelnut-cut',\n",
       " 'cable-good',\n",
       " 'zipper-split_teeth',\n",
       " 'zipper-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-metal_contamination',\n",
       " 'tile-good',\n",
       " 'zipper-split_teeth',\n",
       " 'hazelnut-good',\n",
       " 'leather-good',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-hole',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'pill-crack',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'transistor-cut_lead',\n",
       " 'pill-faulty_imprint',\n",
       " 'metal_nut-bent',\n",
       " 'grid-glue',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'hazelnut-good',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'zipper-split_teeth',\n",
       " 'carpet-thread',\n",
       " 'cable-bent_wire',\n",
       " 'cable-good',\n",
       " 'cable-good',\n",
       " 'zipper-good',\n",
       " 'tile-glue_strip',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-hole',\n",
       " 'metal_nut-bent',\n",
       " 'toothbrush-good',\n",
       " 'hazelnut-good',\n",
       " 'bottle-broken_small',\n",
       " 'leather-good',\n",
       " 'toothbrush-defective',\n",
       " 'screw-good',\n",
       " 'screw-scratch_neck',\n",
       " 'zipper-split_teeth',\n",
       " 'leather-good',\n",
       " 'cable-missing_wire',\n",
       " 'capsule-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'leather-poke',\n",
       " 'pill-good',\n",
       " 'leather-cut',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'capsule-good',\n",
       " 'zipper-split_teeth',\n",
       " 'wood-combined',\n",
       " 'hazelnut-print',\n",
       " 'carpet-good',\n",
       " 'screw-good',\n",
       " 'grid-metal_contamination',\n",
       " 'wood-scratch',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-crack',\n",
       " 'capsule-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'carpet-good',\n",
       " 'transistor-cut_lead',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'transistor-cut_lead',\n",
       " 'hazelnut-hole',\n",
       " 'tile-rough',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'pill-crack',\n",
       " 'bottle-good',\n",
       " 'pill-scratch',\n",
       " 'pill-faulty_imprint',\n",
       " 'bottle-broken_large',\n",
       " 'wood-good',\n",
       " 'pill-scratch',\n",
       " 'cable-good',\n",
       " 'tile-good',\n",
       " 'leather-good',\n",
       " 'leather-fold',\n",
       " 'tile-oil',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'tile-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-good',\n",
       " 'leather-good',\n",
       " 'metal_nut-bent',\n",
       " 'cable-missing_cable',\n",
       " 'screw-manipulated_front',\n",
       " 'pill-color',\n",
       " 'grid-good',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'zipper-rough',\n",
       " 'tile-good',\n",
       " 'bottle-contamination',\n",
       " 'cable-missing_cable',\n",
       " 'leather-poke',\n",
       " 'hazelnut-cut',\n",
       " 'screw-thread_side',\n",
       " 'bottle-broken_small',\n",
       " 'carpet-color',\n",
       " 'screw-good',\n",
       " 'transistor-damaged_case',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'wood-good',\n",
       " 'pill-crack',\n",
       " 'pill-good',\n",
       " 'screw-scratch_neck',\n",
       " 'grid-thread',\n",
       " 'carpet-thread',\n",
       " 'wood-scratch',\n",
       " 'carpet-color',\n",
       " 'leather-poke',\n",
       " 'hazelnut-good',\n",
       " 'zipper-good',\n",
       " 'capsule-good',\n",
       " 'transistor-good',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-misplaced',\n",
       " 'zipper-fabric_border',\n",
       " 'transistor-good',\n",
       " 'pill-crack',\n",
       " 'hazelnut-good',\n",
       " 'pill-scratch',\n",
       " 'cable-bent_wire',\n",
       " 'hazelnut-good',\n",
       " 'bottle-good',\n",
       " 'pill-good',\n",
       " 'pill-good',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-thread',\n",
       " 'pill-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'wood-scratch',\n",
       " 'hazelnut-good',\n",
       " 'carpet-thread',\n",
       " 'pill-color',\n",
       " 'pill-good',\n",
       " 'tile-good',\n",
       " 'carpet-good',\n",
       " 'bottle-good',\n",
       " 'pill-crack',\n",
       " 'pill-combined',\n",
       " 'capsule-crack',\n",
       " 'metal_nut-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-flip',\n",
       " 'transistor-good',\n",
       " 'capsule-scratch',\n",
       " 'tile-oil',\n",
       " 'zipper-good',\n",
       " 'bottle-good',\n",
       " 'pill-good',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'hazelnut-hole',\n",
       " 'cable-missing_cable',\n",
       " 'pill-scratch',\n",
       " 'bottle-good',\n",
       " 'capsule-good',\n",
       " 'grid-bent',\n",
       " 'zipper-fabric_border',\n",
       " 'metal_nut-good',\n",
       " 'pill-color',\n",
       " 'wood-color',\n",
       " 'carpet-metal_contamination',\n",
       " 'wood-combined',\n",
       " 'zipper-fabric_interior',\n",
       " 'hazelnut-hole',\n",
       " 'screw-thread_side',\n",
       " 'pill-combined',\n",
       " 'screw-good',\n",
       " 'bottle-broken_large',\n",
       " 'capsule-crack',\n",
       " 'transistor-cut_lead',\n",
       " 'leather-color',\n",
       " 'screw-good',\n",
       " 'wood-combined',\n",
       " 'cable-good',\n",
       " 'leather-good',\n",
       " 'toothbrush-good',\n",
       " 'toothbrush-good',\n",
       " 'screw-scratch_head',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'zipper-rough',\n",
       " 'carpet-good',\n",
       " 'toothbrush-defective',\n",
       " 'toothbrush-good',\n",
       " 'zipper-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'zipper-rough',\n",
       " 'screw-good',\n",
       " 'pill-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-crack',\n",
       " 'tile-rough',\n",
       " 'metal_nut-color',\n",
       " 'tile-gray_stroke',\n",
       " 'wood-good',\n",
       " 'pill-good',\n",
       " 'bottle-broken_large',\n",
       " 'leather-glue',\n",
       " 'zipper-split_teeth',\n",
       " 'cable-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'capsule-scratch',\n",
       " 'grid-bent',\n",
       " 'toothbrush-good',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'bottle-contamination',\n",
       " 'tile-crack',\n",
       " 'screw-good',\n",
       " 'pill-scratch',\n",
       " 'hazelnut-print',\n",
       " 'cable-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'capsule-good',\n",
       " 'cable-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'carpet-color',\n",
       " 'grid-bent',\n",
       " 'cable-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-good',\n",
       " 'tile-good',\n",
       " 'capsule-poke',\n",
       " 'screw-scratch_head',\n",
       " 'carpet-good',\n",
       " 'pill-good',\n",
       " 'hazelnut-hole',\n",
       " 'bottle-broken_large',\n",
       " 'screw-good',\n",
       " 'screw-manipulated_front',\n",
       " 'tile-crack',\n",
       " 'transistor-bent_lead',\n",
       " 'leather-color',\n",
       " 'bottle-contamination',\n",
       " 'tile-oil',\n",
       " 'capsule-good',\n",
       " 'tile-oil',\n",
       " 'wood-liquid',\n",
       " 'screw-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'transistor-good',\n",
       " 'capsule-poke',\n",
       " 'zipper-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'zipper-rough',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'carpet-cut',\n",
       " 'transistor-good',\n",
       " 'toothbrush-defective',\n",
       " 'cable-good',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'leather-poke',\n",
       " 'pill-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'pill-color',\n",
       " 'carpet-good',\n",
       " 'screw-thread_top',\n",
       " 'wood-hole',\n",
       " 'leather-good',\n",
       " 'leather-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-bent',\n",
       " 'capsule-good',\n",
       " 'capsule-poke',\n",
       " 'transistor-good',\n",
       " 'tile-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-color',\n",
       " 'leather-good',\n",
       " 'metal_nut-flip',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'leather-good',\n",
       " 'cable-poke_insulation',\n",
       " 'leather-cut',\n",
       " 'screw-good',\n",
       " 'pill-scratch',\n",
       " 'screw-good',\n",
       " 'cable-poke_insulation',\n",
       " 'carpet-good',\n",
       " 'transistor-damaged_case',\n",
       " 'leather-good',\n",
       " 'screw-good',\n",
       " 'pill-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-broken_teeth',\n",
       " 'leather-fold',\n",
       " 'capsule-crack',\n",
       " 'screw-scratch_neck',\n",
       " 'metal_nut-scratch',\n",
       " 'tile-glue_strip',\n",
       " 'pill-good',\n",
       " 'zipper-combined',\n",
       " 'capsule-squeeze',\n",
       " 'leather-glue',\n",
       " 'screw-thread_top',\n",
       " 'metal_nut-flip',\n",
       " 'screw-thread_top',\n",
       " 'tile-oil',\n",
       " 'transistor-cut_lead',\n",
       " 'hazelnut-good',\n",
       " 'screw-good',\n",
       " 'tile-rough',\n",
       " 'zipper-good',\n",
       " 'metal_nut-flip',\n",
       " 'leather-glue',\n",
       " 'cable-bent_wire',\n",
       " 'toothbrush-good',\n",
       " 'cable-good',\n",
       " 'toothbrush-good',\n",
       " 'tile-good',\n",
       " 'cable-cable_swap',\n",
       " 'capsule-squeeze',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'tile-oil',\n",
       " 'leather-cut',\n",
       " 'carpet-color',\n",
       " 'leather-good',\n",
       " 'carpet-thread',\n",
       " 'leather-fold',\n",
       " 'screw-scratch_neck',\n",
       " 'screw-manipulated_front',\n",
       " 'bottle-contamination',\n",
       " 'leather-good',\n",
       " 'leather-glue',\n",
       " 'capsule-scratch',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-hole',\n",
       " 'capsule-scratch',\n",
       " 'metal_nut-scratch',\n",
       " 'tile-good',\n",
       " 'grid-thread',\n",
       " 'screw-good',\n",
       " 'carpet-thread',\n",
       " 'capsule-crack',\n",
       " 'transistor-bent_lead',\n",
       " 'capsule-good',\n",
       " 'wood-hole',\n",
       " 'zipper-good',\n",
       " 'screw-scratch_neck',\n",
       " 'pill-crack',\n",
       " 'hazelnut-hole',\n",
       " 'hazelnut-good',\n",
       " 'cable-combined',\n",
       " 'screw-scratch_neck',\n",
       " 'hazelnut-print',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'bottle-broken_small',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'tile-glue_strip',\n",
       " 'pill-pill_type',\n",
       " 'leather-good',\n",
       " 'screw-scratch_head',\n",
       " 'screw-good',\n",
       " 'leather-good',\n",
       " 'carpet-good',\n",
       " 'screw-thread_side',\n",
       " 'transistor-good',\n",
       " 'pill-scratch',\n",
       " 'capsule-squeeze',\n",
       " 'grid-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'screw-good',\n",
       " 'pill-color',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-good',\n",
       " 'cable-missing_wire',\n",
       " 'cable-good',\n",
       " 'bottle-good',\n",
       " 'zipper-fabric_border',\n",
       " 'bottle-good',\n",
       " 'leather-fold',\n",
       " 'screw-good',\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile-glue_strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable-poke_insulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper-good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                  label\n",
       "0         0        tile-glue_strip\n",
       "1         1              grid-good\n",
       "2         2        transistor-good\n",
       "3         3       tile-gray_stroke\n",
       "4         4              tile-good\n",
       "...     ...                    ...\n",
       "2149   2149       tile-gray_stroke\n",
       "2150   2150             screw-good\n",
       "2151   2151              grid-good\n",
       "2152   2152  cable-poke_insulation\n",
       "2153   2153            zipper-good\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./submission/label_result_add_0502_수정.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
