{"cells":[{"cell_type":"markdown","source":["# Summary"],"metadata":{"id":"e0G5UhOCQ-B7"}},{"cell_type":"markdown","source":["+ Baseline  \n","https://dacon.io/competitions/official/235805/codeshare/3620?page=1&dtype=recent  \n","이전 다른 이미지 대회에서 수상했던 코드를 참고하였습니다.  \n","좋은 코드 공유 감사합니다."],"metadata":{"id":"9sR1yDa7Snr-"}},{"cell_type":"markdown","source":["+ Data Augmentation   \n","데이터 불균형이 심해서 학습 이전에 Anomaly한 데이터만 다양한 방법으로 증강하여 새로운 데이터 셋 폴더에 저장하고,  \n","csv 파일에도 추가한 데이터에 대한 정보를 합쳐서 새로 만들어 주었습니다.  \n","그리고, 학습 과정에서도 pytorch transforms을 활용하여 데이터를 증강시켜주었습니다."],"metadata":{"id":"pvLmtuPARI5f"}},{"cell_type":"markdown","source":["+ Training parameter\n","    + Lamb Optimizer\n","    + ReduceLROnplateau Scheduler\n","    + WarmUpLR Scheduler\n","    + Focal Loss\n","    + Early Stopping\n"],"metadata":{"id":"W5VL9xmSTTB7"}},{"cell_type":"markdown","source":["+ Model  \n","모델은 5 k-fold로 진행하면서 Public 기준으로 결과가 좋은 상위 2개 모델을 골라서 앙상블을 진행해주었습니다.   \n","그리고, 2가지 모델에 대해서 network 구조와, transforms 방법을 조금 다르게 설정 하였습니다. 다른 모델은 주석으로 표시해두었습니다.\n","    + cait_s36_384, img_size: 384\n","    + tf_efficientnet_b7_ns, img_size: 600"],"metadata":{"id":"fZHNFjE2UD9K"}},{"cell_type":"markdown","source":["+ TTA  \n","https://github.com/qubvel/ttach  \n","d4_transform()"],"metadata":{"id":"9GHaqcv_WFKE"}},{"cell_type":"markdown","metadata":{"id":"RnEJua88P494"},"source":["# Library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHE-uhVuP498"},"outputs":[],"source":["import os\n","import cv2\n","import time\n","import random\n","import logging\n","import easydict\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from os.path import join as opj\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import f1_score\n","from PIL import Image\n","from natsort import natsorted\n","\n","import timm\n","import torch\n","import torch.nn as nn\n","import torch_optimizer as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, grad_scaler\n","from torchvision import transforms\n","from torch import Tensor\n","from torchvision.transforms import functional as F\n","import torch.cuda.amp as amp\n","from adamp import AdamP, SGDP\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"2rKLdll2P49-"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFmxhgFsP49-"},"outputs":[],"source":["args = easydict.EasyDict(\n","    {'exp_num':'0',\n","     \n","     # Path settings\n","     'data_path':'./open',\n","     'Kfold':5,\n","     \n","     'model_path':'label_results_cait_s36_384/',\n","#      'model_path':'label_results_tf_efficientnet_b7_ns_600/',\n","\n","     'image_type':'train_1024', \n","     'class_num' : 88,\n","\n","     # Model parameter settings\n","     'model_name':'cait_s36_384',\n","#      'model_name':'tf_efficientnet_b7_ns',\n","     \n","     'drop_path_rate':0.2,\n","     \n","     # Training parameter settings\n","     ## Base Parameter\n","     'img_size':384,\n","#      'img_size':600, # tf_efficientnet_b7_ns\n","     \n","     'batch_size':16,\n","     'epochs':100,\n","     'optimizer':'Lamb',\n","     'initial_lr':5e-4,\n","     'weight_decay':1e-3,\n","\n","     ## Augmentation\n","     'aug_ver':2,\n","\n","     ## Scheduler (OnecycleLR)\n","     'scheduler':'Reduce',\n","     'warm_epoch':5,\n","     'max_lr':1e-3,\n","\n","     ### Cosine Annealing\n","     'min_lr':5e-5,\n","     'tmax':145,\n","\n","     ## etc.\n","     'patience': 5,\n","     'clipping':None,\n","\n","     # Hardware settings\n","     'amp':True,\n","     'multi_gpu':True,\n","     'logging':False,\n","     'num_workers':4,\n","     'seed':42\n","     \n","     \n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUgFuBOGP49_"},"outputs":[],"source":["os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 0 and 1 to use"]},{"cell_type":"markdown","metadata":{"id":"94Z1fQh_P4-A"},"source":["# Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"LAGmd8oVP4-B"},"source":["train 폴더를 train_add_data 이름으로 새로운 폴더에 복사 후 진행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSlWCGrsP4-B"},"outputs":[],"source":["DATA_DIR = './open'\n","\n","train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df.csv'))\n","train_df = train_df.drop('index', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTMBBArPP4-C"},"outputs":[],"source":["seed = 42\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","random.seed(seed)\n","np.random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"iD1C6U1xP4-C"},"source":["# Random_Crop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZUgDEA2P4-D"},"outputs":[],"source":["def get_random_crop(image, crop_height, crop_width):\n","\n","    max_x = image.shape[1] - crop_width\n","    max_y = image.shape[0] - crop_height\n","\n","    x = np.random.randint(0, max_x)\n","    y = np.random.randint(0, max_y)\n","\n","    crop = image[y: y + crop_height, x: x + crop_width]\n","\n","    return crop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E50qv5nPP4-D"},"outputs":[],"source":["oslabel = list(train_df['label'].unique())\n","\n","\n","for label in tqdm(oslabel):\n","    if 'good' not in label:\n","        print(label)\n","        idx = 0\n","        one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n","        images_list = natsorted(one_sample['file_name'])\n","        print(images_list)\n","        for _, image_name in enumerate(images_list):\n","            image = np.array(Image.open(opj('./open/train_add_data/', image_name)).convert('RGB'))\n","            \n","            aug_img = cv2.resize(image, dsize=(1024, 1024))\n","            aug_img = get_random_crop(aug_img, 900, 900)\n","            aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n","            save_path = opj('./open/train_add_data', f'{label}_{idx}_crop.png')\n","            save_name = f'{label}_{idx}_crop.png'\n","            idx += 1\n","            cv2.imwrite(save_path, aug_img)\n","            train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist()"]},{"cell_type":"markdown","metadata":{"id":"n9tIDY_-P4-E"},"source":["# Rotation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69ujCQjSP4-E"},"outputs":[],"source":["def rotation(img, angle):\n","    angle = int(random.uniform(-angle, angle))\n","    h, w = img.shape[:2]\n","    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n","    img = cv2.warpAffine(img, M, (w, h)) \n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fbx3jbZLP4-F"},"outputs":[],"source":["for label in tqdm(oslabel):\n","    if 'good' not in label:\n","        print(label)\n","        idx = 0\n","        one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n","        images_list = natsorted(one_sample['file_name'])\n","        print(images_list)\n","        for _, image_name in enumerate(images_list):\n","            image = np.array(Image.open(opj('./open/train_add_data/', image_name)).convert('RGB'))\n","            \n","            aug_img = rotation(image, 30)  \n","            aug_img = cv2.resize(aug_img, dsize=(1024, 1024))\n","            aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n","            save_path = opj('./open/train_add_data', f'{label}_{idx}_rotation.png')\n","            save_name = f'{label}_{idx}_rotation.png'\n","            idx += 1\n","            cv2.imwrite(save_path, aug_img)\n","            train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist() "]},{"cell_type":"markdown","metadata":{"id":"hxMMtrwzP4-F"},"source":["# Flip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjHbZrs_P4-G"},"outputs":[],"source":["for label in tqdm(oslabel):\n","    if 'good' not in label:\n","        print(label)\n","        idx = 0\n","        one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n","        images_list = natsorted(one_sample['file_name'])\n","        print(images_list)\n","        for _, image_name in enumerate(images_list):\n","            image = np.array(Image.open(opj('./open/train_add_data/', image_name)).convert('RGB'))\n","            \n","            aug_img = cv2.flip(image, 1)\n","            aug_img = cv2.resize(aug_img, dsize=(1024, 1024))\n","            aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n","            save_path = opj('./open/train_add_data', f'{label}_{idx}_flip1.png')\n","            save_name = f'{label}_{idx}_flip1.png'\n","            idx += 1\n","            cv2.imwrite(save_path, aug_img)\n","            train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist()   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bJbyp8sP4-H"},"outputs":[],"source":["for label in tqdm(oslabel):\n","    if 'good' not in label:\n","        print(label)\n","        idx = 0\n","        one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n","        images_list = natsorted(one_sample['file_name'])\n","        print(images_list)\n","        for _, image_name in enumerate(images_list):\n","            image = np.array(Image.open(opj('./open/train_add_data/', image_name)).convert('RGB'))\n","            \n","            aug_img = cv2.flip(image, 2)\n","            aug_img = cv2.resize(aug_img, dsize=(1024, 1024))\n","            aug_img = cv2.cvtColor(aug_img, cv2.COLOR_BGR2RGB)\n","            save_path = opj('./open/train_add_data', f'{label}_{idx}_flip2.png')\n","            save_name = f'{label}_{idx}_flip2.png'\n","            idx += 1\n","            cv2.imwrite(save_path, aug_img)\n","            train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist()   "]},{"cell_type":"markdown","metadata":{"id":"sIf3pw5KP4-I"},"source":["# label 인코딩 후 증강된 csv 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-3BP6zQP4-I"},"outputs":[],"source":["train_labels = train_df[\"label\"]\n","\n","label_unique = sorted(np.unique(train_labels))\n","label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n","\n","train_labels = [label_unique[k] for k in train_labels]\n","train_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlO4ShSoP4-I"},"outputs":[],"source":["train_df['encoder_label'] = train_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cSE3dBCP4-I"},"outputs":[],"source":["train_df.to_csv('./open/train_df_add_data.csv')"]},{"cell_type":"markdown","metadata":{"id":"rOW-0xT1P4-J"},"source":["# 증강된 데이터 셋 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gkXe7qNP4-J","outputId":"28a74d5d-20b8-4b62-ad1e-13cf784ca463"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Unnamed: 0  file_name       class state            label  encoder_label\n","0           0  10000.png  transistor  good  transistor-good             72\n","1           1  10001.png     capsule  good     capsule-good             15\n","2           2  10002.png  transistor  good  transistor-good             72\n","3           3  10003.png        wood  good        wood-good             76\n","4           4  10004.png      bottle  good      bottle-good              3\n","   index  file_name\n","0      0  20000.png\n","1      1  20001.png\n","2      2  20002.png\n","3      3  20003.png\n","4      4  20004.png\n","(13997, 6)\n","(2154, 2)\n"]}],"source":["DATA_DIR = './open'\n","\n","train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df_add_data.csv'))\n","test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n","\n","print(train_df.head())\n","print(test_df.head())\n","print(train_df.shape)\n","print(test_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJ8D-gr5P4-K","outputId":"8566f180-10ac-4359-fd94-bf40b47eef79"},"outputs":[{"data":{"text/plain":["array(['transistor-good', 'capsule-good', 'wood-good', 'bottle-good',\n","       'screw-good', 'cable-bent_wire', 'carpet-hole', 'hazelnut-good',\n","       'pill-pill_type', 'cable-good', 'metal_nut-scratch', 'pill-good',\n","       'screw-thread_side', 'zipper-fabric_border', 'leather-good',\n","       'pill-scratch', 'toothbrush-good', 'hazelnut-crack',\n","       'screw-manipulated_front', 'zipper-good', 'tile-good',\n","       'carpet-good', 'metal_nut-good', 'bottle-contamination',\n","       'grid-good', 'zipper-split_teeth', 'pill-crack', 'wood-combined',\n","       'pill-color', 'screw-thread_top', 'cable-missing_cable',\n","       'capsule-squeeze', 'zipper-rough', 'capsule-crack', 'capsule-poke',\n","       'metal_nut-flip', 'carpet-metal_contamination', 'metal_nut-color',\n","       'transistor-bent_lead', 'zipper-fabric_interior', 'leather-fold',\n","       'tile-glue_strip', 'screw-scratch_neck', 'screw-scratch_head',\n","       'hazelnut-cut', 'bottle-broken_large', 'bottle-broken_small',\n","       'leather-cut', 'cable-cut_outer_insulation',\n","       'zipper-squeezed_teeth', 'toothbrush-defective',\n","       'cable-cut_inner_insulation', 'pill-contamination',\n","       'cable-missing_wire', 'carpet-thread', 'grid-broken',\n","       'pill-faulty_imprint', 'hazelnut-hole', 'leather-glue',\n","       'leather-poke', 'transistor-damaged_case', 'wood-scratch',\n","       'tile-gray_stroke', 'capsule-faulty_imprint', 'grid-glue',\n","       'zipper-combined', 'carpet-color', 'grid-bent', 'pill-combined',\n","       'hazelnut-print', 'cable-combined', 'capsule-scratch',\n","       'metal_nut-bent', 'zipper-broken_teeth', 'tile-oil',\n","       'transistor-misplaced', 'grid-thread', 'grid-metal_contamination',\n","       'carpet-cut', 'wood-color', 'cable-cable_swap', 'tile-crack',\n","       'leather-color', 'cable-poke_insulation', 'transistor-cut_lead',\n","       'wood-hole', 'tile-rough', 'wood-liquid'], dtype=object)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["train_df['label'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyqbnsQCP4-K","outputId":"df73b88b-ac3f-4414-9490-46bcaaa8971c"},"outputs":[{"data":{"text/plain":["88"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["class_num = len(train_df.encoder_label.unique())\n","class_num"]},{"cell_type":"markdown","metadata":{"id":"h6_MJUMxP4-K"},"source":["# Utils for training and Logging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL2rNes_P4-K"},"outputs":[],"source":["# Warmup Learning rate scheduler\n","from torch.optim.lr_scheduler import _LRScheduler\n","class WarmUpLR(_LRScheduler):\n","    \"\"\"warmup_training learning rate scheduler\n","    Args:\n","        optimizer: optimzier(e.g. SGD)\n","        total_iters: totoal_iters of warmup phase\n","    \"\"\"\n","    def __init__(self, optimizer, total_iters, last_epoch=-1):\n","        \n","        self.total_iters = total_iters\n","        super().__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        \"\"\"we will use the first m batches, and set the learning\n","        rate to base_lr * m / total_iters\n","        \"\"\"\n","        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n","\n","# Logging\n","def get_root_logger(logger_name='basicsr',\n","                    log_level=logging.INFO,\n","                    log_file=None):\n","\n","    logger = logging.getLogger(logger_name)\n","    # if the logger has been initialized, just return it\n","    if logger.hasHandlers():\n","        return logger\n","\n","    format_str = '%(asctime)s %(levelname)s: %(message)s'\n","    logging.basicConfig(format=format_str, level=log_level)\n","\n","    if log_file is not None:\n","        file_handler = logging.FileHandler(log_file, 'w')\n","        file_handler.setFormatter(logging.Formatter(format_str))\n","        file_handler.setLevel(log_level)\n","        logger.addHandler(file_handler)\n","\n","    return logger\n","\n","class AvgMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        self.losses = []\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","        self.losses.append(val)"]},{"cell_type":"markdown","metadata":{"id":"u8qkJuPIP4-L"},"source":["# Dataset & Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlaCQmy6P4-L"},"outputs":[],"source":["class Train_Dataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.img_path = df['file_name'].values\n","        self.target = df['encoder_label'].values \n","        self.transform = transform\n","\n","        print(f'Dataset size:{len(self.img_path)}')\n","\n","    def __getitem__(self, idx):\n","        \n","        image = Image.open(opj('./open/train_add_data/', self.img_path[idx])).convert('RGB')\n","        image = self.transform(image)\n","        target = self.target[idx]\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.img_path)\n","\n","class Test_dataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.img_path = df['file_name'].values\n","        self.transform = transform\n","\n","        print(f'Test Dataset size:{len(self.img_path)}')\n","\n","    def __getitem__(self, idx):\n","\n","        image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n","        image = self.transform(image)\n","\n","        return image\n","\n","    def __len__(self):\n","        return len(self.img_path)\n","\n","def get_loader(df, phase: str, batch_size, shuffle,\n","               num_workers, transform):\n","    if phase == 'test':\n","        dataset = Test_dataset(df, transform)\n","        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n","    else:\n","        dataset = Train_Dataset(df, transform)\n","        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \n","                                 pin_memory=True,\n","                                 drop_last=False)\n","    return data_loader\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PbWoAeCRP4-L"},"source":["# Dataset Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k25lZ8nmP4-M"},"outputs":[],"source":["class RandomRotation(transforms.RandomRotation):\n","    def __init__(self, p: float, degrees: int):\n","        super(RandomRotation, self).__init__(degrees)\n","        self.p = p\n","\n","    def forward(self, img):\n","        if torch.rand(1) < self.p:\n","            fill = self.fill\n","            if isinstance(img, Tensor):\n","                if isinstance(fill, (int, float)):\n","                    fill = [float(fill)] * F.get_image_num_channels(img)\n","                else:\n","                    fill = [float(f) for f in fill]\n","            angle = self.get_params(self.degrees)\n","\n","            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n","        return img"]},{"cell_type":"markdown","metadata":{"id":"xJATcivnP4-M"},"source":["## cait_s36_384"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCzIxu2KP4-M"},"outputs":[],"source":["def get_train_augmentation(img_size, ver):\n","    if ver==1: # for validset\n","        transform = transforms.Compose([\n","                transforms.Resize((img_size, img_size)),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225]),\n","            ])\n","\n","    if ver == 2:\n","\n","        transform = transforms.Compose([\n","                transforms.RandomHorizontalFlip(),\n","                transforms.RandomVerticalFlip(),\n","                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n","                transforms.RandomAffine((-20,20)),\n","                RandomRotation(0.5, degrees=5),\n","                transforms.Resize((img_size, img_size)),\n","                transforms.ToTensor(), \n","                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225]),\n","            ])\n","    \n","    \n","    return transform"]},{"cell_type":"markdown","metadata":{"id":"m44oKCoMP4-N"},"source":["## tf_efficientnet_b7_ns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RC6RTpSNP4-N"},"outputs":[],"source":["# def get_train_augmentation(img_size, ver):\n","#     if ver==1: # for validset\n","#         transform = transforms.Compose([\n","#                 transforms.Resize((img_size, img_size)),\n","#                 transforms.ToTensor(),\n","#                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","#                                      std=[0.229, 0.224, 0.225]),\n","#             ])\n","\n","#     if ver == 2:\n","\n","#         transform = transforms.Compose([\n","#                 transforms.RandomHorizontalFlip(),\n","#                 transforms.RandomVerticalFlip(),\n","#                 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n","#                 transforms.RandomAffine((-20,20)),\n","#                 RandomRotation(0.7, degrees=5),\n","#                 transforms.Resize((img_size, img_size)),\n","#                 transforms.ToTensor(), \n","#                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","#                                      std=[0.229, 0.224, 0.225]),\n","#             ])\n","    \n","    \n","#     return transform"]},{"cell_type":"markdown","metadata":{"id":"V2NddPC8P4-N"},"source":["# Network"]},{"cell_type":"markdown","metadata":{"id":"dVECEdV8P4-O"},"source":["## cait_s36_384"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fbEEYhhP4-O"},"outputs":[],"source":["class Network(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.model_ft = timm.create_model(args.model_name, pretrained=True, num_classes=88)\n","\n","    def forward(self, x):\n","        out = self.model_ft(x)\n","        return out\n","\n","class Network_test(nn.Module):\n","    def __init__(self, encoder_name):\n","        super().__init__()\n","        self.model_ft = timm.create_model(args.model_name, pretrained=True, num_classes=88)\n","\n","    def forward(self, x):\n","        out = self.model_ft(x)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"_dpcpEoNP4-O"},"source":["## tf_efficientnet_b7_ns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fI0HRz0WP4-O"},"outputs":[],"source":["# class Network(nn.Module):\n","#     def __init__(self, args):\n","#         super().__init__()\n","#         self.model_ft = timm.create_model(args.model_name, pretrained=True,  drop_path_rate=args.drop_path_rate,\n","#                                           num_classes=88)\n","\n","        \n","#     def forward(self, x):\n","#         out = self.model_ft(x)\n","#         return out\n","\n","# class Network_test(nn.Module):\n","#     def __init__(self, encoder_name):\n","#         super().__init__()\n","#         self.model_ft = timm.create_model(args.model_name, pretrained=True, num_classes=88)\n","\n","#     def forward(self, x):\n","#         out = self.model_ft(x)\n","#         return out"]},{"cell_type":"markdown","metadata":{"id":"sf2G3__aP4-O"},"source":["# FocalLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykzCOoRlP4-O"},"outputs":[],"source":["class FocalLoss(nn.Module):\n","    \"\"\"\n","    https://dacon.io/competitions/official/235585/codeshare/1796\n","    \"\"\"\n","\n","    def __init__(self, gamma=2.0, eps=1e-7):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        # print(self.gamma)\n","        self.eps = eps\n","        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n","\n","    def forward(self, input, target):\n","        logp = self.ce(input, target)\n","        p = torch.exp(-logp)\n","        loss = (1 - p) ** self.gamma * logp\n","        return loss.mean()"]},{"cell_type":"markdown","metadata":{"id":"XRG3PDzrP4-P"},"source":["# Trainer for Training & Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJYilzGMP4-P"},"outputs":[],"source":["class Trainer():\n","    def __init__(self, args, save_path):\n","        '''\n","        args: arguments\n","        save_path: Model 가중치 저장 경로\n","        '''\n","        super(Trainer, self).__init__()\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # Logging\n","        log_file = os.path.join(save_path, 'log.log')\n","        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n","        self.logger.info(args)\n","        # self.logger.info(args.tag)\n","\n","        # Train, Valid Set load\n","        ############################################################################\n","        if args.step == 0 :\n","            df_train = pd.read_csv(opj(args.data_path, 'train_df_add_data.csv'))\n","        else :\n","            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n","\n","#         if args.image_type is not None:\n","#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n","#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n","\n","        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n","        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['encoder_label'])):\n","            df_train.loc[val_idx, 'fold'] = fold\n","        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n","\n","        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n","        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n","\n","        # Augmentation\n","        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n","        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n","\n","        # TrainLoader\n","        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n","                                       num_workers=args.num_workers, transform=self.train_transform)\n","        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n","                                       num_workers=args.num_workers, transform=self.test_transform)\n","\n","        # Network\n","        self.model = Network(args).to(self.device)\n","\n","        # Loss\n","#         self.criterion = nn.CrossEntropyLoss()\n","        self.criterion = FocalLoss()\n","        \n","        # Optimizer & Scheduler\n","        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr)\n","        \n","        iter_per_epoch = len(self.train_loader)\n","        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n","\n","        if args.scheduler == 'step':\n","            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n","        elif args.scheduler == 'cos':\n","            tmax = args.tmax # half-cycle \n","            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n","        elif args.scheduler == 'cycle':\n","            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n","        else:\n","            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5, mode=\"max\", verbose=True)\n","            \n","        if args.multi_gpu:\n","            self.model = nn.DataParallel(self.model).to(self.device)\n","\n","        # Train / Validate\n","        best_loss = np.inf\n","        best_acc = 0\n","        best_epoch = 0\n","        early_stopping = 0\n","        start = time.time()\n","        for epoch in range(1, args.epochs+1):\n","            self.epoch = epoch\n","\n","            if args.scheduler == 'cos':\n","                if epoch > args.warm_epoch:\n","                    self.scheduler.step()\n","\n","            # Training\n","            train_loss, train_acc, train_f1 = self.training(args)\n","\n","            # Model weight in Multi_GPU or Single GPU\n","            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n","\n","            # Validation\n","            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n","\n","            # Save models\n","            if val_loss < best_loss:\n","                early_stopping = 0\n","                best_epoch = epoch\n","                best_loss = val_loss\n","                best_acc = val_acc\n","                best_f1 = val_f1\n","\n","                torch.save({'epoch':epoch,\n","                            'state_dict':state_dict,\n","                            'optimizer': self.optimizer.state_dict(),\n","                            'scheduler': self.scheduler.state_dict(),\n","                    }, os.path.join(save_path, 'best_model.pth'))\n","                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n","            else:\n","                early_stopping += 1\n","\n","            # Early Stopping\n","            if early_stopping == args.patience:\n","                break\n","\n","        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n","        end = time.time()\n","        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n","\n","    # Training\n","    def training(self, args):\n","        self.model.train()\n","        train_loss = AvgMeter()\n","        train_acc = 0\n","        preds_list = []\n","        targets_list = []\n","\n","        scaler = grad_scaler.GradScaler()\n","        \n","        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n","            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n","            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n","            \n","            if self.epoch <= args.warm_epoch:\n","                self.warmup_scheduler.step()\n","\n","            self.model.zero_grad(set_to_none=True)\n","    \n","            if args.amp:\n","                with autocast():\n","                    preds = self.model(images)\n","                    loss = self.criterion(preds, targets)\n","                    \n","                scaler.scale(loss).backward()\n","\n","                # Gradient Clipping\n","                if args.clipping is not None:\n","                    scaler.unscale_(self.optimizer)\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n","\n","                scaler.step(self.optimizer)\n","                scaler.update()\n","\n","            else:\n","                preds = self.model(images)\n","                loss = self.criterion(preds, targets)\n","                loss.backward()\n","                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n","                self.optimizer.step()\n","\n","            if args.scheduler == 'cycle':\n","                if self.epoch > args.warm_epoch:\n","                    self.scheduler.step()\n","\n","            # Metric\n","            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n","            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n","            targets_list.extend(targets.cpu().detach().numpy())\n","            # log\n","            train_loss.update(loss.item(), n=images.size(0))\n","\n","        train_acc /= len(self.train_loader.dataset)\n","        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n","\n","        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n","        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n","        return train_loss.avg, train_acc, train_f1\n","            \n","    # Validation or Dev\n","    def validate(self, args, phase='val'):\n","        self.model.eval()\n","        with torch.no_grad():\n","            val_loss = AvgMeter()\n","            val_acc = 0\n","            preds_list = []\n","            targets_list = []\n","\n","            for i, (images, targets) in enumerate(self.val_loader):\n","                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n","                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n","\n","                preds = self.model(images)\n","                loss = self.criterion(preds, targets)\n","\n","                # Metric\n","                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n","                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n","                targets_list.extend(targets.cpu().detach().numpy())\n","\n","                # log\n","                val_loss.update(loss.item(), n=images.size(0))\n","            val_acc /= len(self.val_loader.dataset)\n","            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n","\n","            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n","        return val_loss.avg, val_acc, val_f1"]},{"cell_type":"markdown","metadata":{"id":"jSkrVGVZP4-P"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jEFbu5hJP4-Q"},"outputs":[],"source":["def main(args):\n","    print('<---- Training Params ---->')\n","    \n","    # Random Seed\n","    seed = args.seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = True\n","\n","    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n","    \n","    # Create model directory\n","    os.makedirs(save_path, exist_ok=True)\n","    Trainer(args, save_path)\n","\n","    return save_path"]},{"cell_type":"markdown","metadata":{"id":"oKlpzOB-P4-Q"},"source":["# TTA Inference & Ensemble_5fold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufm1gMxuP4-Q"},"outputs":[],"source":["import ttach as tta\n","\n","def predict(encoder_name, test_loader, device, model_path):\n","    \n","    model = Network_test(encoder_name).to(device)\n","    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n","    \n","#     model.eval()\n","    tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.d4_transform())\n","    tta_model.eval()\n","    \n","    preds_list = []\n","    with torch.no_grad():\n","        for images in tqdm(test_loader):\n","            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n","            preds = tta_model(images)\n","            preds = torch.softmax(preds, dim=1)\n","            preds_list.extend(preds.cpu().tolist())\n","\n","    return np.array(preds_list)\n","\n","def ensemble_5fold(model_path_list, test_loader, device):\n","    predict_list = []\n","    for model_path in model_path_list:\n","        prediction = predict(encoder_name=args.model_name, test_loader = test_loader, device = device, model_path = model_path)\n","        predict_list.append(prediction)\n","    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n","\n","    return ensemble\n"]},{"cell_type":"markdown","metadata":{"id":"V1WKmCuoP4-Q"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPNgsXwTP4-Q","outputId":"e593d2f3-69c6-41de-c3c1-429bc1bf30a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device: cuda\n","Current cuda device: 0\n","Count of using GPUs: 2\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print('Device:', device)\n","print('Current cuda device:', torch.cuda.current_device())\n","print('Count of using GPUs:', torch.cuda.device_count())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YWwAXzSP4-R"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","sub = pd.read_csv('./open/sample_submission.csv')\n","df_train = pd.read_csv('./open/train_df_add_data.csv')\n","df_test = pd.read_csv('./open/test_df.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"-jAxq4JZP4-R"},"outputs":[],"source":["args.step = 0\n","models_path = []\n","for s_fold in range(5): # 5fold\n","    args.fold = s_fold\n","    args.exp_num = str(s_fold)\n","    save_path = main(args)\n","    models_path.append(save_path)"]},{"cell_type":"markdown","metadata":{"id":"GTuG4IQNP4-R"},"source":["# Inference & Ensemble_Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnaQ1obVP4-R","outputId":"85f39e63-69d7-4948-9aa9-7d72c16502ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Dataset size:2154\n"]}],"source":["img_size = 600\n","args.model_name = 'tf_efficientnet_b7_ns'\n","\n","test_transform = get_train_augmentation(img_size=img_size, ver=1)\n","test_dataset = Test_dataset(df_test, test_transform)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76jr9U8WP4-S"},"outputs":[],"source":["models_path = ['./label_results_tf_efficientnet_b7_ns_600/000', \n","               './label_results_tf_efficientnet_b7_ns_600/001', \n","               './label_results_tf_efficientnet_b7_ns_600/002', \n","               './label_results_tf_efficientnet_b7_ns_600/003', \n","               './label_results_tf_efficientnet_b7_ns_600/004']"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EIHXiGuyP4-S"},"outputs":[],"source":["ensemble1 = ensemble_5fold(models_path, test_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soLJTEeUP4-S","outputId":"1032bcdc-501f-4e9d-eb00-6a8fbbb5b786"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Dataset size:2154\n"]}],"source":["img_size = 384\n","args.model_name = 'cait_s36_384'\n","\n","test_transform = get_train_augmentation(img_size=img_size, ver=1)\n","test_dataset = Test_dataset(df_test, test_transform)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TLIq8xLZP4-S"},"outputs":[],"source":["models_path = ['./label_results_cait_s36_384/000', \n","               './label_results_cait_s36_384/001', \n","               './label_results_cait_s36_384/002', \n","               './label_results_cait_s36_384/003', \n","               './label_results_cait_s36_384/004']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psj2NrxvP4-S"},"outputs":[],"source":["ensemble2 = ensemble_5fold(models_path, test_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSCugq0ZP4-T"},"outputs":[],"source":["ensemble = (ensemble1+ensemble2) / 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIKHbgrBP4-T"},"outputs":[],"source":["f_pred = ensemble.argmax(axis=1).tolist()\n","f_pred"]},{"cell_type":"markdown","metadata":{"id":"tfdeTLiOP4-T"},"source":["# Submit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wpc5L7n4P4-T"},"outputs":[],"source":["train_y = pd.read_csv(\"./open/train_df_add_data.csv\")\n","\n","train_labels = train_y[\"label\"]\n","\n","label_unique = sorted(np.unique(train_labels))\n","label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJ63kWEbP4-T"},"outputs":[],"source":["label_decoder = {val:key for key, val in label_unique.items()}\n","\n","f_result = [label_decoder[result] for result in f_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4P15asG3P4-T"},"outputs":[],"source":["f_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sz0YzqzuP4-U"},"outputs":[],"source":["submission = pd.read_csv(\"./open/sample_submission.csv\")\n","\n","submission[\"label\"] = f_result\n","\n","submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufYJUoCUP4-U"},"outputs":[],"source":["submission.to_csv(\"./submission/final.csv\", index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvHGEI2cP4-U"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"esemble_final.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}