{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "import torch.cuda.amp as amp\n",
    "from adamp import AdamP, SGDP\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./open',\n",
    "     'Kfold':5,\n",
    "     'model_path':'label_results_tf_efficientnet_b4_ns/',\n",
    "     'image_type':'train_1024', \n",
    "     'class_num' : 88,\n",
    "\n",
    "     # Model parameter settings\n",
    "     'model_name':'tf_efficientnet_b4_ns',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':512,\n",
    "     'batch_size':16,\n",
    "     'epochs':100,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-4,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'Reduce',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-5,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience': 5,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':True,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "     \n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,3\"  # Set the GPUs 2 and 3 to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 증강된 데이터 셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  file_name       class state            label  encoder_label\n",
      "0           0  10000.png  transistor  good  transistor-good             72\n",
      "1           1  10001.png     capsule  good     capsule-good             15\n",
      "2           2  10002.png  transistor  good  transistor-good             72\n",
      "3           3  10003.png        wood  good        wood-good             76\n",
      "4           4  10004.png      bottle  good      bottle-good              3\n",
      "   index  file_name\n",
      "0      0  20000.png\n",
      "1      1  20001.png\n",
      "2      2  20002.png\n",
      "3      3  20003.png\n",
      "4      4  20004.png\n",
      "(13997, 6)\n",
      "(2154, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './open'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df_add_data.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['transistor-good', 'capsule-good', 'wood-good', 'bottle-good',\n",
       "       'screw-good', 'cable-bent_wire', 'carpet-hole', 'hazelnut-good',\n",
       "       'pill-pill_type', 'cable-good', 'metal_nut-scratch', 'pill-good',\n",
       "       'screw-thread_side', 'zipper-fabric_border', 'leather-good',\n",
       "       'pill-scratch', 'toothbrush-good', 'hazelnut-crack',\n",
       "       'screw-manipulated_front', 'zipper-good', 'tile-good',\n",
       "       'carpet-good', 'metal_nut-good', 'bottle-contamination',\n",
       "       'grid-good', 'zipper-split_teeth', 'pill-crack', 'wood-combined',\n",
       "       'pill-color', 'screw-thread_top', 'cable-missing_cable',\n",
       "       'capsule-squeeze', 'zipper-rough', 'capsule-crack', 'capsule-poke',\n",
       "       'metal_nut-flip', 'carpet-metal_contamination', 'metal_nut-color',\n",
       "       'transistor-bent_lead', 'zipper-fabric_interior', 'leather-fold',\n",
       "       'tile-glue_strip', 'screw-scratch_neck', 'screw-scratch_head',\n",
       "       'hazelnut-cut', 'bottle-broken_large', 'bottle-broken_small',\n",
       "       'leather-cut', 'cable-cut_outer_insulation',\n",
       "       'zipper-squeezed_teeth', 'toothbrush-defective',\n",
       "       'cable-cut_inner_insulation', 'pill-contamination',\n",
       "       'cable-missing_wire', 'carpet-thread', 'grid-broken',\n",
       "       'pill-faulty_imprint', 'hazelnut-hole', 'leather-glue',\n",
       "       'leather-poke', 'transistor-damaged_case', 'wood-scratch',\n",
       "       'tile-gray_stroke', 'capsule-faulty_imprint', 'grid-glue',\n",
       "       'zipper-combined', 'carpet-color', 'grid-bent', 'pill-combined',\n",
       "       'hazelnut-print', 'cable-combined', 'capsule-scratch',\n",
       "       'metal_nut-bent', 'zipper-broken_teeth', 'tile-oil',\n",
       "       'transistor-misplaced', 'grid-thread', 'grid-metal_contamination',\n",
       "       'carpet-cut', 'wood-color', 'cable-cable_swap', 'tile-crack',\n",
       "       'leather-color', 'cable-poke_insulation', 'transistor-cut_lead',\n",
       "       'wood-hole', 'tile-rough', 'wood-liquid'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_num = len(train_df.encoder_label.unique())\n",
    "class_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, p: float, degrees: int):\n",
    "        super(RandomRotation, self).__init__(degrees)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            fill = self.fill\n",
    "            if isinstance(img, Tensor):\n",
    "                if isinstance(fill, (int, float)):\n",
    "                    fill = [float(fill)] * F.get_image_num_channels(img)\n",
    "                else:\n",
    "                    fill = [float(f) for f in fill]\n",
    "            angle = self.get_params(self.degrees)\n",
    "\n",
    "            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.target = df['encoder_label'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = Image.open(opj('./open/train_add_data/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \n",
    "                                 pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    if ver == 2:\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "                transforms.RandomAffine((-20,20)),\n",
    "                RandomRotation(0.5, degrees=5),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model(args.model_name, pretrained=True, num_classes=88)\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model(args.model_name, pretrained=True, num_classes=88)\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight: tensor([2.4437, 2.2216, 2.2216, 1.8708, 3.4911, 4.0729, 4.0729, 3.4911, 4.8875,\n",
      "        1.7455, 4.0729, 4.8875, 4.8875, 2.0365, 2.2216, 1.7854, 2.2216, 2.0365,\n",
      "        2.4437, 2.4437, 2.7153, 1.3964, 2.7153, 2.7153, 2.4437, 4.0729, 4.0729,\n",
      "        4.0729, 1.4811, 4.0729, 4.0729, 2.7153, 2.7153, 1.0000, 2.7153, 2.7153,\n",
      "        2.4437, 2.4437, 2.7153, 2.4437, 1.5959, 2.7153, 1.8798, 2.2216, 2.0365,\n",
      "        1.7773, 2.0365, 1.8798, 2.7153, 2.2216, 1.8798, 2.4437, 1.4644, 4.8875,\n",
      "        2.0365, 1.2219, 2.0365, 2.0365, 1.8798, 2.0365, 2.0365, 2.7153, 2.7153,\n",
      "        1.7000, 3.0547, 2.7153, 3.0547, 1.6292, 6.5167, 4.8875, 4.8875, 4.8875,\n",
      "        1.8357, 4.8875, 6.1094, 4.0729, 1.5830, 4.8875, 4.8875, 2.2216, 2.4437,\n",
      "        3.0547, 2.7153, 3.0547, 1.6292, 2.7153, 2.7153, 3.0547],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # weighted crossentropy loss를 위한 weight 계산 함수\n",
    "\n",
    "class_num = train_df.groupby([\"encoder_label\"])[\"encoder_label\"].count().tolist()\n",
    "class_weight = torch.tensor(np.max(class_num) / class_num).to(\"cuda\", dtype=torch.float)\n",
    "print(f\"class_weight: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://dacon.io/competitions/official/235585/codeshare/1796\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train_df_add_data.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "#         if args.image_type is not None:\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['encoder_label'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "\n",
    "        # Loss\n",
    "#         self.criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "        self.criterion = FocalLoss()\n",
    "#         self.criterion = CutMixCrossEntropyLoss(True)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "#         self.optimizer = Lookahead(torch.optim.Adam(self.model.parameters(), lr=args.initial_lr), k=5, alpha=0.5)\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5, mode=\"max\", verbose=True)\n",
    "            \n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        \n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "    \n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                    \n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./open/sample_submission.csv')\n",
    "df_train = pd.read_csv('./open/train_df_add_data.csv')\n",
    "df_test = pd.read_csv('./open/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "\n",
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "#     model.eval()\n",
    "    tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.d4_transform())\n",
    "    tta_model.eval()\n",
    "    \n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = tta_model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'tf_efficientnet_b4_ns', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 23:27:28,721 INFO: {'exp_num': '0', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_tf_efficientnet_b4_ns/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'tf_efficientnet_b4_ns', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11197\n",
      "Dataset size:2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 23:27:29,121 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b4_ns-d6313a46.pth\n",
      "100%|██████████| 700/700 [12:07<00:00,  1.04s/it]\n",
      "2022-05-04 23:57:57,718 INFO: Epoch:[001/100]\n",
      "2022-05-04 23:57:57,719 INFO: Train Loss:4.062 | Acc:0.0874 | F1:0.0602\n",
      "2022-05-04 23:58:35,130 INFO: val Loss:3.273 | Acc:0.2339 | F1:0.1192\n",
      "2022-05-04 23:58:35,699 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 700/700 [11:54<00:00,  1.02s/it]\n",
      "2022-05-05 00:10:30,233 INFO: Epoch:[002/100]\n",
      "2022-05-05 00:10:30,234 INFO: Train Loss:2.188 | Acc:0.3550 | F1:0.2256\n",
      "2022-05-05 00:11:08,695 INFO: val Loss:1.281 | Acc:0.4668 | F1:0.3397\n",
      "2022-05-05 00:11:09,410 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 700/700 [11:29<00:00,  1.02it/s]\n",
      "2022-05-05 00:22:38,930 INFO: Epoch:[003/100]\n",
      "2022-05-05 00:22:38,931 INFO: Train Loss:0.929 | Acc:0.5941 | F1:0.5282\n",
      "2022-05-05 00:23:17,068 INFO: val Loss:0.411 | Acc:0.7671 | F1:0.7166\n",
      "2022-05-05 00:23:17,785 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 700/700 [12:01<00:00,  1.03s/it]\n",
      "2022-05-05 00:35:19,600 INFO: Epoch:[004/100]\n",
      "2022-05-05 00:35:19,601 INFO: Train Loss:0.308 | Acc:0.8508 | F1:0.8363\n",
      "2022-05-05 00:35:56,794 INFO: val Loss:0.107 | Acc:0.9321 | F1:0.9243\n",
      "2022-05-05 00:35:57,500 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 700/700 [11:53<00:00,  1.02s/it]\n",
      "2022-05-05 00:47:51,474 INFO: Epoch:[005/100]\n",
      "2022-05-05 00:47:51,475 INFO: Train Loss:0.099 | Acc:0.9382 | F1:0.9357\n",
      "2022-05-05 00:48:29,334 INFO: val Loss:0.039 | Acc:0.9721 | F1:0.9708\n",
      "2022-05-05 00:48:30,045 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 700/700 [11:44<00:00,  1.01s/it]\n",
      "2022-05-05 01:00:14,149 INFO: Epoch:[006/100]\n",
      "2022-05-05 01:00:14,150 INFO: Train Loss:0.051 | Acc:0.9661 | F1:0.9655\n",
      "2022-05-05 01:00:51,503 INFO: val Loss:0.024 | Acc:0.9836 | F1:0.9813\n",
      "2022-05-05 01:00:52,295 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 700/700 [11:58<00:00,  1.03s/it]\n",
      "2022-05-05 01:12:50,755 INFO: Epoch:[007/100]\n",
      "2022-05-05 01:12:50,756 INFO: Train Loss:0.036 | Acc:0.9752 | F1:0.9751\n",
      "2022-05-05 01:13:27,965 INFO: val Loss:0.011 | Acc:0.9904 | F1:0.9886\n",
      "2022-05-05 01:13:28,765 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 700/700 [11:57<00:00,  1.02s/it]\n",
      "2022-05-05 01:25:25,880 INFO: Epoch:[008/100]\n",
      "2022-05-05 01:25:25,881 INFO: Train Loss:0.028 | Acc:0.9804 | F1:0.9804\n",
      "2022-05-05 01:26:03,141 INFO: val Loss:0.017 | Acc:0.9904 | F1:0.9902\n",
      "100%|██████████| 700/700 [11:54<00:00,  1.02s/it]\n",
      "2022-05-05 01:37:57,344 INFO: Epoch:[009/100]\n",
      "2022-05-05 01:37:57,345 INFO: Train Loss:0.022 | Acc:0.9846 | F1:0.9836\n",
      "2022-05-05 01:38:34,478 INFO: val Loss:0.016 | Acc:0.9918 | F1:0.9917\n",
      "100%|██████████| 700/700 [11:16<00:00,  1.03it/s]\n",
      "2022-05-05 01:49:51,039 INFO: Epoch:[010/100]\n",
      "2022-05-05 01:49:51,040 INFO: Train Loss:0.018 | Acc:0.9885 | F1:0.9879\n",
      "2022-05-05 01:50:27,426 INFO: val Loss:0.017 | Acc:0.9861 | F1:0.9848\n",
      "100%|██████████| 700/700 [11:52<00:00,  1.02s/it]\n",
      "2022-05-05 02:02:19,863 INFO: Epoch:[011/100]\n",
      "2022-05-05 02:02:19,864 INFO: Train Loss:0.018 | Acc:0.9884 | F1:0.9889\n",
      "2022-05-05 02:02:57,183 INFO: val Loss:0.005 | Acc:0.9946 | F1:0.9945\n",
      "2022-05-05 02:02:58,315 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 700/700 [11:49<00:00,  1.01s/it]\n",
      "2022-05-05 02:14:47,529 INFO: Epoch:[012/100]\n",
      "2022-05-05 02:14:47,529 INFO: Train Loss:0.016 | Acc:0.9897 | F1:0.9900\n",
      "2022-05-05 02:15:25,788 INFO: val Loss:0.003 | Acc:0.9975 | F1:0.9974\n",
      "2022-05-05 02:15:26,485 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 700/700 [11:32<00:00,  1.01it/s]\n",
      "2022-05-05 02:26:59,182 INFO: Epoch:[013/100]\n",
      "2022-05-05 02:26:59,183 INFO: Train Loss:0.015 | Acc:0.9911 | F1:0.9913\n",
      "2022-05-05 02:27:36,088 INFO: val Loss:0.002 | Acc:0.9982 | F1:0.9984\n",
      "2022-05-05 02:27:36,898 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 700/700 [11:28<00:00,  1.02it/s]\n",
      "2022-05-05 02:39:05,586 INFO: Epoch:[014/100]\n",
      "2022-05-05 02:39:05,586 INFO: Train Loss:0.012 | Acc:0.9912 | F1:0.9917\n",
      "2022-05-05 02:39:43,129 INFO: val Loss:0.016 | Acc:0.9918 | F1:0.9922\n",
      "100%|██████████| 700/700 [11:54<00:00,  1.02s/it]\n",
      "2022-05-05 02:51:37,747 INFO: Epoch:[015/100]\n",
      "2022-05-05 02:51:37,748 INFO: Train Loss:0.015 | Acc:0.9908 | F1:0.9909\n",
      "2022-05-05 02:52:14,452 INFO: val Loss:0.006 | Acc:0.9946 | F1:0.9946\n",
      "100%|██████████| 700/700 [11:51<00:00,  1.02s/it]\n",
      "2022-05-05 03:04:05,496 INFO: Epoch:[016/100]\n",
      "2022-05-05 03:04:05,497 INFO: Train Loss:0.012 | Acc:0.9931 | F1:0.9932\n",
      "2022-05-05 03:04:43,259 INFO: val Loss:0.003 | Acc:0.9975 | F1:0.9977\n",
      "100%|██████████| 700/700 [11:25<00:00,  1.02it/s]\n",
      "2022-05-05 03:16:08,403 INFO: Epoch:[017/100]\n",
      "2022-05-05 03:16:08,404 INFO: Train Loss:0.012 | Acc:0.9933 | F1:0.9932\n",
      "2022-05-05 03:16:45,330 INFO: val Loss:0.012 | Acc:0.9939 | F1:0.9935\n",
      "100%|██████████| 700/700 [11:35<00:00,  1.01it/s]\n",
      "2022-05-05 03:28:20,789 INFO: Epoch:[018/100]\n",
      "2022-05-05 03:28:20,790 INFO: Train Loss:0.010 | Acc:0.9938 | F1:0.9932\n",
      "2022-05-05 03:28:58,834 INFO: val Loss:0.004 | Acc:0.9975 | F1:0.9979\n",
      "2022-05-05 03:28:58,836 INFO: \n",
      "Best Val Epoch:13 | Val Loss:0.0022 | Val Acc:0.9982 | Val F1:0.9984\n",
      "2022-05-05 03:28:58,836 INFO: Total Process time:223.152Minute\n",
      "2022-05-05 03:28:58,852 INFO: {'exp_num': '1', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_tf_efficientnet_b4_ns/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'tf_efficientnet_b4_ns', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11197\n",
      "Dataset size:2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 03:28:59,271 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 700/700 [12:00<00:00,  1.03s/it]\n",
      "2022-05-05 03:41:00,738 INFO: Epoch:[001/100]\n",
      "2022-05-05 03:41:00,739 INFO: Train Loss:4.065 | Acc:0.0895 | F1:0.0624\n",
      "2022-05-05 03:41:38,227 INFO: val Loss:3.251 | Acc:0.2136 | F1:0.1054\n",
      "2022-05-05 03:41:38,866 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 700/700 [12:03<00:00,  1.03s/it]\n",
      "2022-05-05 03:53:42,460 INFO: Epoch:[002/100]\n",
      "2022-05-05 03:53:42,461 INFO: Train Loss:2.188 | Acc:0.3580 | F1:0.2283\n",
      "2022-05-05 03:54:20,307 INFO: val Loss:1.205 | Acc:0.4393 | F1:0.3205\n",
      "2022-05-05 03:54:21,026 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 700/700 [11:52<00:00,  1.02s/it]\n",
      "2022-05-05 04:06:13,496 INFO: Epoch:[003/100]\n",
      "2022-05-05 04:06:13,497 INFO: Train Loss:0.901 | Acc:0.6066 | F1:0.5433\n",
      "2022-05-05 04:06:49,988 INFO: val Loss:0.394 | Acc:0.7521 | F1:0.7003\n",
      "2022-05-05 04:06:50,711 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 700/700 [11:39<00:00,  1.00it/s]\n",
      "2022-05-05 04:18:30,258 INFO: Epoch:[004/100]\n",
      "2022-05-05 04:18:30,259 INFO: Train Loss:0.305 | Acc:0.8495 | F1:0.8347\n",
      "2022-05-05 04:19:07,290 INFO: val Loss:0.074 | Acc:0.9489 | F1:0.9492\n",
      "2022-05-05 04:19:08,077 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 700/700 [11:53<00:00,  1.02s/it]\n",
      "2022-05-05 04:31:01,131 INFO: Epoch:[005/100]\n",
      "2022-05-05 04:31:01,132 INFO: Train Loss:0.096 | Acc:0.9394 | F1:0.9379\n",
      "2022-05-05 04:31:38,290 INFO: val Loss:0.033 | Acc:0.9764 | F1:0.9759\n",
      "2022-05-05 04:31:39,122 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 700/700 [11:55<00:00,  1.02s/it]\n",
      "2022-05-05 04:43:34,334 INFO: Epoch:[006/100]\n",
      "2022-05-05 04:43:34,335 INFO: Train Loss:0.053 | Acc:0.9667 | F1:0.9644\n",
      "2022-05-05 04:44:11,318 INFO: val Loss:0.015 | Acc:0.9893 | F1:0.9866\n",
      "2022-05-05 04:44:12,041 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 700/700 [11:46<00:00,  1.01s/it]\n",
      "2022-05-05 04:55:58,079 INFO: Epoch:[007/100]\n",
      "2022-05-05 04:55:58,080 INFO: Train Loss:0.033 | Acc:0.9778 | F1:0.9777\n",
      "2022-05-05 04:56:34,474 INFO: val Loss:0.026 | Acc:0.9871 | F1:0.9848\n",
      "100%|██████████| 700/700 [11:48<00:00,  1.01s/it]\n",
      "2022-05-05 05:08:22,545 INFO: Epoch:[008/100]\n",
      "2022-05-05 05:08:22,546 INFO: Train Loss:0.028 | Acc:0.9807 | F1:0.9804\n",
      "2022-05-05 05:08:59,590 INFO: val Loss:0.014 | Acc:0.9754 | F1:0.9782\n",
      "2022-05-05 05:09:00,349 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 700/700 [11:47<00:00,  1.01s/it]\n",
      "2022-05-05 05:20:48,004 INFO: Epoch:[009/100]\n",
      "2022-05-05 05:20:48,005 INFO: Train Loss:0.023 | Acc:0.9847 | F1:0.9839\n",
      "2022-05-05 05:21:25,349 INFO: val Loss:0.015 | Acc:0.9914 | F1:0.9907\n",
      "100%|██████████| 700/700 [11:50<00:00,  1.01s/it]\n",
      "2022-05-05 05:33:15,616 INFO: Epoch:[010/100]\n",
      "2022-05-05 05:33:15,617 INFO: Train Loss:0.022 | Acc:0.9855 | F1:0.9856\n",
      "2022-05-05 05:33:52,319 INFO: val Loss:0.006 | Acc:0.9939 | F1:0.9942\n",
      "2022-05-05 05:33:52,970 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 700/700 [12:01<00:00,  1.03s/it]\n",
      "2022-05-05 05:45:54,788 INFO: Epoch:[011/100]\n",
      "2022-05-05 05:45:54,789 INFO: Train Loss:0.016 | Acc:0.9887 | F1:0.9890\n",
      "2022-05-05 05:46:32,130 INFO: val Loss:0.003 | Acc:0.9982 | F1:0.9985\n",
      "2022-05-05 05:46:32,866 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 700/700 [11:56<00:00,  1.02s/it]\n",
      "2022-05-05 05:58:29,419 INFO: Epoch:[012/100]\n",
      "2022-05-05 05:58:29,420 INFO: Train Loss:0.016 | Acc:0.9891 | F1:0.9898\n",
      "2022-05-05 05:59:05,826 INFO: val Loss:0.010 | Acc:0.9957 | F1:0.9965\n",
      "100%|██████████| 700/700 [11:47<00:00,  1.01s/it]\n",
      "2022-05-05 06:10:53,327 INFO: Epoch:[013/100]\n",
      "2022-05-05 06:10:53,327 INFO: Train Loss:0.014 | Acc:0.9901 | F1:0.9910\n",
      "2022-05-05 06:11:30,425 INFO: val Loss:0.004 | Acc:0.9971 | F1:0.9964\n",
      "100%|██████████| 700/700 [11:46<00:00,  1.01s/it]\n",
      "2022-05-05 06:23:17,435 INFO: Epoch:[014/100]\n",
      "2022-05-05 06:23:17,435 INFO: Train Loss:0.012 | Acc:0.9913 | F1:0.9910\n",
      "2022-05-05 06:23:54,979 INFO: val Loss:0.014 | Acc:0.9918 | F1:0.9930\n",
      "100%|██████████| 700/700 [11:47<00:00,  1.01s/it]\n",
      "2022-05-05 06:35:42,034 INFO: Epoch:[015/100]\n",
      "2022-05-05 06:35:42,035 INFO: Train Loss:0.011 | Acc:0.9927 | F1:0.9929\n",
      "2022-05-05 06:36:19,256 INFO: val Loss:0.004 | Acc:0.9968 | F1:0.9972\n",
      "100%|██████████| 700/700 [11:48<00:00,  1.01s/it]\n",
      "2022-05-05 06:48:08,192 INFO: Epoch:[016/100]\n",
      "2022-05-05 06:48:08,193 INFO: Train Loss:0.014 | Acc:0.9928 | F1:0.9928\n",
      "2022-05-05 06:48:45,192 INFO: val Loss:0.005 | Acc:0.9968 | F1:0.9976\n",
      "2022-05-05 06:48:45,194 INFO: \n",
      "Best Val Epoch:11 | Val Loss:0.0028 | Val Acc:0.9982 | Val F1:0.9985\n",
      "2022-05-05 06:48:45,195 INFO: Total Process time:199.755Minute\n",
      "2022-05-05 06:48:45,206 INFO: {'exp_num': '2', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_tf_efficientnet_b4_ns/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'tf_efficientnet_b4_ns', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11198\n",
      "Dataset size:2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 06:48:45,586 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 700/700 [12:00<00:00,  1.03s/it]\n",
      "2022-05-05 07:00:46,242 INFO: Epoch:[001/100]\n",
      "2022-05-05 07:00:46,243 INFO: Train Loss:4.069 | Acc:0.0812 | F1:0.0555\n",
      "2022-05-05 07:01:21,886 INFO: val Loss:3.226 | Acc:0.2262 | F1:0.1184\n",
      "2022-05-05 07:01:22,531 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 700/700 [11:52<00:00,  1.02s/it]\n",
      "2022-05-05 07:13:14,780 INFO: Epoch:[002/100]\n",
      "2022-05-05 07:13:14,781 INFO: Train Loss:2.180 | Acc:0.3525 | F1:0.2243\n",
      "2022-05-05 07:13:52,168 INFO: val Loss:1.296 | Acc:0.4562 | F1:0.3124\n",
      "2022-05-05 07:13:52,902 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 700/700 [11:56<00:00,  1.02s/it]\n",
      "2022-05-05 07:25:49,713 INFO: Epoch:[003/100]\n",
      "2022-05-05 07:25:49,714 INFO: Train Loss:0.933 | Acc:0.5928 | F1:0.5276\n",
      "2022-05-05 07:26:26,422 INFO: val Loss:0.406 | Acc:0.8010 | F1:0.7451\n",
      "2022-05-05 07:26:27,120 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 700/700 [11:57<00:00,  1.02s/it]\n",
      "2022-05-05 07:38:24,565 INFO: Epoch:[004/100]\n",
      "2022-05-05 07:38:24,566 INFO: Train Loss:0.314 | Acc:0.8410 | F1:0.8245\n",
      "2022-05-05 07:39:02,255 INFO: val Loss:0.092 | Acc:0.9293 | F1:0.9162\n",
      "2022-05-05 07:39:03,008 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 700/700 [11:51<00:00,  1.02s/it]\n",
      "2022-05-05 07:50:54,811 INFO: Epoch:[005/100]\n",
      "2022-05-05 07:50:54,812 INFO: Train Loss:0.100 | Acc:0.9370 | F1:0.9358\n",
      "2022-05-05 07:51:31,646 INFO: val Loss:0.072 | Acc:0.9582 | F1:0.9569\n",
      "2022-05-05 07:51:32,328 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 700/700 [11:50<00:00,  1.02s/it]\n",
      "2022-05-05 08:03:23,330 INFO: Epoch:[006/100]\n",
      "2022-05-05 08:03:23,331 INFO: Train Loss:0.053 | Acc:0.9645 | F1:0.9632\n",
      "2022-05-05 08:04:01,276 INFO: val Loss:0.046 | Acc:0.9707 | F1:0.9695\n",
      "2022-05-05 08:04:01,994 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 700/700 [12:07<00:00,  1.04s/it]\n",
      "2022-05-05 08:16:09,125 INFO: Epoch:[007/100]\n",
      "2022-05-05 08:16:09,125 INFO: Train Loss:0.037 | Acc:0.9759 | F1:0.9757\n",
      "2022-05-05 08:16:46,180 INFO: val Loss:0.023 | Acc:0.9886 | F1:0.9879\n",
      "2022-05-05 08:16:46,918 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 700/700 [12:00<00:00,  1.03s/it]\n",
      "2022-05-05 08:28:47,451 INFO: Epoch:[008/100]\n",
      "2022-05-05 08:28:47,451 INFO: Train Loss:0.029 | Acc:0.9805 | F1:0.9796\n",
      "2022-05-05 08:29:24,921 INFO: val Loss:0.016 | Acc:0.9886 | F1:0.9883\n",
      "2022-05-05 08:29:25,645 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 700/700 [12:08<00:00,  1.04s/it]\n",
      "2022-05-05 08:41:33,752 INFO: Epoch:[009/100]\n",
      "2022-05-05 08:41:33,752 INFO: Train Loss:0.024 | Acc:0.9838 | F1:0.9843\n",
      "2022-05-05 08:42:11,564 INFO: val Loss:0.015 | Acc:0.9929 | F1:0.9937\n",
      "2022-05-05 08:42:12,287 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 700/700 [11:43<00:00,  1.00s/it]\n",
      "2022-05-05 08:53:55,431 INFO: Epoch:[010/100]\n",
      "2022-05-05 08:53:55,431 INFO: Train Loss:0.020 | Acc:0.9876 | F1:0.9880\n",
      "2022-05-05 08:54:33,748 INFO: val Loss:0.028 | Acc:0.9900 | F1:0.9915\n",
      "100%|██████████| 700/700 [12:00<00:00,  1.03s/it]\n",
      "2022-05-05 09:06:34,559 INFO: Epoch:[011/100]\n",
      "2022-05-05 09:06:34,560 INFO: Train Loss:0.016 | Acc:0.9893 | F1:0.9895\n",
      "2022-05-05 09:07:11,570 INFO: val Loss:0.032 | Acc:0.9896 | F1:0.9905\n",
      "100%|██████████| 700/700 [11:48<00:00,  1.01s/it]\n",
      "2022-05-05 09:18:59,830 INFO: Epoch:[012/100]\n",
      "2022-05-05 09:18:59,831 INFO: Train Loss:0.014 | Acc:0.9904 | F1:0.9901\n",
      "2022-05-05 09:19:36,473 INFO: val Loss:0.010 | Acc:0.9954 | F1:0.9944\n",
      "2022-05-05 09:19:37,192 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 700/700 [11:42<00:00,  1.00s/it]\n",
      "2022-05-05 09:31:19,337 INFO: Epoch:[013/100]\n",
      "2022-05-05 09:31:19,337 INFO: Train Loss:0.014 | Acc:0.9905 | F1:0.9909\n",
      "2022-05-05 09:31:57,048 INFO: val Loss:0.007 | Acc:0.9950 | F1:0.9955\n",
      "2022-05-05 09:31:57,822 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 700/700 [12:04<00:00,  1.03s/it]\n",
      "2022-05-05 09:44:02,309 INFO: Epoch:[014/100]\n",
      "2022-05-05 09:44:02,310 INFO: Train Loss:0.014 | Acc:0.9909 | F1:0.9911\n",
      "2022-05-05 09:44:39,841 INFO: val Loss:0.012 | Acc:0.9968 | F1:0.9971\n",
      "100%|██████████| 700/700 [12:00<00:00,  1.03s/it]\n",
      "2022-05-05 09:56:39,993 INFO: Epoch:[015/100]\n",
      "2022-05-05 09:56:39,994 INFO: Train Loss:0.011 | Acc:0.9934 | F1:0.9924\n",
      "2022-05-05 09:57:18,074 INFO: val Loss:0.005 | Acc:0.9964 | F1:0.9961\n",
      "2022-05-05 09:57:18,804 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 700/700 [12:06<00:00,  1.04s/it]\n",
      "2022-05-05 10:09:25,300 INFO: Epoch:[016/100]\n",
      "2022-05-05 10:09:25,301 INFO: Train Loss:0.013 | Acc:0.9917 | F1:0.9917\n",
      "2022-05-05 10:10:03,878 INFO: val Loss:0.005 | Acc:0.9979 | F1:0.9981\n",
      "2022-05-05 10:10:04,597 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 700/700 [11:57<00:00,  1.03s/it]\n",
      "2022-05-05 10:22:02,458 INFO: Epoch:[017/100]\n",
      "2022-05-05 10:22:02,459 INFO: Train Loss:0.012 | Acc:0.9924 | F1:0.9917\n",
      "2022-05-05 10:22:40,207 INFO: val Loss:0.017 | Acc:0.9954 | F1:0.9957\n",
      "100%|██████████| 700/700 [11:58<00:00,  1.03s/it]\n",
      "2022-05-05 10:34:38,793 INFO: Epoch:[018/100]\n",
      "2022-05-05 10:34:38,794 INFO: Train Loss:0.011 | Acc:0.9930 | F1:0.9937\n",
      "2022-05-05 10:35:17,388 INFO: val Loss:0.007 | Acc:0.9954 | F1:0.9959\n",
      "100%|██████████| 700/700 [11:56<00:00,  1.02s/it]\n",
      "2022-05-05 10:47:14,127 INFO: Epoch:[019/100]\n",
      "2022-05-05 10:47:14,128 INFO: Train Loss:0.011 | Acc:0.9935 | F1:0.9938\n",
      "2022-05-05 10:47:51,990 INFO: val Loss:0.007 | Acc:0.9957 | F1:0.9967\n",
      "100%|██████████| 700/700 [11:46<00:00,  1.01s/it]\n",
      "2022-05-05 10:59:38,412 INFO: Epoch:[020/100]\n",
      "2022-05-05 10:59:38,413 INFO: Train Loss:0.009 | Acc:0.9937 | F1:0.9942\n",
      "2022-05-05 11:00:16,602 INFO: val Loss:0.002 | Acc:0.9982 | F1:0.9984\n",
      "2022-05-05 11:00:17,372 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 700/700 [11:00<00:00,  1.06it/s]\n",
      "2022-05-05 11:11:17,614 INFO: Epoch:[021/100]\n",
      "2022-05-05 11:11:17,614 INFO: Train Loss:0.010 | Acc:0.9926 | F1:0.9932\n",
      "2022-05-05 11:11:54,535 INFO: val Loss:0.009 | Acc:0.9929 | F1:0.9941\n",
      "100%|██████████| 700/700 [11:32<00:00,  1.01it/s]\n",
      "2022-05-05 11:23:26,564 INFO: Epoch:[022/100]\n",
      "2022-05-05 11:23:26,565 INFO: Train Loss:0.008 | Acc:0.9949 | F1:0.9945\n",
      "2022-05-05 11:24:01,540 INFO: val Loss:0.019 | Acc:0.9889 | F1:0.9897\n",
      "100%|██████████| 700/700 [11:05<00:00,  1.05it/s]\n",
      "2022-05-05 11:35:06,870 INFO: Epoch:[023/100]\n",
      "2022-05-05 11:35:06,871 INFO: Train Loss:0.012 | Acc:0.9944 | F1:0.9948\n",
      "2022-05-05 11:35:42,756 INFO: val Loss:0.006 | Acc:0.9961 | F1:0.9961\n",
      "100%|██████████| 700/700 [11:17<00:00,  1.03it/s]\n",
      "2022-05-05 11:47:00,702 INFO: Epoch:[024/100]\n",
      "2022-05-05 11:47:00,703 INFO: Train Loss:0.011 | Acc:0.9944 | F1:0.9951\n",
      "2022-05-05 11:47:36,984 INFO: val Loss:0.001 | Acc:0.9996 | F1:0.9997\n",
      "2022-05-05 11:47:37,725 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 700/700 [11:12<00:00,  1.04it/s]\n",
      "2022-05-05 11:58:50,398 INFO: Epoch:[025/100]\n",
      "2022-05-05 11:58:50,399 INFO: Train Loss:0.007 | Acc:0.9962 | F1:0.9962\n",
      "2022-05-05 11:59:27,782 INFO: val Loss:0.002 | Acc:0.9996 | F1:0.9997\n",
      "100%|██████████| 700/700 [11:22<00:00,  1.03it/s]\n",
      "2022-05-05 12:10:50,636 INFO: Epoch:[026/100]\n",
      "2022-05-05 12:10:50,637 INFO: Train Loss:0.010 | Acc:0.9938 | F1:0.9941\n",
      "2022-05-05 12:11:27,583 INFO: val Loss:0.004 | Acc:0.9993 | F1:0.9993\n",
      "100%|██████████| 700/700 [11:20<00:00,  1.03it/s]\n",
      "2022-05-05 12:22:47,981 INFO: Epoch:[027/100]\n",
      "2022-05-05 12:22:47,982 INFO: Train Loss:0.010 | Acc:0.9941 | F1:0.9935\n",
      "2022-05-05 12:23:24,940 INFO: val Loss:0.026 | Acc:0.9918 | F1:0.9865\n",
      "100%|██████████| 700/700 [11:04<00:00,  1.05it/s]\n",
      "2022-05-05 12:34:29,694 INFO: Epoch:[028/100]\n",
      "2022-05-05 12:34:29,695 INFO: Train Loss:0.009 | Acc:0.9947 | F1:0.9947\n",
      "2022-05-05 12:35:06,779 INFO: val Loss:0.002 | Acc:0.9989 | F1:0.9990\n",
      "100%|██████████| 700/700 [11:25<00:00,  1.02it/s]\n",
      "2022-05-05 12:46:32,286 INFO: Epoch:[029/100]\n",
      "2022-05-05 12:46:32,287 INFO: Train Loss:0.010 | Acc:0.9943 | F1:0.9948\n",
      "2022-05-05 12:47:07,574 INFO: val Loss:0.005 | Acc:0.9968 | F1:0.9973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:47:07,577 INFO: \n",
      "Best Val Epoch:24 | Val Loss:0.0008 | Val Acc:0.9996 | Val F1:0.9997\n",
      "2022-05-05 12:47:07,577 INFO: Total Process time:358.360Minute\n",
      "2022-05-05 12:47:07,600 INFO: {'exp_num': '3', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_tf_efficientnet_b4_ns/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'tf_efficientnet_b4_ns', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11198\n",
      "Dataset size:2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 12:47:07,940 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 700/700 [11:34<00:00,  1.01it/s]\n",
      "2022-05-05 12:58:42,274 INFO: Epoch:[001/100]\n",
      "2022-05-05 12:58:42,274 INFO: Train Loss:4.063 | Acc:0.0869 | F1:0.0588\n",
      "2022-05-05 12:59:19,622 INFO: val Loss:3.220 | Acc:0.2379 | F1:0.1275\n",
      "2022-05-05 12:59:20,206 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 700/700 [11:23<00:00,  1.02it/s]\n",
      "2022-05-05 13:10:43,661 INFO: Epoch:[002/100]\n",
      "2022-05-05 13:10:43,662 INFO: Train Loss:2.198 | Acc:0.3466 | F1:0.2166\n",
      "2022-05-05 13:11:21,447 INFO: val Loss:1.293 | Acc:0.4712 | F1:0.3564\n",
      "2022-05-05 13:11:22,211 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 700/700 [11:48<00:00,  1.01s/it]\n",
      "2022-05-05 13:23:10,938 INFO: Epoch:[003/100]\n",
      "2022-05-05 13:23:10,939 INFO: Train Loss:0.935 | Acc:0.5921 | F1:0.5244\n",
      "2022-05-05 13:23:47,248 INFO: val Loss:0.448 | Acc:0.7660 | F1:0.7092\n",
      "2022-05-05 13:23:47,998 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 700/700 [11:45<00:00,  1.01s/it]\n",
      "2022-05-05 13:35:33,417 INFO: Epoch:[004/100]\n",
      "2022-05-05 13:35:33,418 INFO: Train Loss:0.317 | Acc:0.8413 | F1:0.8257\n",
      "2022-05-05 13:36:11,390 INFO: val Loss:0.103 | Acc:0.9232 | F1:0.9179\n",
      "2022-05-05 13:36:12,121 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 700/700 [12:02<00:00,  1.03s/it]\n",
      "2022-05-05 13:48:14,802 INFO: Epoch:[005/100]\n",
      "2022-05-05 13:48:14,803 INFO: Train Loss:0.100 | Acc:0.9391 | F1:0.9376\n",
      "2022-05-05 13:48:53,407 INFO: val Loss:0.164 | Acc:0.9436 | F1:0.9424\n",
      "100%|██████████| 700/700 [11:50<00:00,  1.01s/it]\n",
      "2022-05-05 14:00:43,660 INFO: Epoch:[006/100]\n",
      "2022-05-05 14:00:43,661 INFO: Train Loss:0.054 | Acc:0.9662 | F1:0.9661\n",
      "2022-05-05 14:01:21,565 INFO: val Loss:0.039 | Acc:0.9832 | F1:0.9840\n",
      "2022-05-05 14:01:22,288 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 700/700 [11:57<00:00,  1.03s/it]\n",
      "2022-05-05 14:13:19,963 INFO: Epoch:[007/100]\n",
      "2022-05-05 14:13:19,964 INFO: Train Loss:0.033 | Acc:0.9759 | F1:0.9751\n",
      "2022-05-05 14:13:57,765 INFO: val Loss:0.015 | Acc:0.9907 | F1:0.9897\n",
      "2022-05-05 14:13:58,541 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 700/700 [11:54<00:00,  1.02s/it]\n",
      "2022-05-05 14:25:52,654 INFO: Epoch:[008/100]\n",
      "2022-05-05 14:25:52,655 INFO: Train Loss:0.027 | Acc:0.9820 | F1:0.9815\n",
      "2022-05-05 14:26:30,470 INFO: val Loss:0.025 | Acc:0.9875 | F1:0.9818\n",
      "100%|██████████| 700/700 [11:58<00:00,  1.03s/it]\n",
      "2022-05-05 14:38:28,962 INFO: Epoch:[009/100]\n",
      "2022-05-05 14:38:28,963 INFO: Train Loss:0.023 | Acc:0.9845 | F1:0.9836\n",
      "2022-05-05 14:39:07,000 INFO: val Loss:0.022 | Acc:0.9918 | F1:0.9932\n",
      "100%|██████████| 700/700 [11:42<00:00,  1.00s/it]\n",
      "2022-05-05 14:50:49,653 INFO: Epoch:[010/100]\n",
      "2022-05-05 14:50:49,654 INFO: Train Loss:0.023 | Acc:0.9863 | F1:0.9862\n",
      "2022-05-05 14:51:28,445 INFO: val Loss:0.007 | Acc:0.9964 | F1:0.9960\n",
      "2022-05-05 14:51:29,196 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 700/700 [11:46<00:00,  1.01s/it]\n",
      "2022-05-05 15:03:15,824 INFO: Epoch:[011/100]\n",
      "2022-05-05 15:03:15,825 INFO: Train Loss:0.016 | Acc:0.9899 | F1:0.9906\n",
      "2022-05-05 15:03:53,906 INFO: val Loss:0.022 | Acc:0.9896 | F1:0.9844\n",
      "100%|██████████| 700/700 [11:43<00:00,  1.00s/it]\n",
      "2022-05-05 15:15:37,135 INFO: Epoch:[012/100]\n",
      "2022-05-05 15:15:37,136 INFO: Train Loss:0.016 | Acc:0.9889 | F1:0.9885\n",
      "2022-05-05 15:16:15,153 INFO: val Loss:0.011 | Acc:0.9936 | F1:0.9932\n",
      "100%|██████████| 700/700 [11:43<00:00,  1.01s/it]\n",
      "2022-05-05 15:27:59,152 INFO: Epoch:[013/100]\n",
      "2022-05-05 15:27:59,153 INFO: Train Loss:0.015 | Acc:0.9908 | F1:0.9906\n",
      "2022-05-05 15:28:36,829 INFO: val Loss:0.009 | Acc:0.9925 | F1:0.9924\n",
      "100%|██████████| 700/700 [12:07<00:00,  1.04s/it]\n",
      "2022-05-05 15:40:44,190 INFO: Epoch:[014/100]\n",
      "2022-05-05 15:40:44,190 INFO: Train Loss:0.013 | Acc:0.9913 | F1:0.9916\n",
      "2022-05-05 15:41:21,459 INFO: val Loss:0.010 | Acc:0.9936 | F1:0.9936\n",
      "100%|██████████| 700/700 [11:50<00:00,  1.02s/it]\n",
      "2022-05-05 15:53:12,158 INFO: Epoch:[015/100]\n",
      "2022-05-05 15:53:12,159 INFO: Train Loss:0.012 | Acc:0.9928 | F1:0.9928\n",
      "2022-05-05 15:53:49,626 INFO: val Loss:0.003 | Acc:0.9989 | F1:0.9988\n",
      "2022-05-05 15:53:50,348 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 700/700 [11:58<00:00,  1.03s/it]\n",
      "2022-05-05 16:05:49,137 INFO: Epoch:[016/100]\n",
      "2022-05-05 16:05:49,138 INFO: Train Loss:0.012 | Acc:0.9926 | F1:0.9932\n",
      "2022-05-05 16:06:27,686 INFO: val Loss:0.002 | Acc:0.9982 | F1:0.9984\n",
      "2022-05-05 16:06:28,448 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 700/700 [11:49<00:00,  1.01s/it]\n",
      "2022-05-05 16:18:17,772 INFO: Epoch:[017/100]\n",
      "2022-05-05 16:18:17,773 INFO: Train Loss:0.010 | Acc:0.9940 | F1:0.9939\n",
      "2022-05-05 16:18:55,667 INFO: val Loss:0.005 | Acc:0.9986 | F1:0.9987\n",
      "100%|██████████| 700/700 [12:04<00:00,  1.04s/it]\n",
      "2022-05-05 16:31:00,603 INFO: Epoch:[018/100]\n",
      "2022-05-05 16:31:00,604 INFO: Train Loss:0.014 | Acc:0.9923 | F1:0.9921\n",
      "2022-05-05 16:31:39,086 INFO: val Loss:0.002 | Acc:0.9993 | F1:0.9995\n",
      "100%|██████████| 700/700 [12:03<00:00,  1.03s/it]\n",
      "2022-05-05 16:43:42,480 INFO: Epoch:[019/100]\n",
      "2022-05-05 16:43:42,481 INFO: Train Loss:0.011 | Acc:0.9935 | F1:0.9939\n",
      "2022-05-05 16:44:20,615 INFO: val Loss:0.005 | Acc:0.9986 | F1:0.9988\n",
      "100%|██████████| 700/700 [11:46<00:00,  1.01s/it]\n",
      "2022-05-05 16:56:07,081 INFO: Epoch:[020/100]\n",
      "2022-05-05 16:56:07,082 INFO: Train Loss:0.011 | Acc:0.9937 | F1:0.9936\n",
      "2022-05-05 16:56:44,330 INFO: val Loss:0.005 | Acc:0.9979 | F1:0.9983\n",
      "100%|██████████| 700/700 [11:48<00:00,  1.01s/it]\n",
      "2022-05-05 17:08:32,821 INFO: Epoch:[021/100]\n",
      "2022-05-05 17:08:32,822 INFO: Train Loss:0.008 | Acc:0.9946 | F1:0.9945\n",
      "2022-05-05 17:09:08,639 INFO: val Loss:0.008 | Acc:0.9964 | F1:0.9967\n",
      "2022-05-05 17:09:08,641 INFO: \n",
      "Best Val Epoch:16 | Val Loss:0.0018 | Val Acc:0.9982 | Val F1:0.9984\n",
      "2022-05-05 17:09:08,642 INFO: Total Process time:262.007Minute\n",
      "2022-05-05 17:09:08,653 INFO: {'exp_num': '4', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_tf_efficientnet_b4_ns/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'tf_efficientnet_b4_ns', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11198\n",
      "Dataset size:2799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 17:09:09,041 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 700/700 [08:56<00:00,  1.30it/s]\n",
      "2022-05-05 17:18:06,336 INFO: Epoch:[001/100]\n",
      "2022-05-05 17:18:06,337 INFO: Train Loss:4.068 | Acc:0.0827 | F1:0.0565\n",
      "2022-05-05 17:18:43,859 INFO: val Loss:3.250 | Acc:0.2222 | F1:0.1188\n",
      "2022-05-05 17:18:44,487 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 700/700 [07:47<00:00,  1.50it/s]\n",
      "2022-05-05 17:26:31,860 INFO: Epoch:[002/100]\n",
      "2022-05-05 17:26:31,861 INFO: Train Loss:2.185 | Acc:0.3562 | F1:0.2267\n",
      "2022-05-05 17:27:08,659 INFO: val Loss:1.287 | Acc:0.4598 | F1:0.3404\n",
      "2022-05-05 17:27:09,357 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 700/700 [07:51<00:00,  1.49it/s]\n",
      "2022-05-05 17:35:00,416 INFO: Epoch:[003/100]\n",
      "2022-05-05 17:35:00,417 INFO: Train Loss:0.912 | Acc:0.6052 | F1:0.5448\n",
      "2022-05-05 17:35:37,042 INFO: val Loss:0.397 | Acc:0.7999 | F1:0.7538\n",
      "2022-05-05 17:35:37,859 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 700/700 [07:43<00:00,  1.51it/s]\n",
      "2022-05-05 17:43:21,390 INFO: Epoch:[004/100]\n",
      "2022-05-05 17:43:21,391 INFO: Train Loss:0.297 | Acc:0.8504 | F1:0.8350\n",
      "2022-05-05 17:43:59,185 INFO: val Loss:0.091 | Acc:0.9421 | F1:0.9333\n",
      "2022-05-05 17:43:59,958 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 700/700 [12:04<00:00,  1.03s/it]\n",
      "2022-05-05 17:56:04,453 INFO: Epoch:[005/100]\n",
      "2022-05-05 17:56:04,454 INFO: Train Loss:0.099 | Acc:0.9406 | F1:0.9393\n",
      "2022-05-05 17:56:41,932 INFO: val Loss:0.023 | Acc:0.9814 | F1:0.9788\n",
      "2022-05-05 17:56:42,686 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 700/700 [11:03<00:00,  1.06it/s]\n",
      "2022-05-05 18:07:46,206 INFO: Epoch:[006/100]\n",
      "2022-05-05 18:07:46,207 INFO: Train Loss:0.051 | Acc:0.9669 | F1:0.9663\n",
      "2022-05-05 18:08:23,751 INFO: val Loss:0.034 | Acc:0.9789 | F1:0.9762\n",
      "100%|██████████| 700/700 [07:52<00:00,  1.48it/s]\n",
      "2022-05-05 18:16:16,666 INFO: Epoch:[007/100]\n",
      "2022-05-05 18:16:16,667 INFO: Train Loss:0.034 | Acc:0.9767 | F1:0.9757\n",
      "2022-05-05 18:16:54,574 INFO: val Loss:0.013 | Acc:0.9914 | F1:0.9890\n",
      "2022-05-05 18:16:55,287 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 700/700 [07:38<00:00,  1.53it/s]\n",
      "2022-05-05 18:24:33,307 INFO: Epoch:[008/100]\n",
      "2022-05-05 18:24:33,308 INFO: Train Loss:0.026 | Acc:0.9821 | F1:0.9818\n",
      "2022-05-05 18:25:10,488 INFO: val Loss:0.014 | Acc:0.9929 | F1:0.9919\n",
      "100%|██████████| 700/700 [07:53<00:00,  1.48it/s]\n",
      "2022-05-05 18:33:04,412 INFO: Epoch:[009/100]\n",
      "2022-05-05 18:33:04,413 INFO: Train Loss:0.024 | Acc:0.9834 | F1:0.9832\n",
      "2022-05-05 18:33:41,823 INFO: val Loss:0.012 | Acc:0.9943 | F1:0.9942\n",
      "2022-05-05 18:33:42,528 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 700/700 [07:42<00:00,  1.51it/s]\n",
      "2022-05-05 18:41:24,828 INFO: Epoch:[010/100]\n",
      "2022-05-05 18:41:24,829 INFO: Train Loss:0.020 | Acc:0.9869 | F1:0.9861\n",
      "2022-05-05 18:42:01,425 INFO: val Loss:0.008 | Acc:0.9971 | F1:0.9977\n",
      "2022-05-05 18:42:02,128 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 700/700 [07:47<00:00,  1.50it/s]\n",
      "2022-05-05 18:49:49,686 INFO: Epoch:[011/100]\n",
      "2022-05-05 18:49:49,687 INFO: Train Loss:0.018 | Acc:0.9895 | F1:0.9892\n",
      "2022-05-05 18:50:27,107 INFO: val Loss:0.007 | Acc:0.9968 | F1:0.9962\n",
      "2022-05-05 18:50:27,857 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 700/700 [07:52<00:00,  1.48it/s]\n",
      "2022-05-05 18:58:20,707 INFO: Epoch:[012/100]\n",
      "2022-05-05 18:58:20,708 INFO: Train Loss:0.015 | Acc:0.9884 | F1:0.9885\n",
      "2022-05-05 18:58:57,404 INFO: val Loss:0.016 | Acc:0.9918 | F1:0.9910\n",
      "100%|██████████| 700/700 [07:48<00:00,  1.49it/s]\n",
      "2022-05-05 19:06:45,809 INFO: Epoch:[013/100]\n",
      "2022-05-05 19:06:45,810 INFO: Train Loss:0.017 | Acc:0.9883 | F1:0.9887\n",
      "2022-05-05 19:07:22,451 INFO: val Loss:0.003 | Acc:0.9982 | F1:0.9983\n",
      "2022-05-05 19:07:23,210 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 700/700 [07:59<00:00,  1.46it/s]\n",
      "2022-05-05 19:15:22,742 INFO: Epoch:[014/100]\n",
      "2022-05-05 19:15:22,743 INFO: Train Loss:0.013 | Acc:0.9909 | F1:0.9914\n",
      "2022-05-05 19:15:58,899 INFO: val Loss:0.005 | Acc:0.9957 | F1:0.9961\n",
      "100%|██████████| 700/700 [07:40<00:00,  1.52it/s]\n",
      "2022-05-05 19:23:39,306 INFO: Epoch:[015/100]\n",
      "2022-05-05 19:23:39,307 INFO: Train Loss:0.014 | Acc:0.9920 | F1:0.9918\n",
      "2022-05-05 19:24:16,620 INFO: val Loss:0.003 | Acc:0.9986 | F1:0.9987\n",
      "2022-05-05 19:24:17,484 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 700/700 [07:50<00:00,  1.49it/s]\n",
      "2022-05-05 19:32:07,733 INFO: Epoch:[016/100]\n",
      "2022-05-05 19:32:07,734 INFO: Train Loss:0.010 | Acc:0.9923 | F1:0.9928\n",
      "2022-05-05 19:32:44,149 INFO: val Loss:0.004 | Acc:0.9975 | F1:0.9968\n",
      "100%|██████████| 700/700 [07:43<00:00,  1.51it/s]\n",
      "2022-05-05 19:40:27,388 INFO: Epoch:[017/100]\n",
      "2022-05-05 19:40:27,389 INFO: Train Loss:0.009 | Acc:0.9939 | F1:0.9940\n",
      "2022-05-05 19:41:04,919 INFO: val Loss:0.004 | Acc:0.9982 | F1:0.9983\n",
      "100%|██████████| 700/700 [07:52<00:00,  1.48it/s]\n",
      "2022-05-05 19:48:57,415 INFO: Epoch:[018/100]\n",
      "2022-05-05 19:48:57,416 INFO: Train Loss:0.012 | Acc:0.9916 | F1:0.9921\n",
      "2022-05-05 19:49:33,503 INFO: val Loss:0.001 | Acc:0.9989 | F1:0.9985\n",
      "2022-05-05 19:49:34,244 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 700/700 [07:50<00:00,  1.49it/s]\n",
      "2022-05-05 19:57:25,242 INFO: Epoch:[019/100]\n",
      "2022-05-05 19:57:25,243 INFO: Train Loss:0.010 | Acc:0.9936 | F1:0.9940\n",
      "2022-05-05 19:58:01,579 INFO: val Loss:0.010 | Acc:0.9957 | F1:0.9957\n",
      "100%|██████████| 700/700 [07:44<00:00,  1.51it/s]\n",
      "2022-05-05 20:05:46,505 INFO: Epoch:[020/100]\n",
      "2022-05-05 20:05:46,506 INFO: Train Loss:0.009 | Acc:0.9948 | F1:0.9949\n",
      "2022-05-05 20:06:23,044 INFO: val Loss:0.001 | Acc:0.9986 | F1:0.9987\n",
      "2022-05-05 20:06:23,828 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 700/700 [07:51<00:00,  1.48it/s]\n",
      "2022-05-05 20:14:15,713 INFO: Epoch:[021/100]\n",
      "2022-05-05 20:14:15,713 INFO: Train Loss:0.014 | Acc:0.9927 | F1:0.9933\n",
      "2022-05-05 20:14:52,214 INFO: val Loss:0.004 | Acc:0.9961 | F1:0.9965\n",
      "100%|██████████| 700/700 [07:47<00:00,  1.50it/s]\n",
      "2022-05-05 20:22:39,416 INFO: Epoch:[022/100]\n",
      "2022-05-05 20:22:39,417 INFO: Train Loss:0.009 | Acc:0.9935 | F1:0.9933\n",
      "2022-05-05 20:23:15,877 INFO: val Loss:0.010 | Acc:0.9968 | F1:0.9972\n",
      "100%|██████████| 700/700 [07:48<00:00,  1.49it/s]\n",
      "2022-05-05 20:31:04,548 INFO: Epoch:[023/100]\n",
      "2022-05-05 20:31:04,549 INFO: Train Loss:0.009 | Acc:0.9952 | F1:0.9955\n",
      "2022-05-05 20:31:41,310 INFO: val Loss:0.007 | Acc:0.9968 | F1:0.9969\n",
      "100%|██████████| 700/700 [07:44<00:00,  1.51it/s]\n",
      "2022-05-05 20:39:25,889 INFO: Epoch:[024/100]\n",
      "2022-05-05 20:39:25,890 INFO: Train Loss:0.007 | Acc:0.9953 | F1:0.9954\n",
      "2022-05-05 20:40:03,103 INFO: val Loss:0.001 | Acc:0.9986 | F1:0.9989\n",
      "2022-05-05 20:40:03,811 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 700/700 [07:43<00:00,  1.51it/s]\n",
      "2022-05-05 20:47:47,263 INFO: Epoch:[025/100]\n",
      "2022-05-05 20:47:47,265 INFO: Train Loss:0.012 | Acc:0.9931 | F1:0.9928\n",
      "2022-05-05 20:48:23,433 INFO: val Loss:0.005 | Acc:0.9936 | F1:0.9873\n",
      "100%|██████████| 700/700 [07:48<00:00,  1.49it/s]\n",
      "2022-05-05 20:56:12,003 INFO: Epoch:[026/100]\n",
      "2022-05-05 20:56:12,004 INFO: Train Loss:0.007 | Acc:0.9951 | F1:0.9952\n",
      "2022-05-05 20:56:48,232 INFO: val Loss:0.004 | Acc:0.9971 | F1:0.9964\n",
      "100%|██████████| 700/700 [07:49<00:00,  1.49it/s]\n",
      "2022-05-05 21:04:37,390 INFO: Epoch:[027/100]\n",
      "2022-05-05 21:04:37,391 INFO: Train Loss:0.008 | Acc:0.9946 | F1:0.9945\n",
      "2022-05-05 21:05:15,245 INFO: val Loss:0.009 | Acc:0.9929 | F1:0.9917\n",
      "100%|██████████| 700/700 [07:47<00:00,  1.50it/s]\n",
      "2022-05-05 21:13:03,094 INFO: Epoch:[028/100]\n",
      "2022-05-05 21:13:03,096 INFO: Train Loss:0.005 | Acc:0.9962 | F1:0.9963\n",
      "2022-05-05 21:13:40,672 INFO: val Loss:0.001 | Acc:0.9989 | F1:0.9989\n",
      "100%|██████████| 700/700 [07:52<00:00,  1.48it/s]\n",
      "2022-05-05 21:21:33,312 INFO: Epoch:[029/100]\n",
      "2022-05-05 21:21:33,313 INFO: Train Loss:0.007 | Acc:0.9956 | F1:0.9957\n",
      "2022-05-05 21:22:10,947 INFO: val Loss:0.002 | Acc:0.9989 | F1:0.9991\n",
      "2022-05-05 21:22:10,949 INFO: \n",
      "Best Val Epoch:24 | Val Loss:0.0007 | Val Acc:0.9986 | Val F1:0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 21:22:10,950 INFO: Total Process time:253.026Minute\n"
     ]
    }
   ],
   "source": [
    "args.step = 0\n",
    "models_path = []\n",
    "for s_fold in range(5): # 5fold\n",
    "    args.fold = s_fold\n",
    "    args.exp_num = str(s_fold)\n",
    "    save_path = main(args)\n",
    "    models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size:2154\n"
     ]
    }
   ],
   "source": [
    "img_size = 512\n",
    "\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_path = ['./label_results/000', './label_results/001', './label_results/002', './label_results/003', './label_results/004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 23:06:18,107 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 34/34 [03:30<00:00,  6.20s/it]\n",
      "2022-05-05 23:09:49,993 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 34/34 [03:39<00:00,  6.47s/it]\n",
      "2022-05-05 23:13:30,893 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 34/34 [03:50<00:00,  6.79s/it]\n",
      "2022-05-05 23:17:25,312 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 34/34 [03:51<00:00,  6.80s/it]\n",
      "2022-05-05 23:21:17,863 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth)\n",
      "100%|██████████| 34/34 [03:52<00:00,  6.85s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble = ensemble_5fold(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62,\n",
       " 28,\n",
       " 72,\n",
       " 64,\n",
       " 63,\n",
       " 50,\n",
       " 17,\n",
       " 32,\n",
       " 40,\n",
       " 39,\n",
       " 0,\n",
       " 76,\n",
       " 85,\n",
       " 60,\n",
       " 62,\n",
       " 2,\n",
       " 25,\n",
       " 20,\n",
       " 21,\n",
       " 47,\n",
       " 66,\n",
       " 84,\n",
       " 0,\n",
       " 58,\n",
       " 51,\n",
       " 55,\n",
       " 51,\n",
       " 60,\n",
       " 40,\n",
       " 39,\n",
       " 55,\n",
       " 9,\n",
       " 59,\n",
       " 83,\n",
       " 84,\n",
       " 82,\n",
       " 40,\n",
       " 9,\n",
       " 37,\n",
       " 44,\n",
       " 45,\n",
       " 10,\n",
       " 60,\n",
       " 14,\n",
       " 44,\n",
       " 63,\n",
       " 68,\n",
       " 15,\n",
       " 84,\n",
       " 55,\n",
       " 9,\n",
       " 72,\n",
       " 31,\n",
       " 35,\n",
       " 84,\n",
       " 46,\n",
       " 20,\n",
       " 21,\n",
       " 38,\n",
       " 15,\n",
       " 73,\n",
       " 51,\n",
       " 15,\n",
       " 87,\n",
       " 13,\n",
       " 36,\n",
       " 72,\n",
       " 13,\n",
       " 53,\n",
       " 43,\n",
       " 25,\n",
       " 76,\n",
       " 32,\n",
       " 61,\n",
       " 41,\n",
       " 84,\n",
       " 9,\n",
       " 40,\n",
       " 9,\n",
       " 9,\n",
       " 50,\n",
       " 76,\n",
       " 57,\n",
       " 75,\n",
       " 72,\n",
       " 1,\n",
       " 82,\n",
       " 22,\n",
       " 40,\n",
       " 83,\n",
       " 36,\n",
       " 84,\n",
       " 9,\n",
       " 42,\n",
       " 55,\n",
       " 65,\n",
       " 69,\n",
       " 72,\n",
       " 45,\n",
       " 45,\n",
       " 9,\n",
       " 21,\n",
       " 52,\n",
       " 17,\n",
       " 76,\n",
       " 2,\n",
       " 85,\n",
       " 49,\n",
       " 72,\n",
       " 34,\n",
       " 36,\n",
       " 58,\n",
       " 69,\n",
       " 72,\n",
       " 84,\n",
       " 15,\n",
       " 28,\n",
       " 52,\n",
       " 13,\n",
       " 41,\n",
       " 55,\n",
       " 19,\n",
       " 45,\n",
       " 72,\n",
       " 77,\n",
       " 70,\n",
       " 61,\n",
       " 70,\n",
       " 54,\n",
       " 55,\n",
       " 28,\n",
       " 46,\n",
       " 50,\n",
       " 42,\n",
       " 15,\n",
       " 25,\n",
       " 57,\n",
       " 2,\n",
       " 52,\n",
       " 78,\n",
       " 72,\n",
       " 33,\n",
       " 7,\n",
       " 9,\n",
       " 84,\n",
       " 35,\n",
       " 23,\n",
       " 84,\n",
       " 85,\n",
       " 76,\n",
       " 84,\n",
       " 79,\n",
       " 73,\n",
       " 52,\n",
       " 83,\n",
       " 63,\n",
       " 68,\n",
       " 63,\n",
       " 33,\n",
       " 45,\n",
       " 57,\n",
       " 60,\n",
       " 57,\n",
       " 11,\n",
       " 21,\n",
       " 38,\n",
       " 49,\n",
       " 74,\n",
       " 83,\n",
       " 10,\n",
       " 21,\n",
       " 24,\n",
       " 56,\n",
       " 39,\n",
       " 9,\n",
       " 50,\n",
       " 55,\n",
       " 86,\n",
       " 15,\n",
       " 84,\n",
       " 40,\n",
       " 27,\n",
       " 3,\n",
       " 11,\n",
       " 51,\n",
       " 20,\n",
       " 62,\n",
       " 45,\n",
       " 79,\n",
       " 73,\n",
       " 54,\n",
       " 18,\n",
       " 84,\n",
       " 32,\n",
       " 52,\n",
       " 57,\n",
       " 81,\n",
       " 85,\n",
       " 84,\n",
       " 33,\n",
       " 72,\n",
       " 12,\n",
       " 58,\n",
       " 1,\n",
       " 52,\n",
       " 40,\n",
       " 52,\n",
       " 52,\n",
       " 33,\n",
       " 25,\n",
       " 41,\n",
       " 15,\n",
       " 55,\n",
       " 76,\n",
       " 48,\n",
       " 20,\n",
       " 18,\n",
       " 49,\n",
       " 72,\n",
       " 1,\n",
       " 65,\n",
       " 28,\n",
       " 63,\n",
       " 1,\n",
       " 15,\n",
       " 43,\n",
       " 51,\n",
       " 36,\n",
       " 9,\n",
       " 72,\n",
       " 52,\n",
       " 55,\n",
       " 81,\n",
       " 52,\n",
       " 80,\n",
       " 17,\n",
       " 72,\n",
       " 45,\n",
       " 31,\n",
       " 14,\n",
       " 45,\n",
       " 34,\n",
       " 85,\n",
       " 59,\n",
       " 74,\n",
       " 41,\n",
       " 63,\n",
       " 64,\n",
       " 79,\n",
       " 5,\n",
       " 72,\n",
       " 64,\n",
       " 4,\n",
       " 15,\n",
       " 44,\n",
       " 45,\n",
       " 42,\n",
       " 13,\n",
       " 86,\n",
       " 33,\n",
       " 31,\n",
       " 55,\n",
       " 33,\n",
       " 32,\n",
       " 41,\n",
       " 40,\n",
       " 76,\n",
       " 55,\n",
       " 44,\n",
       " 9,\n",
       " 21,\n",
       " 55,\n",
       " 77,\n",
       " 73,\n",
       " 9,\n",
       " 15,\n",
       " 47,\n",
       " 15,\n",
       " 72,\n",
       " 87,\n",
       " 47,\n",
       " 36,\n",
       " 72,\n",
       " 53,\n",
       " 52,\n",
       " 55,\n",
       " 30,\n",
       " 63,\n",
       " 63,\n",
       " 0,\n",
       " 22,\n",
       " 1,\n",
       " 38,\n",
       " 37,\n",
       " 9,\n",
       " 87,\n",
       " 63,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 58,\n",
       " 41,\n",
       " 80,\n",
       " 72,\n",
       " 57,\n",
       " 51,\n",
       " 68,\n",
       " 28,\n",
       " 36,\n",
       " 56,\n",
       " 55,\n",
       " 72,\n",
       " 3,\n",
       " 46,\n",
       " 76,\n",
       " 72,\n",
       " 59,\n",
       " 76,\n",
       " 76,\n",
       " 63,\n",
       " 9,\n",
       " 58,\n",
       " 46,\n",
       " 7,\n",
       " 40,\n",
       " 15,\n",
       " 55,\n",
       " 56,\n",
       " 50,\n",
       " 72,\n",
       " 58,\n",
       " 15,\n",
       " 55,\n",
       " 21,\n",
       " 19,\n",
       " 3,\n",
       " 63,\n",
       " 43,\n",
       " 58,\n",
       " 4,\n",
       " 76,\n",
       " 55,\n",
       " 42,\n",
       " 63,\n",
       " 55,\n",
       " 39,\n",
       " 44,\n",
       " 42,\n",
       " 33,\n",
       " 53,\n",
       " 24,\n",
       " 5,\n",
       " 9,\n",
       " 17,\n",
       " 21,\n",
       " 16,\n",
       " 9,\n",
       " 52,\n",
       " 3,\n",
       " 67,\n",
       " 70,\n",
       " 13,\n",
       " 38,\n",
       " 19,\n",
       " 72,\n",
       " 85,\n",
       " 74,\n",
       " 7,\n",
       " 5,\n",
       " 37,\n",
       " 33,\n",
       " 55,\n",
       " 9,\n",
       " 3,\n",
       " 33,\n",
       " 72,\n",
       " 77,\n",
       " 56,\n",
       " 14,\n",
       " 28,\n",
       " 46,\n",
       " 55,\n",
       " 74,\n",
       " 62,\n",
       " 15,\n",
       " 28,\n",
       " 42,\n",
       " 15,\n",
       " 32,\n",
       " 55,\n",
       " 40,\n",
       " 43,\n",
       " 1,\n",
       " 37,\n",
       " 16,\n",
       " 73,\n",
       " 5,\n",
       " 2,\n",
       " 35,\n",
       " 63,\n",
       " 78,\n",
       " 64,\n",
       " 72,\n",
       " 81,\n",
       " 2,\n",
       " 63,\n",
       " 52,\n",
       " 17,\n",
       " 40,\n",
       " 51,\n",
       " 9,\n",
       " 16,\n",
       " 52,\n",
       " 9,\n",
       " 1,\n",
       " 72,\n",
       " 40,\n",
       " 72,\n",
       " 32,\n",
       " 69,\n",
       " 46,\n",
       " 84,\n",
       " 42,\n",
       " 21,\n",
       " 19,\n",
       " 52,\n",
       " 82,\n",
       " 84,\n",
       " 39,\n",
       " 71,\n",
       " 1,\n",
       " 84,\n",
       " 9,\n",
       " 1,\n",
       " 63,\n",
       " 31,\n",
       " 79,\n",
       " 51,\n",
       " 21,\n",
       " 29,\n",
       " 47,\n",
       " 65,\n",
       " 73,\n",
       " 56,\n",
       " 69,\n",
       " 70,\n",
       " 23,\n",
       " 80,\n",
       " 62,\n",
       " 38,\n",
       " 77,\n",
       " 84,\n",
       " 1,\n",
       " 21,\n",
       " 63,\n",
       " 33,\n",
       " 59,\n",
       " 33,\n",
       " 22,\n",
       " 44,\n",
       " 9,\n",
       " 21,\n",
       " 60,\n",
       " 41,\n",
       " 24,\n",
       " 14,\n",
       " 13,\n",
       " 44,\n",
       " 59,\n",
       " 45,\n",
       " 53,\n",
       " 42,\n",
       " 63,\n",
       " 3,\n",
       " 56,\n",
       " 9,\n",
       " 76,\n",
       " 73,\n",
       " 60,\n",
       " 60,\n",
       " 50,\n",
       " 59,\n",
       " 22,\n",
       " 9,\n",
       " 55,\n",
       " 55,\n",
       " 9,\n",
       " 28,\n",
       " 34,\n",
       " 51,\n",
       " 20,\n",
       " 12,\n",
       " 7,\n",
       " 55,\n",
       " 33,\n",
       " 18,\n",
       " 70,\n",
       " 67,\n",
       " 7,\n",
       " 86,\n",
       " 56,\n",
       " 65,\n",
       " 46,\n",
       " 72,\n",
       " 4,\n",
       " 63,\n",
       " 38,\n",
       " 78,\n",
       " 68,\n",
       " 41,\n",
       " 16,\n",
       " 56,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 84,\n",
       " 9,\n",
       " 46,\n",
       " 40,\n",
       " 9,\n",
       " 64,\n",
       " 53,\n",
       " 76,\n",
       " 45,\n",
       " 52,\n",
       " 81,\n",
       " 9,\n",
       " 76,\n",
       " 13,\n",
       " 20,\n",
       " 15,\n",
       " 34,\n",
       " 33,\n",
       " 34,\n",
       " 28,\n",
       " 55,\n",
       " 86,\n",
       " 79,\n",
       " 49,\n",
       " 67,\n",
       " 9,\n",
       " 76,\n",
       " 29,\n",
       " 55,\n",
       " 28,\n",
       " 87,\n",
       " 72,\n",
       " 63,\n",
       " 33,\n",
       " 54,\n",
       " 9,\n",
       " 13,\n",
       " 47,\n",
       " 52,\n",
       " 81,\n",
       " 62,\n",
       " 55,\n",
       " 15,\n",
       " 81,\n",
       " 56,\n",
       " 39,\n",
       " 15,\n",
       " 40,\n",
       " 23,\n",
       " 9,\n",
       " 33,\n",
       " 28,\n",
       " 28,\n",
       " 33,\n",
       " 59,\n",
       " 33,\n",
       " 79,\n",
       " 33,\n",
       " 52,\n",
       " 11,\n",
       " 77,\n",
       " 86,\n",
       " 21,\n",
       " 27,\n",
       " 82,\n",
       " 49,\n",
       " 42,\n",
       " 63,\n",
       " 20,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 21,\n",
       " 40,\n",
       " 72,\n",
       " 55,\n",
       " 32,\n",
       " 9,\n",
       " 80,\n",
       " 84,\n",
       " 42,\n",
       " 23,\n",
       " 63,\n",
       " 80,\n",
       " 33,\n",
       " 40,\n",
       " 33,\n",
       " 34,\n",
       " 7,\n",
       " 54,\n",
       " 9,\n",
       " 52,\n",
       " 9,\n",
       " 70,\n",
       " 51,\n",
       " 42,\n",
       " 27,\n",
       " 9,\n",
       " 21,\n",
       " 33,\n",
       " 16,\n",
       " 52,\n",
       " 86,\n",
       " 24,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 87,\n",
       " 62,\n",
       " 0,\n",
       " 22,\n",
       " 42,\n",
       " 68,\n",
       " 33,\n",
       " 1,\n",
       " 40,\n",
       " 67,\n",
       " 55,\n",
       " 58,\n",
       " 80,\n",
       " 40,\n",
       " 11,\n",
       " 15,\n",
       " 23,\n",
       " 41,\n",
       " 52,\n",
       " 38,\n",
       " 72,\n",
       " 0,\n",
       " 21,\n",
       " 83,\n",
       " 9,\n",
       " 37,\n",
       " 15,\n",
       " 86,\n",
       " 77,\n",
       " 35,\n",
       " 21,\n",
       " 56,\n",
       " 81,\n",
       " 76,\n",
       " 58,\n",
       " 13,\n",
       " 18,\n",
       " 40,\n",
       " 72,\n",
       " 8,\n",
       " 21,\n",
       " 70,\n",
       " 15,\n",
       " 28,\n",
       " 77,\n",
       " 33,\n",
       " 66,\n",
       " 9,\n",
       " 33,\n",
       " 50,\n",
       " 3,\n",
       " 54,\n",
       " 51,\n",
       " 0,\n",
       " 76,\n",
       " 50,\n",
       " 9,\n",
       " 63,\n",
       " 40,\n",
       " 38,\n",
       " 65,\n",
       " 54,\n",
       " 8,\n",
       " 63,\n",
       " 63,\n",
       " 33,\n",
       " 28,\n",
       " 40,\n",
       " 45,\n",
       " 10,\n",
       " 56,\n",
       " 47,\n",
       " 28,\n",
       " 5,\n",
       " 33,\n",
       " 85,\n",
       " 63,\n",
       " 2,\n",
       " 10,\n",
       " 41,\n",
       " 32,\n",
       " 59,\n",
       " 1,\n",
       " 19,\n",
       " 55,\n",
       " 71,\n",
       " 8,\n",
       " 76,\n",
       " 50,\n",
       " 52,\n",
       " 58,\n",
       " 30,\n",
       " 24,\n",
       " 79,\n",
       " 19,\n",
       " 41,\n",
       " 33,\n",
       " 84,\n",
       " 15,\n",
       " 72,\n",
       " 56,\n",
       " 72,\n",
       " 82,\n",
       " 72,\n",
       " 50,\n",
       " 33,\n",
       " 52,\n",
       " 4,\n",
       " 33,\n",
       " 3,\n",
       " 54,\n",
       " 51,\n",
       " 33,\n",
       " 42,\n",
       " 21,\n",
       " 52,\n",
       " 14,\n",
       " 79,\n",
       " 33,\n",
       " 24,\n",
       " 47,\n",
       " 51,\n",
       " 63,\n",
       " 21,\n",
       " 3,\n",
       " 50,\n",
       " 48,\n",
       " 13,\n",
       " 45,\n",
       " 9,\n",
       " 44,\n",
       " 72,\n",
       " 17,\n",
       " 65,\n",
       " 84,\n",
       " 3,\n",
       " 52,\n",
       " 15,\n",
       " 55,\n",
       " 34,\n",
       " 10,\n",
       " 54,\n",
       " 3,\n",
       " 15,\n",
       " 25,\n",
       " 82,\n",
       " 45,\n",
       " 47,\n",
       " 74,\n",
       " 23,\n",
       " 75,\n",
       " 83,\n",
       " 34,\n",
       " 59,\n",
       " 48,\n",
       " 55,\n",
       " 0,\n",
       " 13,\n",
       " 72,\n",
       " 36,\n",
       " 55,\n",
       " 75,\n",
       " 9,\n",
       " 40,\n",
       " 68,\n",
       " 68,\n",
       " 57,\n",
       " 8,\n",
       " 85,\n",
       " 21,\n",
       " 67,\n",
       " 68,\n",
       " 84,\n",
       " 31,\n",
       " 72,\n",
       " 85,\n",
       " 55,\n",
       " 52,\n",
       " 87,\n",
       " 31,\n",
       " 66,\n",
       " 43,\n",
       " 64,\n",
       " 76,\n",
       " 52,\n",
       " 0,\n",
       " 39,\n",
       " 86,\n",
       " 9,\n",
       " 14,\n",
       " 17,\n",
       " 25,\n",
       " 68,\n",
       " 76,\n",
       " 55,\n",
       " 2,\n",
       " 61,\n",
       " 55,\n",
       " 54,\n",
       " 35,\n",
       " 9,\n",
       " 45,\n",
       " 55,\n",
       " 9,\n",
       " 15,\n",
       " 9,\n",
       " 31,\n",
       " 72,\n",
       " 19,\n",
       " 25,\n",
       " 9,\n",
       " 14,\n",
       " 84,\n",
       " 63,\n",
       " 16,\n",
       " 57,\n",
       " 21,\n",
       " 50,\n",
       " 34,\n",
       " 0,\n",
       " 55,\n",
       " 56,\n",
       " 61,\n",
       " 69,\n",
       " 36,\n",
       " 2,\n",
       " 65,\n",
       " 15,\n",
       " 65,\n",
       " 78,\n",
       " 55,\n",
       " 14,\n",
       " 72,\n",
       " 16,\n",
       " 82,\n",
       " 31,\n",
       " 72,\n",
       " 85,\n",
       " 5,\n",
       " 33,\n",
       " 45,\n",
       " 55,\n",
       " 20,\n",
       " 72,\n",
       " 67,\n",
       " 9,\n",
       " 84,\n",
       " 9,\n",
       " 21,\n",
       " 39,\n",
       " 52,\n",
       " 51,\n",
       " 52,\n",
       " 21,\n",
       " 60,\n",
       " 77,\n",
       " 40,\n",
       " 40,\n",
       " 33,\n",
       " 25,\n",
       " 15,\n",
       " 16,\n",
       " 72,\n",
       " 63,\n",
       " 57,\n",
       " 47,\n",
       " 40,\n",
       " 44,\n",
       " 72,\n",
       " 84,\n",
       " 40,\n",
       " 12,\n",
       " 37,\n",
       " 55,\n",
       " 54,\n",
       " 55,\n",
       " 12,\n",
       " 21,\n",
       " 71,\n",
       " 40,\n",
       " 55,\n",
       " 52,\n",
       " 14,\n",
       " 85,\n",
       " 38,\n",
       " 13,\n",
       " 58,\n",
       " 45,\n",
       " 62,\n",
       " 52,\n",
       " 81,\n",
       " 18,\n",
       " 39,\n",
       " 60,\n",
       " 44,\n",
       " 60,\n",
       " 65,\n",
       " 73,\n",
       " 33,\n",
       " 55,\n",
       " 66,\n",
       " 84,\n",
       " 44,\n",
       " 39,\n",
       " 4,\n",
       " 68,\n",
       " 9,\n",
       " 68,\n",
       " 63,\n",
       " 5,\n",
       " 18,\n",
       " 42,\n",
       " 16,\n",
       " 52,\n",
       " 65,\n",
       " 37,\n",
       " 19,\n",
       " 40,\n",
       " 24,\n",
       " 38,\n",
       " 58,\n",
       " 56,\n",
       " 2,\n",
       " 40,\n",
       " 39,\n",
       " 17,\n",
       " 87,\n",
       " 34,\n",
       " 17,\n",
       " 46,\n",
       " 63,\n",
       " 30,\n",
       " 56,\n",
       " 24,\n",
       " 15,\n",
       " 73,\n",
       " 15,\n",
       " 77,\n",
       " 84,\n",
       " 58,\n",
       " 52,\n",
       " 34,\n",
       " 33,\n",
       " 12,\n",
       " 58,\n",
       " 35,\n",
       " 9,\n",
       " 33,\n",
       " 21,\n",
       " 19,\n",
       " 0,\n",
       " 9,\n",
       " 33,\n",
       " 61,\n",
       " 53,\n",
       " 40,\n",
       " 57,\n",
       " 55,\n",
       " 40,\n",
       " 21,\n",
       " 59,\n",
       " 72,\n",
       " 54,\n",
       " 18,\n",
       " 28,\n",
       " 45,\n",
       " 55,\n",
       " 55,\n",
       " 47,\n",
       " 58,\n",
       " 15,\n",
       " 11,\n",
       " 9,\n",
       " 3,\n",
       " 82,\n",
       " 3,\n",
       " 38,\n",
       " 55,\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pred = ensemble.argmax(axis=1).tolist()\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./open/train_df_add_data.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tile-glue_strip',\n",
       " 'grid-good',\n",
       " 'transistor-good',\n",
       " 'tile-gray_stroke',\n",
       " 'tile-good',\n",
       " 'pill-crack',\n",
       " 'capsule-scratch',\n",
       " 'hazelnut-cut',\n",
       " 'leather-good',\n",
       " 'leather-glue',\n",
       " 'bottle-broken_large',\n",
       " 'wood-good',\n",
       " 'zipper-rough',\n",
       " 'screw-thread_top',\n",
       " 'tile-glue_strip',\n",
       " 'bottle-contamination',\n",
       " 'grid-bent',\n",
       " 'carpet-cut',\n",
       " 'carpet-good',\n",
       " 'pill-color',\n",
       " 'tile-rough',\n",
       " 'zipper-good',\n",
       " 'bottle-broken_large',\n",
       " 'screw-scratch_neck',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-thread_top',\n",
       " 'leather-good',\n",
       " 'leather-glue',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'screw-thread_side',\n",
       " 'zipper-fabric_interior',\n",
       " 'zipper-good',\n",
       " 'zipper-fabric_border',\n",
       " 'leather-good',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-good',\n",
       " 'cable-missing_cable',\n",
       " 'screw-thread_top',\n",
       " 'capsule-faulty_imprint',\n",
       " 'metal_nut-flip',\n",
       " 'tile-good',\n",
       " 'toothbrush-good',\n",
       " 'capsule-good',\n",
       " 'zipper-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'transistor-good',\n",
       " 'hazelnut-crack',\n",
       " 'hazelnut-print',\n",
       " 'zipper-good',\n",
       " 'metal_nut-scratch',\n",
       " 'carpet-cut',\n",
       " 'carpet-good',\n",
       " 'leather-fold',\n",
       " 'capsule-good',\n",
       " 'transistor-misplaced',\n",
       " 'pill-faulty_imprint',\n",
       " 'capsule-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'capsule-crack',\n",
       " 'leather-color',\n",
       " 'transistor-good',\n",
       " 'capsule-crack',\n",
       " 'pill-pill_type',\n",
       " 'metal_nut-color',\n",
       " 'grid-bent',\n",
       " 'wood-good',\n",
       " 'hazelnut-cut',\n",
       " 'tile-crack',\n",
       " 'leather-poke',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'leather-good',\n",
       " 'cable-good',\n",
       " 'cable-good',\n",
       " 'pill-crack',\n",
       " 'wood-good',\n",
       " 'screw-scratch_head',\n",
       " 'wood-combined',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-fabric_border',\n",
       " 'carpet-hole',\n",
       " 'leather-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'leather-color',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-bent',\n",
       " 'screw-good',\n",
       " 'tile-oil',\n",
       " 'transistor-bent_lead',\n",
       " 'transistor-good',\n",
       " 'metal_nut-good',\n",
       " 'metal_nut-good',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'pill-good',\n",
       " 'capsule-scratch',\n",
       " 'wood-good',\n",
       " 'bottle-contamination',\n",
       " 'zipper-rough',\n",
       " 'pill-contamination',\n",
       " 'transistor-good',\n",
       " 'hazelnut-hole',\n",
       " 'leather-color',\n",
       " 'screw-scratch_neck',\n",
       " 'transistor-bent_lead',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'pill-good',\n",
       " 'capsule-crack',\n",
       " 'leather-poke',\n",
       " 'screw-good',\n",
       " 'carpet-color',\n",
       " 'metal_nut-good',\n",
       " 'transistor-good',\n",
       " 'wood-hole',\n",
       " 'transistor-cut_lead',\n",
       " 'tile-crack',\n",
       " 'transistor-cut_lead',\n",
       " 'pill-scratch',\n",
       " 'screw-good',\n",
       " 'grid-good',\n",
       " 'metal_nut-scratch',\n",
       " 'pill-crack',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'grid-bent',\n",
       " 'screw-scratch_head',\n",
       " 'bottle-contamination',\n",
       " 'pill-good',\n",
       " 'wood-liquid',\n",
       " 'transistor-good',\n",
       " 'hazelnut-good',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'cable-good',\n",
       " 'zipper-good',\n",
       " 'hazelnut-print',\n",
       " 'carpet-metal_contamination',\n",
       " 'zipper-good',\n",
       " 'zipper-rough',\n",
       " 'wood-good',\n",
       " 'zipper-good',\n",
       " 'wood-scratch',\n",
       " 'transistor-misplaced',\n",
       " 'pill-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'tile-good',\n",
       " 'toothbrush-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-scratch_head',\n",
       " 'screw-thread_top',\n",
       " 'screw-scratch_head',\n",
       " 'cable-missing_wire',\n",
       " 'carpet-good',\n",
       " 'leather-fold',\n",
       " 'pill-contamination',\n",
       " 'wood-color',\n",
       " 'zipper-fabric_interior',\n",
       " 'cable-missing_cable',\n",
       " 'carpet-good',\n",
       " 'carpet-thread',\n",
       " 'screw-manipulated_front',\n",
       " 'leather-glue',\n",
       " 'cable-good',\n",
       " 'pill-crack',\n",
       " 'screw-good',\n",
       " 'zipper-split_teeth',\n",
       " 'capsule-good',\n",
       " 'zipper-good',\n",
       " 'leather-good',\n",
       " 'grid-glue',\n",
       " 'bottle-good',\n",
       " 'cable-missing_wire',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-cut',\n",
       " 'tile-glue_strip',\n",
       " 'metal_nut-good',\n",
       " 'wood-scratch',\n",
       " 'transistor-misplaced',\n",
       " 'pill-scratch',\n",
       " 'capsule-squeeze',\n",
       " 'zipper-good',\n",
       " 'hazelnut-cut',\n",
       " 'pill-good',\n",
       " 'screw-scratch_head',\n",
       " 'zipper-combined',\n",
       " 'zipper-rough',\n",
       " 'zipper-good',\n",
       " 'hazelnut-good',\n",
       " 'transistor-good',\n",
       " 'cable-poke_insulation',\n",
       " 'screw-scratch_neck',\n",
       " 'bottle-broken_small',\n",
       " 'pill-good',\n",
       " 'leather-good',\n",
       " 'pill-good',\n",
       " 'pill-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-bent',\n",
       " 'leather-poke',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'wood-good',\n",
       " 'pill-combined',\n",
       " 'carpet-cut',\n",
       " 'capsule-squeeze',\n",
       " 'pill-contamination',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_small',\n",
       " 'tile-oil',\n",
       " 'grid-good',\n",
       " 'tile-good',\n",
       " 'bottle-broken_small',\n",
       " 'capsule-good',\n",
       " 'metal_nut-color',\n",
       " 'pill-faulty_imprint',\n",
       " 'leather-color',\n",
       " 'cable-good',\n",
       " 'transistor-good',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'zipper-combined',\n",
       " 'pill-good',\n",
       " 'zipper-broken_teeth',\n",
       " 'capsule-scratch',\n",
       " 'transistor-good',\n",
       " 'metal_nut-good',\n",
       " 'hazelnut-crack',\n",
       " 'capsule-faulty_imprint',\n",
       " 'metal_nut-good',\n",
       " 'hazelnut-hole',\n",
       " 'zipper-rough',\n",
       " 'screw-thread_side',\n",
       " 'wood-color',\n",
       " 'leather-poke',\n",
       " 'tile-good',\n",
       " 'tile-gray_stroke',\n",
       " 'wood-scratch',\n",
       " 'cable-cable_swap',\n",
       " 'transistor-good',\n",
       " 'tile-gray_stroke',\n",
       " 'cable-bent_wire',\n",
       " 'capsule-good',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-good',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-crack',\n",
       " 'zipper-split_teeth',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-crack',\n",
       " 'screw-good',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-cut',\n",
       " 'leather-poke',\n",
       " 'leather-good',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'metal_nut-flip',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'screw-good',\n",
       " 'wood-hole',\n",
       " 'transistor-misplaced',\n",
       " 'cable-good',\n",
       " 'capsule-good',\n",
       " 'pill-color',\n",
       " 'capsule-good',\n",
       " 'transistor-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'pill-color',\n",
       " 'leather-color',\n",
       " 'transistor-good',\n",
       " 'pill-pill_type',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'grid-thread',\n",
       " 'tile-good',\n",
       " 'tile-good',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-hole',\n",
       " 'bottle-broken_small',\n",
       " 'leather-fold',\n",
       " 'leather-cut',\n",
       " 'cable-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'tile-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_neck',\n",
       " 'leather-poke',\n",
       " 'zipper-broken_teeth',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-faulty_imprint',\n",
       " 'toothbrush-good',\n",
       " 'grid-good',\n",
       " 'leather-color',\n",
       " 'screw-manipulated_front',\n",
       " 'screw-good',\n",
       " 'transistor-good',\n",
       " 'bottle-good',\n",
       " 'metal_nut-scratch',\n",
       " 'wood-good',\n",
       " 'transistor-good',\n",
       " 'screw-thread_side',\n",
       " 'wood-good',\n",
       " 'wood-good',\n",
       " 'tile-good',\n",
       " 'cable-good',\n",
       " 'screw-scratch_neck',\n",
       " 'metal_nut-scratch',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'leather-good',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'screw-manipulated_front',\n",
       " 'pill-crack',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'bottle-good',\n",
       " 'tile-good',\n",
       " 'metal_nut-color',\n",
       " 'screw-scratch_neck',\n",
       " 'cable-bent_wire',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'screw-good',\n",
       " 'leather-glue',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-bent',\n",
       " 'hazelnut-good',\n",
       " 'pill-pill_type',\n",
       " 'carpet-thread',\n",
       " 'cable-cable_swap',\n",
       " 'cable-good',\n",
       " 'capsule-scratch',\n",
       " 'carpet-good',\n",
       " 'capsule-poke',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'bottle-good',\n",
       " 'toothbrush-defective',\n",
       " 'transistor-cut_lead',\n",
       " 'capsule-crack',\n",
       " 'leather-fold',\n",
       " 'carpet-color',\n",
       " 'transistor-good',\n",
       " 'zipper-rough',\n",
       " 'wood-color',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'cable-cable_swap',\n",
       " 'leather-cut',\n",
       " 'hazelnut-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'bottle-good',\n",
       " 'hazelnut-good',\n",
       " 'transistor-good',\n",
       " 'wood-hole',\n",
       " 'screw-manipulated_front',\n",
       " 'capsule-faulty_imprint',\n",
       " 'grid-good',\n",
       " 'metal_nut-scratch',\n",
       " 'screw-good',\n",
       " 'wood-color',\n",
       " 'tile-glue_strip',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'hazelnut-cut',\n",
       " 'screw-good',\n",
       " 'leather-good',\n",
       " 'metal_nut-color',\n",
       " 'bottle-broken_small',\n",
       " 'leather-cut',\n",
       " 'capsule-poke',\n",
       " 'transistor-misplaced',\n",
       " 'cable-cable_swap',\n",
       " 'bottle-contamination',\n",
       " 'hazelnut-print',\n",
       " 'tile-good',\n",
       " 'wood-liquid',\n",
       " 'tile-gray_stroke',\n",
       " 'transistor-good',\n",
       " 'zipper-combined',\n",
       " 'bottle-contamination',\n",
       " 'tile-good',\n",
       " 'pill-good',\n",
       " 'capsule-scratch',\n",
       " 'leather-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'cable-good',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'bottle-broken_small',\n",
       " 'transistor-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'hazelnut-cut',\n",
       " 'transistor-bent_lead',\n",
       " 'metal_nut-scratch',\n",
       " 'zipper-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'pill-good',\n",
       " 'zipper-fabric_border',\n",
       " 'zipper-good',\n",
       " 'leather-glue',\n",
       " 'transistor-damaged_case',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'bottle-broken_small',\n",
       " 'tile-good',\n",
       " 'hazelnut-crack',\n",
       " 'wood-scratch',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-good',\n",
       " 'grid-metal_contamination',\n",
       " 'pill-color',\n",
       " 'tile-oil',\n",
       " 'transistor-misplaced',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-bent_lead',\n",
       " 'transistor-cut_lead',\n",
       " 'carpet-metal_contamination',\n",
       " 'zipper-broken_teeth',\n",
       " 'tile-glue_strip',\n",
       " 'leather-fold',\n",
       " 'wood-hole',\n",
       " 'zipper-good',\n",
       " 'bottle-broken_small',\n",
       " 'carpet-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'carpet-hole',\n",
       " 'metal_nut-flip',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'screw-thread_top',\n",
       " 'leather-poke',\n",
       " 'carpet-thread',\n",
       " 'capsule-faulty_imprint',\n",
       " 'capsule-crack',\n",
       " 'metal_nut-flip',\n",
       " 'screw-thread_side',\n",
       " 'metal_nut-good',\n",
       " 'pill-pill_type',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'bottle-good',\n",
       " 'screw-manipulated_front',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'transistor-misplaced',\n",
       " 'screw-thread_top',\n",
       " 'screw-thread_top',\n",
       " 'pill-crack',\n",
       " 'screw-thread_side',\n",
       " 'carpet-hole',\n",
       " 'cable-good',\n",
       " 'screw-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'grid-good',\n",
       " 'hazelnut-hole',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-cut',\n",
       " 'cable-poke_insulation',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'screw-good',\n",
       " 'hazelnut-good',\n",
       " 'capsule-squeeze',\n",
       " 'transistor-cut_lead',\n",
       " 'toothbrush-defective',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'zipper-split_teeth',\n",
       " 'screw-manipulated_front',\n",
       " 'tile-oil',\n",
       " 'metal_nut-scratch',\n",
       " 'transistor-good',\n",
       " 'cable-bent_wire',\n",
       " 'tile-good',\n",
       " 'leather-fold',\n",
       " 'wood-liquid',\n",
       " 'toothbrush-good',\n",
       " 'leather-poke',\n",
       " 'capsule-poke',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-scratch',\n",
       " 'leather-good',\n",
       " 'cable-good',\n",
       " 'tile-gray_stroke',\n",
       " 'pill-pill_type',\n",
       " 'wood-good',\n",
       " 'metal_nut-good',\n",
       " 'pill-good',\n",
       " 'zipper-combined',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'capsule-crack',\n",
       " 'carpet-cut',\n",
       " 'capsule-good',\n",
       " 'hazelnut-hole',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-hole',\n",
       " 'grid-good',\n",
       " 'screw-good',\n",
       " 'zipper-split_teeth',\n",
       " 'wood-scratch',\n",
       " 'pill-contamination',\n",
       " 'toothbrush-defective',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'grid-metal_contamination',\n",
       " 'screw-good',\n",
       " 'grid-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'transistor-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'pill-scratch',\n",
       " 'cable-good',\n",
       " 'capsule-crack',\n",
       " 'pill-color',\n",
       " 'pill-good',\n",
       " 'zipper-combined',\n",
       " 'tile-glue_strip',\n",
       " 'screw-good',\n",
       " 'capsule-good',\n",
       " 'zipper-combined',\n",
       " 'screw-manipulated_front',\n",
       " 'leather-glue',\n",
       " 'capsule-good',\n",
       " 'leather-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-good',\n",
       " 'grid-good',\n",
       " 'hazelnut-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'wood-scratch',\n",
       " 'hazelnut-good',\n",
       " 'pill-good',\n",
       " 'cable-missing_wire',\n",
       " 'wood-hole',\n",
       " 'zipper-split_teeth',\n",
       " 'carpet-good',\n",
       " 'grid-glue',\n",
       " 'zipper-fabric_border',\n",
       " 'pill-contamination',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'carpet-cut',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'carpet-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'screw-good',\n",
       " 'hazelnut-cut',\n",
       " 'cable-good',\n",
       " 'zipper-broken_teeth',\n",
       " 'zipper-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-metal_contamination',\n",
       " 'tile-good',\n",
       " 'zipper-broken_teeth',\n",
       " 'hazelnut-good',\n",
       " 'leather-good',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-hole',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'pill-scratch',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'transistor-cut_lead',\n",
       " 'pill-faulty_imprint',\n",
       " 'metal_nut-bent',\n",
       " 'grid-glue',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'hazelnut-good',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'zipper-split_teeth',\n",
       " 'carpet-thread',\n",
       " 'cable-bent_wire',\n",
       " 'cable-good',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'tile-glue_strip',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-hole',\n",
       " 'metal_nut-bent',\n",
       " 'toothbrush-good',\n",
       " 'hazelnut-good',\n",
       " 'bottle-broken_small',\n",
       " 'leather-good',\n",
       " 'toothbrush-defective',\n",
       " 'screw-good',\n",
       " 'screw-scratch_neck',\n",
       " 'zipper-broken_teeth',\n",
       " 'leather-good',\n",
       " 'cable-missing_wire',\n",
       " 'capsule-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'leather-poke',\n",
       " 'pill-good',\n",
       " 'leather-fold',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'capsule-good',\n",
       " 'zipper-split_teeth',\n",
       " 'wood-hole',\n",
       " 'hazelnut-print',\n",
       " 'carpet-good',\n",
       " 'screw-manipulated_front',\n",
       " 'zipper-combined',\n",
       " 'wood-good',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-crack',\n",
       " 'capsule-squeeze',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'carpet-good',\n",
       " 'transistor-cut_lead',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'wood-hole',\n",
       " 'hazelnut-good',\n",
       " 'tile-rough',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'pill-crack',\n",
       " 'bottle-good',\n",
       " 'pill-scratch',\n",
       " 'pill-faulty_imprint',\n",
       " 'bottle-broken_large',\n",
       " 'wood-good',\n",
       " 'pill-crack',\n",
       " 'cable-good',\n",
       " 'tile-good',\n",
       " 'leather-good',\n",
       " 'leather-fold',\n",
       " 'tile-oil',\n",
       " 'pill-scratch',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'tile-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-good',\n",
       " 'leather-good',\n",
       " 'metal_nut-good',\n",
       " 'cable-missing_cable',\n",
       " 'screw-manipulated_front',\n",
       " 'pill-color',\n",
       " 'grid-good',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'zipper-rough',\n",
       " 'tile-good',\n",
       " 'bottle-contamination',\n",
       " 'cable-missing_cable',\n",
       " 'leather-poke',\n",
       " 'hazelnut-cut',\n",
       " 'screw-thread_side',\n",
       " 'bottle-broken_small',\n",
       " 'carpet-color',\n",
       " 'screw-good',\n",
       " 'transistor-damaged_case',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'wood-good',\n",
       " 'pill-crack',\n",
       " 'pill-good',\n",
       " 'screw-scratch_neck',\n",
       " 'grid-thread',\n",
       " 'carpet-thread',\n",
       " 'wood-scratch',\n",
       " 'carpet-color',\n",
       " 'leather-poke',\n",
       " 'hazelnut-good',\n",
       " 'zipper-good',\n",
       " 'capsule-good',\n",
       " 'transistor-good',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-good',\n",
       " 'zipper-fabric_border',\n",
       " 'transistor-good',\n",
       " 'pill-crack',\n",
       " 'hazelnut-good',\n",
       " 'pill-good',\n",
       " 'cable-bent_wire',\n",
       " 'hazelnut-good',\n",
       " 'bottle-good',\n",
       " 'pill-scratch',\n",
       " 'pill-faulty_imprint',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-good',\n",
       " 'pill-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'wood-scratch',\n",
       " 'hazelnut-good',\n",
       " 'carpet-thread',\n",
       " 'pill-color',\n",
       " 'pill-faulty_imprint',\n",
       " 'tile-good',\n",
       " 'carpet-good',\n",
       " 'bottle-good',\n",
       " 'pill-crack',\n",
       " 'pill-combined',\n",
       " 'capsule-crack',\n",
       " 'metal_nut-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-flip',\n",
       " 'transistor-good',\n",
       " 'capsule-scratch',\n",
       " 'tile-oil',\n",
       " 'zipper-good',\n",
       " 'bottle-good',\n",
       " 'pill-good',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'hazelnut-hole',\n",
       " 'cable-missing_cable',\n",
       " 'pill-scratch',\n",
       " 'bottle-good',\n",
       " 'capsule-good',\n",
       " 'grid-bent',\n",
       " 'zipper-fabric_border',\n",
       " 'metal_nut-good',\n",
       " 'pill-color',\n",
       " 'wood-color',\n",
       " 'carpet-metal_contamination',\n",
       " 'wood-combined',\n",
       " 'zipper-fabric_interior',\n",
       " 'hazelnut-hole',\n",
       " 'screw-thread_side',\n",
       " 'pill-combined',\n",
       " 'screw-good',\n",
       " 'bottle-broken_large',\n",
       " 'capsule-crack',\n",
       " 'transistor-good',\n",
       " 'leather-color',\n",
       " 'screw-good',\n",
       " 'wood-combined',\n",
       " 'cable-good',\n",
       " 'leather-good',\n",
       " 'toothbrush-good',\n",
       " 'toothbrush-good',\n",
       " 'screw-scratch_head',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'zipper-rough',\n",
       " 'carpet-good',\n",
       " 'toothbrush-defective',\n",
       " 'toothbrush-good',\n",
       " 'zipper-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'zipper-rough',\n",
       " 'screw-good',\n",
       " 'pill-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-crack',\n",
       " 'tile-rough',\n",
       " 'metal_nut-color',\n",
       " 'tile-gray_stroke',\n",
       " 'wood-good',\n",
       " 'pill-good',\n",
       " 'bottle-broken_large',\n",
       " 'leather-glue',\n",
       " 'zipper-split_teeth',\n",
       " 'cable-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'capsule-scratch',\n",
       " 'grid-bent',\n",
       " 'toothbrush-good',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'bottle-contamination',\n",
       " 'tile-crack',\n",
       " 'screw-good',\n",
       " 'pill-scratch',\n",
       " 'hazelnut-print',\n",
       " 'cable-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'capsule-good',\n",
       " 'cable-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'carpet-color',\n",
       " 'grid-bent',\n",
       " 'cable-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-good',\n",
       " 'tile-good',\n",
       " 'capsule-poke',\n",
       " 'screw-scratch_head',\n",
       " 'carpet-good',\n",
       " 'pill-crack',\n",
       " 'hazelnut-hole',\n",
       " 'bottle-broken_large',\n",
       " 'screw-good',\n",
       " 'screw-manipulated_front',\n",
       " 'tile-crack',\n",
       " 'transistor-bent_lead',\n",
       " 'leather-color',\n",
       " 'bottle-contamination',\n",
       " 'tile-oil',\n",
       " 'capsule-good',\n",
       " 'tile-oil',\n",
       " 'wood-liquid',\n",
       " 'screw-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'transistor-good',\n",
       " 'capsule-poke',\n",
       " 'zipper-fabric_border',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'zipper-rough',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'carpet-cut',\n",
       " 'transistor-good',\n",
       " 'toothbrush-defective',\n",
       " 'cable-good',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'leather-glue',\n",
       " 'pill-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'pill-good',\n",
       " 'carpet-good',\n",
       " 'screw-thread_top',\n",
       " 'wood-hole',\n",
       " 'leather-good',\n",
       " 'leather-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-bent',\n",
       " 'capsule-good',\n",
       " 'capsule-poke',\n",
       " 'transistor-good',\n",
       " 'tile-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-color',\n",
       " 'leather-good',\n",
       " 'metal_nut-flip',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'leather-good',\n",
       " 'cable-poke_insulation',\n",
       " 'leather-cut',\n",
       " 'screw-good',\n",
       " 'pill-scratch',\n",
       " 'screw-good',\n",
       " 'cable-poke_insulation',\n",
       " 'carpet-good',\n",
       " 'transistor-damaged_case',\n",
       " 'leather-good',\n",
       " 'screw-good',\n",
       " 'pill-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-rough',\n",
       " 'leather-fold',\n",
       " 'capsule-crack',\n",
       " 'screw-scratch_neck',\n",
       " 'metal_nut-good',\n",
       " 'tile-glue_strip',\n",
       " 'pill-good',\n",
       " 'zipper-combined',\n",
       " 'capsule-squeeze',\n",
       " 'leather-glue',\n",
       " 'screw-thread_top',\n",
       " 'metal_nut-flip',\n",
       " 'screw-thread_top',\n",
       " 'tile-oil',\n",
       " 'transistor-misplaced',\n",
       " 'hazelnut-good',\n",
       " 'screw-good',\n",
       " 'tile-rough',\n",
       " 'zipper-good',\n",
       " 'metal_nut-flip',\n",
       " 'leather-glue',\n",
       " 'cable-bent_wire',\n",
       " 'toothbrush-good',\n",
       " 'cable-good',\n",
       " 'toothbrush-good',\n",
       " 'tile-good',\n",
       " 'cable-cable_swap',\n",
       " 'capsule-squeeze',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'tile-oil',\n",
       " 'leather-cut',\n",
       " 'carpet-color',\n",
       " 'leather-good',\n",
       " 'carpet-thread',\n",
       " 'leather-fold',\n",
       " 'screw-scratch_neck',\n",
       " 'screw-manipulated_front',\n",
       " 'bottle-contamination',\n",
       " 'leather-good',\n",
       " 'leather-glue',\n",
       " 'capsule-scratch',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-hole',\n",
       " 'capsule-scratch',\n",
       " 'metal_nut-scratch',\n",
       " 'tile-good',\n",
       " 'grid-thread',\n",
       " 'screw-manipulated_front',\n",
       " 'carpet-thread',\n",
       " 'capsule-good',\n",
       " 'transistor-misplaced',\n",
       " 'capsule-good',\n",
       " 'wood-hole',\n",
       " 'zipper-good',\n",
       " 'screw-scratch_neck',\n",
       " 'pill-good',\n",
       " 'hazelnut-hole',\n",
       " 'hazelnut-good',\n",
       " 'cable-poke_insulation',\n",
       " 'screw-scratch_neck',\n",
       " 'hazelnut-print',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'bottle-broken_large',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'tile-crack',\n",
       " 'pill-pill_type',\n",
       " 'leather-good',\n",
       " 'screw-scratch_head',\n",
       " 'screw-good',\n",
       " 'leather-good',\n",
       " 'carpet-good',\n",
       " 'screw-thread_side',\n",
       " 'transistor-good',\n",
       " 'pill-scratch',\n",
       " 'capsule-squeeze',\n",
       " 'grid-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'screw-good',\n",
       " 'pill-color',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-good',\n",
       " 'cable-missing_wire',\n",
       " 'cable-good',\n",
       " 'bottle-good',\n",
       " 'zipper-fabric_border',\n",
       " 'bottle-good',\n",
       " 'leather-fold',\n",
       " 'screw-good',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile-glue_strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper-good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index             label\n",
       "0         0   tile-glue_strip\n",
       "1         1         grid-good\n",
       "2         2   transistor-good\n",
       "3         3  tile-gray_stroke\n",
       "4         4         tile-good\n",
       "...     ...               ...\n",
       "2149   2149  tile-gray_stroke\n",
       "2150   2150        screw-good\n",
       "2151   2151         grid-good\n",
       "2152   2152        cable-good\n",
       "2153   2153       zipper-good\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./submission/label_result_add_0505_label_tf_efficientnet_b4_ns_tta.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
