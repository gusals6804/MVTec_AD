{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file_name       class state            label  label2  class2  state2\n",
      "0  10000.png  transistor  good  transistor-good      72      12      25\n",
      "1  10001.png     capsule  good     capsule-good      15       2      25\n",
      "2  10002.png  transistor  good  transistor-good      72      12      25\n",
      "3  10003.png        wood  good        wood-good      76      13      25\n",
      "4  10004.png      bottle  good      bottle-good       3       0      25\n",
      "   index  file_name\n",
      "0      0  20000.png\n",
      "1      1  20001.png\n",
      "2      2  20002.png\n",
      "3      3  20003.png\n",
      "4      4  20004.png\n",
      "(4277, 7)\n",
      "(2154, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './open'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df2.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = pd.read_csv(\"./open/train_df.csv\")\n",
    "\n",
    "# train_labels = train_y[\"state\"]\n",
    "\n",
    "# label_unique = sorted(np.unique(train_labels))\n",
    "# label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "# train_labels = [label_unique[k] for k in train_labels]\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['state2'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./open/train_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(train_df.class2.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./open',\n",
    "     'Kfold':5,\n",
    "     'model_path':'class_results/',\n",
    "     'image_type':'train_1024', \n",
    "     'class_num' : class_num,\n",
    "\n",
    "     # Model parameter settings\n",
    "     'model_name':'efficientnet_b3',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':224,\n",
    "     'batch_size':16,\n",
    "     'epochs':70,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-4,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-6,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':5,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':True,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "     \n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return x + self.conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def coatnet_0():\n",
    "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_1():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_2():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [128, 128, 256, 512, 1026]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_3():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_4():\n",
    "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, p: float, degrees: int):\n",
    "        super(RandomRotation, self).__init__(degrees)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            fill = self.fill\n",
    "            if isinstance(img, Tensor):\n",
    "                if isinstance(fill, (int, float)):\n",
    "                    fill = [float(fill)] * F.get_image_num_channels(img)\n",
    "                else:\n",
    "                    fill = [float(f) for f in fill]\n",
    "            angle = self.get_params(self.degrees)\n",
    "\n",
    "            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.target = df['class2'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         image = cv2.imread(opj('./open/train/', self.img_path[idx])).astype(np.float32)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "#         target = self.target[idx]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "        \n",
    "        image = Image.open(opj('./open/train/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "#         image = cv2.imread(opj('./open/train/', self.img_path[idx])).astype(np.float32)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "#         target = self.target[idx]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomAffine((-20, 20)),\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "#         self.cnn = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "#             args.model_name,\n",
    "#             pretrained=True,\n",
    "#             num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "#         )\n",
    "\n",
    "        self.model_ft = coatnet_3()\n",
    "        num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "#         self.cnn = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "#             args.model_name,\n",
    "#             pretrained=True,\n",
    "#             num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "#         )\n",
    "\n",
    "        self.model_ft = coatnet_3()\n",
    "        num_ftrs = self.model_ft.fc.in_features\n",
    "        self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted crossentropy loss를 위한 weight 계산 함수\n",
    "def get_class_weight():\n",
    "    return 1 / train_df['class2'].value_counts().sort_index().values\n",
    "\n",
    "class_weight = get_class_weight()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train_df2.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "#         if args.image_type is not None:\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['class2'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss(weight= torch.Tensor(class_weight).cuda())\n",
    "#         self.criterion = CutMixCrossEntropyLoss(True)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 2 and 3 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./open/sample_submission.csv')\n",
    "df_train = pd.read_csv('./open/train_df2.csv')\n",
    "df_test = pd.read_csv('./open/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 19:57:18,561 INFO: {'exp_num': '0', 'data_path': './open', 'Kfold': 5, 'model_path': 'class_results/', 'image_type': 'train_1024', 'class_num': 15, 'model_name': 'efficientnet_b3', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 70, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3421\n",
      "Dataset size:856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:18<00:00,  2.74it/s]\n",
      "2022-04-12 19:58:41,439 INFO: Epoch:[001/070]\n",
      "2022-04-12 19:58:41,440 INFO: Train Loss:1.995 | Acc:0.4987 | F1:0.4848\n",
      "2022-04-12 19:58:52,836 INFO: val Loss:0.408 | Acc:0.9136 | F1:0.9051\n",
      "2022-04-12 19:58:55,720 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.19it/s]\n",
      "2022-04-12 20:00:02,828 INFO: Epoch:[002/070]\n",
      "2022-04-12 20:00:02,828 INFO: Train Loss:0.230 | Acc:0.9471 | F1:0.9430\n",
      "2022-04-12 20:00:13,380 INFO: val Loss:0.094 | Acc:0.9439 | F1:0.9324\n",
      "2022-04-12 20:00:16,280 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.23it/s]\n",
      "2022-04-12 20:01:22,586 INFO: Epoch:[003/070]\n",
      "2022-04-12 20:01:22,586 INFO: Train Loss:0.077 | Acc:0.9795 | F1:0.9792\n",
      "2022-04-12 20:01:33,586 INFO: val Loss:0.009 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:01:36,508 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 20:02:42,432 INFO: Epoch:[004/070]\n",
      "2022-04-12 20:02:42,433 INFO: Train Loss:0.083 | Acc:0.9804 | F1:0.9806\n",
      "2022-04-12 20:02:53,388 INFO: val Loss:0.011 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 20:03:59,160 INFO: Epoch:[005/070]\n",
      "2022-04-12 20:03:59,160 INFO: Train Loss:0.071 | Acc:0.9801 | F1:0.9765\n",
      "2022-04-12 20:04:09,982 INFO: val Loss:0.071 | Acc:0.9801 | F1:0.9768\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.22it/s]\n",
      "2022-04-12 20:05:16,535 INFO: Epoch:[006/070]\n",
      "2022-04-12 20:05:16,536 INFO: Train Loss:0.018 | Acc:0.9965 | F1:0.9952\n",
      "2022-04-12 20:05:27,387 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:05:30,261 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.27it/s]\n",
      "2022-04-12 20:06:35,806 INFO: Epoch:[007/070]\n",
      "2022-04-12 20:06:35,806 INFO: Train Loss:0.005 | Acc:0.9982 | F1:0.9982\n",
      "2022-04-12 20:06:46,857 INFO: val Loss:0.003 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 20:07:54,084 INFO: Epoch:[008/070]\n",
      "2022-04-12 20:07:54,085 INFO: Train Loss:0.007 | Acc:0.9980 | F1:0.9979\n",
      "2022-04-12 20:08:04,904 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.26it/s]\n",
      "2022-04-12 20:09:10,505 INFO: Epoch:[009/070]\n",
      "2022-04-12 20:09:10,506 INFO: Train Loss:0.009 | Acc:0.9974 | F1:0.9973\n",
      "2022-04-12 20:09:21,442 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.24it/s]\n",
      "2022-04-12 20:10:27,564 INFO: Epoch:[010/070]\n",
      "2022-04-12 20:10:27,565 INFO: Train Loss:0.022 | Acc:0.9956 | F1:0.9937\n",
      "2022-04-12 20:10:38,480 INFO: val Loss:0.001 | Acc:0.9988 | F1:0.9975\n",
      "100%|██████████| 214/214 [01:04<00:00,  3.33it/s]\n",
      "2022-04-12 20:11:42,736 INFO: Epoch:[011/070]\n",
      "2022-04-12 20:11:42,737 INFO: Train Loss:0.024 | Acc:0.9950 | F1:0.9947\n",
      "2022-04-12 20:11:53,520 INFO: val Loss:0.001 | Acc:0.9988 | F1:0.9975\n",
      "2022-04-12 20:11:53,521 INFO: \n",
      "Best Val Epoch:6 | Val Loss:0.0001 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2022-04-12 20:11:53,522 INFO: Total Process time:14.503Minute\n",
      "2022-04-12 20:11:53,525 INFO: {'exp_num': '1', 'data_path': './open', 'Kfold': 5, 'model_path': 'class_results/', 'image_type': 'train_1024', 'class_num': 15, 'model_name': 'efficientnet_b3', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 70, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3421\n",
      "Dataset size:856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:06<00:00,  3.21it/s]\n",
      "2022-04-12 20:13:01,126 INFO: Epoch:[001/070]\n",
      "2022-04-12 20:13:01,126 INFO: Train Loss:2.006 | Acc:0.5007 | F1:0.5007\n",
      "2022-04-12 20:13:12,001 INFO: val Loss:0.482 | Acc:0.8715 | F1:0.8367\n",
      "2022-04-12 20:13:14,883 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.26it/s]\n",
      "2022-04-12 20:14:20,494 INFO: Epoch:[002/070]\n",
      "2022-04-12 20:14:20,495 INFO: Train Loss:0.210 | Acc:0.9556 | F1:0.9541\n",
      "2022-04-12 20:14:31,858 INFO: val Loss:0.019 | Acc:0.9977 | F1:0.9978\n",
      "2022-04-12 20:14:34,579 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 20:15:40,460 INFO: Epoch:[003/070]\n",
      "2022-04-12 20:15:40,461 INFO: Train Loss:0.080 | Acc:0.9772 | F1:0.9759\n",
      "2022-04-12 20:15:51,730 INFO: val Loss:0.107 | Acc:0.9918 | F1:0.9764\n",
      "100%|██████████| 214/214 [01:03<00:00,  3.36it/s]\n",
      "2022-04-12 20:16:55,393 INFO: Epoch:[004/070]\n",
      "2022-04-12 20:16:55,393 INFO: Train Loss:0.074 | Acc:0.9845 | F1:0.9837\n",
      "2022-04-12 20:17:06,738 INFO: val Loss:0.015 | Acc:0.9965 | F1:0.9968\n",
      "2022-04-12 20:17:09,447 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.27it/s]\n",
      "2022-04-12 20:18:14,915 INFO: Epoch:[005/070]\n",
      "2022-04-12 20:18:14,916 INFO: Train Loss:0.054 | Acc:0.9842 | F1:0.9839\n",
      "2022-04-12 20:18:26,117 INFO: val Loss:0.078 | Acc:0.9790 | F1:0.9758\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 20:19:32,924 INFO: Epoch:[006/070]\n",
      "2022-04-12 20:19:32,925 INFO: Train Loss:0.022 | Acc:0.9942 | F1:0.9942\n",
      "2022-04-12 20:19:44,464 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:19:47,259 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.23it/s]\n",
      "2022-04-12 20:20:53,500 INFO: Epoch:[007/070]\n",
      "2022-04-12 20:20:53,501 INFO: Train Loss:0.008 | Acc:0.9980 | F1:0.9980\n",
      "2022-04-12 20:21:04,327 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:21:07,054 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.24it/s]\n",
      "2022-04-12 20:22:13,167 INFO: Epoch:[008/070]\n",
      "2022-04-12 20:22:13,168 INFO: Train Loss:0.011 | Acc:0.9971 | F1:0.9971\n",
      "2022-04-12 20:22:24,156 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 20:23:29,983 INFO: Epoch:[009/070]\n",
      "2022-04-12 20:23:29,984 INFO: Train Loss:0.015 | Acc:0.9962 | F1:0.9946\n",
      "2022-04-12 20:23:41,260 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.23it/s]\n",
      "2022-04-12 20:24:47,567 INFO: Epoch:[010/070]\n",
      "2022-04-12 20:24:47,568 INFO: Train Loss:0.013 | Acc:0.9956 | F1:0.9958\n",
      "2022-04-12 20:24:58,864 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:25:01,686 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.22it/s]\n",
      "2022-04-12 20:26:08,134 INFO: Epoch:[011/070]\n",
      "2022-04-12 20:26:08,135 INFO: Train Loss:0.020 | Acc:0.9947 | F1:0.9944\n",
      "2022-04-12 20:26:19,261 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.29it/s]\n",
      "2022-04-12 20:27:24,356 INFO: Epoch:[012/070]\n",
      "2022-04-12 20:27:24,357 INFO: Train Loss:0.042 | Acc:0.9942 | F1:0.9915\n",
      "2022-04-12 20:27:35,745 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 20:28:41,547 INFO: Epoch:[013/070]\n",
      "2022-04-12 20:28:41,547 INFO: Train Loss:0.058 | Acc:0.9950 | F1:0.9929\n",
      "2022-04-12 20:28:52,877 INFO: val Loss:0.002 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.19it/s]\n",
      "2022-04-12 20:29:59,924 INFO: Epoch:[014/070]\n",
      "2022-04-12 20:29:59,924 INFO: Train Loss:0.033 | Acc:0.9939 | F1:0.9931\n",
      "2022-04-12 20:30:11,312 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 20:31:18,177 INFO: Epoch:[015/070]\n",
      "2022-04-12 20:31:18,178 INFO: Train Loss:0.050 | Acc:0.9915 | F1:0.9909\n",
      "2022-04-12 20:31:29,372 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:31:29,373 INFO: \n",
      "Best Val Epoch:10 | Val Loss:0.0000 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2022-04-12 20:31:29,374 INFO: Total Process time:19.581Minute\n",
      "2022-04-12 20:31:29,377 INFO: {'exp_num': '2', 'data_path': './open', 'Kfold': 5, 'model_path': 'class_results/', 'image_type': 'train_1024', 'class_num': 15, 'model_name': 'efficientnet_b3', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 70, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3422\n",
      "Dataset size:855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:08<00:00,  3.11it/s]\n",
      "2022-04-12 20:32:39,136 INFO: Epoch:[001/070]\n",
      "2022-04-12 20:32:39,136 INFO: Train Loss:1.991 | Acc:0.4787 | F1:0.4784\n",
      "2022-04-12 20:32:51,139 INFO: val Loss:0.402 | Acc:0.9146 | F1:0.9061\n",
      "2022-04-12 20:32:53,946 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.17it/s]\n",
      "2022-04-12 20:34:01,458 INFO: Epoch:[002/070]\n",
      "2022-04-12 20:34:01,459 INFO: Train Loss:0.219 | Acc:0.9497 | F1:0.9496\n",
      "2022-04-12 20:34:13,110 INFO: val Loss:0.215 | Acc:0.9462 | F1:0.9365\n",
      "2022-04-12 20:34:15,872 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 20:35:22,661 INFO: Epoch:[003/070]\n",
      "2022-04-12 20:35:22,662 INFO: Train Loss:0.082 | Acc:0.9810 | F1:0.9798\n",
      "2022-04-12 20:35:34,368 INFO: val Loss:0.022 | Acc:0.9953 | F1:0.9953\n",
      "2022-04-12 20:35:37,107 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 20:36:44,044 INFO: Epoch:[004/070]\n",
      "2022-04-12 20:36:44,045 INFO: Train Loss:0.065 | Acc:0.9819 | F1:0.9809\n",
      "2022-04-12 20:36:55,257 INFO: val Loss:0.004 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:36:58,021 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 20:38:03,948 INFO: Epoch:[005/070]\n",
      "2022-04-12 20:38:03,949 INFO: Train Loss:0.086 | Acc:0.9831 | F1:0.9799\n",
      "2022-04-12 20:38:15,187 INFO: val Loss:0.004 | Acc:0.9988 | F1:0.9975\n",
      "2022-04-12 20:38:17,952 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.19it/s]\n",
      "2022-04-12 20:39:25,056 INFO: Epoch:[006/070]\n",
      "2022-04-12 20:39:25,056 INFO: Train Loss:0.023 | Acc:0.9930 | F1:0.9911\n",
      "2022-04-12 20:39:36,754 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:39:39,525 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.19it/s]\n",
      "2022-04-12 20:40:46,605 INFO: Epoch:[007/070]\n",
      "2022-04-12 20:40:46,606 INFO: Train Loss:0.007 | Acc:0.9974 | F1:0.9974\n",
      "2022-04-12 20:40:57,766 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:41:00,531 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 20:42:07,821 INFO: Epoch:[008/070]\n",
      "2022-04-12 20:42:07,821 INFO: Train Loss:0.010 | Acc:0.9968 | F1:0.9964\n",
      "2022-04-12 20:42:19,579 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:42:22,463 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.19it/s]\n",
      "2022-04-12 20:43:29,496 INFO: Epoch:[009/070]\n",
      "2022-04-12 20:43:29,497 INFO: Train Loss:0.013 | Acc:0.9968 | F1:0.9968\n",
      "2022-04-12 20:43:41,137 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:03<00:00,  3.39it/s]\n",
      "2022-04-12 20:44:44,324 INFO: Epoch:[010/070]\n",
      "2022-04-12 20:44:44,325 INFO: Train Loss:0.010 | Acc:0.9977 | F1:0.9977\n",
      "2022-04-12 20:44:55,785 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 20:46:02,651 INFO: Epoch:[011/070]\n",
      "2022-04-12 20:46:02,651 INFO: Train Loss:0.035 | Acc:0.9950 | F1:0.9949\n",
      "2022-04-12 20:46:14,395 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 20:47:21,303 INFO: Epoch:[012/070]\n",
      "2022-04-12 20:47:21,304 INFO: Train Loss:0.020 | Acc:0.9956 | F1:0.9925\n",
      "2022-04-12 20:47:32,899 INFO: val Loss:0.015 | Acc:0.9977 | F1:0.9945\n",
      "100%|██████████| 214/214 [01:04<00:00,  3.33it/s]\n",
      "2022-04-12 20:48:37,169 INFO: Epoch:[013/070]\n",
      "2022-04-12 20:48:37,169 INFO: Train Loss:0.035 | Acc:0.9942 | F1:0.9923\n",
      "2022-04-12 20:48:48,778 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:48:48,779 INFO: \n",
      "Best Val Epoch:8 | Val Loss:0.0000 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2022-04-12 20:48:48,780 INFO: Total Process time:17.307Minute\n",
      "2022-04-12 20:48:48,783 INFO: {'exp_num': '3', 'data_path': './open', 'Kfold': 5, 'model_path': 'class_results/', 'image_type': 'train_1024', 'class_num': 15, 'model_name': 'efficientnet_b3', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 70, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3422\n",
      "Dataset size:855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 20:49:57,079 INFO: Epoch:[001/070]\n",
      "2022-04-12 20:49:57,080 INFO: Train Loss:1.979 | Acc:0.5073 | F1:0.4965\n",
      "2022-04-12 20:50:08,427 INFO: val Loss:0.378 | Acc:0.9310 | F1:0.9143\n",
      "2022-04-12 20:50:11,134 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.16it/s]\n",
      "2022-04-12 20:51:18,929 INFO: Epoch:[002/070]\n",
      "2022-04-12 20:51:18,930 INFO: Train Loss:0.226 | Acc:0.9439 | F1:0.9428\n",
      "2022-04-12 20:51:30,437 INFO: val Loss:0.029 | Acc:0.9977 | F1:0.9979\n",
      "2022-04-12 20:51:33,165 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.19it/s]\n",
      "2022-04-12 20:52:40,170 INFO: Epoch:[003/070]\n",
      "2022-04-12 20:52:40,171 INFO: Train Loss:0.082 | Acc:0.9790 | F1:0.9771\n",
      "2022-04-12 20:52:51,252 INFO: val Loss:0.022 | Acc:0.9930 | F1:0.9930\n",
      "2022-04-12 20:52:53,982 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.16it/s]\n",
      "2022-04-12 20:54:01,669 INFO: Epoch:[004/070]\n",
      "2022-04-12 20:54:01,670 INFO: Train Loss:0.104 | Acc:0.9787 | F1:0.9749\n",
      "2022-04-12 20:54:13,040 INFO: val Loss:0.051 | Acc:0.9825 | F1:0.9825\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 20:55:20,451 INFO: Epoch:[005/070]\n",
      "2022-04-12 20:55:20,451 INFO: Train Loss:0.062 | Acc:0.9854 | F1:0.9853\n",
      "2022-04-12 20:55:32,055 INFO: val Loss:0.044 | Acc:0.9825 | F1:0.9775\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 20:56:39,271 INFO: Epoch:[006/070]\n",
      "2022-04-12 20:56:39,271 INFO: Train Loss:0.034 | Acc:0.9918 | F1:0.9909\n",
      "2022-04-12 20:56:50,616 INFO: val Loss:0.002 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:56:53,357 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.15it/s]\n",
      "2022-04-12 20:58:01,285 INFO: Epoch:[007/070]\n",
      "2022-04-12 20:58:01,285 INFO: Train Loss:0.013 | Acc:0.9962 | F1:0.9962\n",
      "2022-04-12 20:58:12,695 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:58:15,383 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.21it/s]\n",
      "2022-04-12 20:59:21,992 INFO: Epoch:[008/070]\n",
      "2022-04-12 20:59:21,992 INFO: Train Loss:0.009 | Acc:0.9977 | F1:0.9976\n",
      "2022-04-12 20:59:33,017 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 20:59:35,746 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.25it/s]\n",
      "2022-04-12 21:00:41,597 INFO: Epoch:[009/070]\n",
      "2022-04-12 21:00:41,598 INFO: Train Loss:0.010 | Acc:0.9982 | F1:0.9983\n",
      "2022-04-12 21:00:52,826 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:00:55,619 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 214/214 [01:04<00:00,  3.31it/s]\n",
      "2022-04-12 21:02:00,267 INFO: Epoch:[010/070]\n",
      "2022-04-12 21:02:00,267 INFO: Train Loss:0.017 | Acc:0.9962 | F1:0.9963\n",
      "2022-04-12 21:02:11,853 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.22it/s]\n",
      "2022-04-12 21:03:18,325 INFO: Epoch:[011/070]\n",
      "2022-04-12 21:03:18,326 INFO: Train Loss:0.042 | Acc:0.9933 | F1:0.9913\n",
      "2022-04-12 21:03:29,883 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.17it/s]\n",
      "2022-04-12 21:04:37,404 INFO: Epoch:[012/070]\n",
      "2022-04-12 21:04:37,405 INFO: Train Loss:0.039 | Acc:0.9927 | F1:0.9918\n",
      "2022-04-12 21:04:48,860 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 21:05:55,660 INFO: Epoch:[013/070]\n",
      "2022-04-12 21:05:55,661 INFO: Train Loss:0.024 | Acc:0.9947 | F1:0.9948\n",
      "2022-04-12 21:06:07,072 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 21:07:13,937 INFO: Epoch:[014/070]\n",
      "2022-04-12 21:07:13,938 INFO: Train Loss:0.025 | Acc:0.9936 | F1:0.9901\n",
      "2022-04-12 21:07:25,126 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:07:25,127 INFO: \n",
      "Best Val Epoch:9 | Val Loss:0.0000 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2022-04-12 21:07:25,128 INFO: Total Process time:18.589Minute\n",
      "2022-04-12 21:07:25,132 INFO: {'exp_num': '4', 'data_path': './open', 'Kfold': 5, 'model_path': 'class_results/', 'image_type': 'train_1024', 'class_num': 15, 'model_name': 'efficientnet_b3', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 70, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3422\n",
      "Dataset size:855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:06<00:00,  3.22it/s]\n",
      "2022-04-12 21:08:32,566 INFO: Epoch:[001/070]\n",
      "2022-04-12 21:08:32,566 INFO: Train Loss:2.005 | Acc:0.4883 | F1:0.4779\n",
      "2022-04-12 21:08:44,181 INFO: val Loss:0.491 | Acc:0.8503 | F1:0.8069\n",
      "2022-04-12 21:08:46,971 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.19it/s]\n",
      "2022-04-12 21:09:54,090 INFO: Epoch:[002/070]\n",
      "2022-04-12 21:09:54,091 INFO: Train Loss:0.233 | Acc:0.9506 | F1:0.9484\n",
      "2022-04-12 21:10:05,543 INFO: val Loss:0.069 | Acc:0.9673 | F1:0.9673\n",
      "2022-04-12 21:10:08,270 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.22it/s]\n",
      "2022-04-12 21:11:14,735 INFO: Epoch:[003/070]\n",
      "2022-04-12 21:11:14,736 INFO: Train Loss:0.065 | Acc:0.9845 | F1:0.9837\n",
      "2022-04-12 21:11:26,135 INFO: val Loss:0.026 | Acc:0.9988 | F1:0.9974\n",
      "2022-04-12 21:11:28,826 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 21:12:36,100 INFO: Epoch:[004/070]\n",
      "2022-04-12 21:12:36,101 INFO: Train Loss:0.068 | Acc:0.9807 | F1:0.9784\n",
      "2022-04-12 21:12:47,364 INFO: val Loss:0.064 | Acc:0.9801 | F1:0.9784\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.21it/s]\n",
      "2022-04-12 21:13:53,977 INFO: Epoch:[005/070]\n",
      "2022-04-12 21:13:53,978 INFO: Train Loss:0.115 | Acc:0.9778 | F1:0.9744\n",
      "2022-04-12 21:14:05,489 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:14:08,190 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.21it/s]\n",
      "2022-04-12 21:15:14,846 INFO: Epoch:[006/070]\n",
      "2022-04-12 21:15:14,846 INFO: Train Loss:0.032 | Acc:0.9924 | F1:0.9912\n",
      "2022-04-12 21:15:26,376 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:15:28,721 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.17it/s]\n",
      "2022-04-12 21:16:36,284 INFO: Epoch:[007/070]\n",
      "2022-04-12 21:16:36,285 INFO: Train Loss:0.003 | Acc:0.9991 | F1:0.9991\n",
      "2022-04-12 21:16:47,520 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:16:50,327 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:03<00:00,  3.35it/s]\n",
      "2022-04-12 21:17:54,229 INFO: Epoch:[008/070]\n",
      "2022-04-12 21:17:54,229 INFO: Train Loss:0.010 | Acc:0.9982 | F1:0.9983\n",
      "2022-04-12 21:18:05,626 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:18:08,448 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:05<00:00,  3.28it/s]\n",
      "2022-04-12 21:19:13,729 INFO: Epoch:[009/070]\n",
      "2022-04-12 21:19:13,730 INFO: Train Loss:0.007 | Acc:0.9982 | F1:0.9983\n",
      "2022-04-12 21:19:25,349 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.22it/s]\n",
      "2022-04-12 21:20:31,804 INFO: Epoch:[010/070]\n",
      "2022-04-12 21:20:31,805 INFO: Train Loss:0.020 | Acc:0.9965 | F1:0.9961\n",
      "2022-04-12 21:20:43,003 INFO: val Loss:0.000 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.17it/s]\n",
      "2022-04-12 21:21:50,564 INFO: Epoch:[011/070]\n",
      "2022-04-12 21:21:50,565 INFO: Train Loss:0.012 | Acc:0.9971 | F1:0.9971\n",
      "2022-04-12 21:22:02,027 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "100%|██████████| 214/214 [01:06<00:00,  3.20it/s]\n",
      "2022-04-12 21:23:08,984 INFO: Epoch:[012/070]\n",
      "2022-04-12 21:23:08,985 INFO: Train Loss:0.038 | Acc:0.9947 | F1:0.9944\n",
      "2022-04-12 21:23:20,556 INFO: val Loss:0.005 | Acc:0.9988 | F1:0.9987\n",
      "100%|██████████| 214/214 [01:07<00:00,  3.18it/s]\n",
      "2022-04-12 21:24:27,879 INFO: Epoch:[013/070]\n",
      "2022-04-12 21:24:27,879 INFO: Train Loss:0.036 | Acc:0.9930 | F1:0.9911\n",
      "2022-04-12 21:24:39,370 INFO: val Loss:0.001 | Acc:1.0000 | F1:1.0000\n",
      "2022-04-12 21:24:39,371 INFO: \n",
      "Best Val Epoch:8 | Val Loss:0.0000 | Val Acc:1.0000 | Val F1:1.0000\n",
      "2022-04-12 21:24:39,371 INFO: Total Process time:17.221Minute\n"
     ]
    }
   ],
   "source": [
    "args.step = 0\n",
    "models_path = []\n",
    "for s_fold in range(5): # 5fold\n",
    "    args.fold = s_fold\n",
    "    args.exp_num = str(s_fold)\n",
    "    save_path = main(args)\n",
    "    models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size:2154\n"
     ]
    }
   ],
   "source": [
    "img_size = 224\n",
    "\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_path = ['./class_results/000', './class_results/001', './class_results/002', './class_results/003', './class_results/004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_040', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [01:16<00:00,  2.25s/it]\n",
      "100%|██████████| 34/34 [01:14<00:00,  2.20s/it]\n",
      "100%|██████████| 34/34 [01:14<00:00,  2.20s/it]\n",
      "100%|██████████| 34/34 [01:14<00:00,  2.19s/it]\n",
      "100%|██████████| 34/34 [01:14<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble = ensemble_5fold(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 4,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 13,\n",
       " 14,\n",
       " 9,\n",
       " 10,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 10,\n",
       " 14,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 13,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 10,\n",
       " 11,\n",
       " 2,\n",
       " 14,\n",
       " 9,\n",
       " 1,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 14,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 10,\n",
       " 6,\n",
       " 14,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 13,\n",
       " 9,\n",
       " 13,\n",
       " 12,\n",
       " 0,\n",
       " 14,\n",
       " 3,\n",
       " 6,\n",
       " 14,\n",
       " 6,\n",
       " 14,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 0,\n",
       " 14,\n",
       " 8,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 13,\n",
       " 12,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 14,\n",
       " 5,\n",
       " 3,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 14,\n",
       " 10,\n",
       " 11,\n",
       " 10,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 14,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 14,\n",
       " 2,\n",
       " 14,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 3,\n",
       " 10,\n",
       " 7,\n",
       " 13,\n",
       " 13,\n",
       " 8,\n",
       " 2,\n",
       " 14,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 5,\n",
       " 12,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 13,\n",
       " 8,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 12,\n",
       " 0,\n",
       " 10,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 9,\n",
       " 14,\n",
       " 8,\n",
       " 14,\n",
       " 2,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 14,\n",
       " 9,\n",
       " 13,\n",
       " 6,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 1,\n",
       " 12,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 14,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 13,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 13,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 12,\n",
       " 14,\n",
       " 8,\n",
       " 6,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 10,\n",
       " 10,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 14,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 14,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 0,\n",
       " 7,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 13,\n",
       " 13,\n",
       " 10,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 13,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 11,\n",
       " 12,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 12,\n",
       " 14,\n",
       " 13,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 12,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 13,\n",
       " 10,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 10,\n",
       " 13,\n",
       " 10,\n",
       " 12,\n",
       " 14,\n",
       " 0,\n",
       " 10,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 12,\n",
       " 6,\n",
       " 12,\n",
       " 5,\n",
       " 12,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 6,\n",
       " 12,\n",
       " 0,\n",
       " 14,\n",
       " 1,\n",
       " 0,\n",
       " 10,\n",
       " 5,\n",
       " 13,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 10,\n",
       " 12,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 3,\n",
       " 14,\n",
       " 10,\n",
       " 6,\n",
       " 13,\n",
       " 14,\n",
       " 0,\n",
       " 3,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 12,\n",
       " 11,\n",
       " 1,\n",
       " 14,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 12,\n",
       " 1,\n",
       " 10,\n",
       " 6,\n",
       " 13,\n",
       " 11,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 10,\n",
       " 8,\n",
       " 13,\n",
       " 7,\n",
       " 8,\n",
       " 14,\n",
       " 1,\n",
       " 13,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 14,\n",
       " 13,\n",
       " 8,\n",
       " 11,\n",
       " 1,\n",
       " 13,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 10,\n",
       " 9,\n",
       " 2,\n",
       " 14,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 13,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 13,\n",
       " 14,\n",
       " 3,\n",
       " 4,\n",
       " 14,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 3,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 3,\n",
       " 10,\n",
       " 14,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 14,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 14,\n",
       " 10,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 11,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 14,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 12,\n",
       " 0,\n",
       " 3,\n",
       " 14,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 14,\n",
       " 13,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 1,\n",
       " 3,\n",
       " 12,\n",
       " 2,\n",
       " 4,\n",
       " 12,\n",
       " 5,\n",
       " 10,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 13,\n",
       " 8,\n",
       " 1,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 8,\n",
       " 1,\n",
       " 10,\n",
       " 10,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 14,\n",
       " 10,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 12,\n",
       " 1,\n",
       " 13,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 13,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 14,\n",
       " 2,\n",
       " 12,\n",
       " 9,\n",
       " 12,\n",
       " 14,\n",
       " 12,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 14,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 8,\n",
       " 13,\n",
       " 3,\n",
       " 13,\n",
       " 14,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 12,\n",
       " 6,\n",
       " 9,\n",
       " 13,\n",
       " 1,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 1,\n",
       " 14,\n",
       " 3,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 5,\n",
       " 12,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 14,\n",
       " 5,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 14,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 0,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 12,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 14,\n",
       " 10,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 6,\n",
       " 0,\n",
       " 10,\n",
       " 2,\n",
       " 10,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 14,\n",
       " 5,\n",
       " 12,\n",
       " 14,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 12,\n",
       " 11,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 12,\n",
       " 14,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 12,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 14,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 14,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 5,\n",
       " 9,\n",
       " 10,\n",
       " 14,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 11,\n",
       " 10,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 10,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 14,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 10,\n",
       " 4,\n",
       " 9,\n",
       " 3,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 13,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 10,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pred = ensemble.argmax(axis=1).tolist()\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./open/train_df.csv\")\n",
    "\n",
    "train_labels = train_y[\"class\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tile',\n",
       " 'grid',\n",
       " 'transistor',\n",
       " 'tile',\n",
       " 'tile',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'hazelnut',\n",
       " 'leather',\n",
       " 'leather',\n",
       " 'bottle',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'tile',\n",
       " 'bottle',\n",
       " 'grid',\n",
       " 'carpet',\n",
       " 'carpet',\n",
       " 'pill',\n",
       " 'tile',\n",
       " 'zipper',\n",
       " 'bottle',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'metal_nut',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'toothbrush',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'transistor',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'zipper',\n",
       " 'metal_nut',\n",
       " 'carpet',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'metal_nut',\n",
       " 'grid',\n",
       " 'wood',\n",
       " 'hazelnut',\n",
       " 'tile',\n",
       " 'leather',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'bottle',\n",
       " 'zipper',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'tile',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'metal_nut',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'wood',\n",
       " 'bottle',\n",
       " 'zipper',\n",
       " 'pill',\n",
       " 'transistor',\n",
       " 'hazelnut',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'carpet',\n",
       " 'metal_nut',\n",
       " 'transistor',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'tile',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'grid',\n",
       " 'metal_nut',\n",
       " 'pill',\n",
       " 'metal_nut',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'screw',\n",
       " 'bottle',\n",
       " 'pill',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'carpet',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'toothbrush',\n",
       " 'tile',\n",
       " 'hazelnut',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'pill',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'zipper',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'grid',\n",
       " 'bottle',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'carpet',\n",
       " 'tile',\n",
       " 'metal_nut',\n",
       " 'wood',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'transistor',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'bottle',\n",
       " 'pill',\n",
       " 'leather',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'hazelnut',\n",
       " 'grid',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'carpet',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'transistor',\n",
       " 'bottle',\n",
       " 'tile',\n",
       " 'grid',\n",
       " 'tile',\n",
       " 'bottle',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'pill',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'zipper',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'metal_nut',\n",
       " 'hazelnut',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'hazelnut',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'leather',\n",
       " 'tile',\n",
       " 'tile',\n",
       " 'wood',\n",
       " 'cable',\n",
       " 'transistor',\n",
       " 'tile',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'metal_nut',\n",
       " 'metal_nut',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'leather',\n",
       " 'leather',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'pill',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'grid',\n",
       " 'tile',\n",
       " 'tile',\n",
       " 'bottle',\n",
       " 'carpet',\n",
       " 'bottle',\n",
       " 'leather',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'zipper',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'toothbrush',\n",
       " 'grid',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'bottle',\n",
       " 'metal_nut',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'wood',\n",
       " 'tile',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'carpet',\n",
       " 'carpet',\n",
       " 'bottle',\n",
       " 'tile',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'metal_nut',\n",
       " 'metal_nut',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'carpet',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'carpet',\n",
       " 'capsule',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'bottle',\n",
       " 'toothbrush',\n",
       " 'transistor',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'carpet',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'wood',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'bottle',\n",
       " 'hazelnut',\n",
       " 'transistor',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'tile',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'metal_nut',\n",
       " 'capsule',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'metal_nut',\n",
       " 'bottle',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'cable',\n",
       " 'bottle',\n",
       " 'hazelnut',\n",
       " 'tile',\n",
       " 'wood',\n",
       " 'tile',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'bottle',\n",
       " 'tile',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'bottle',\n",
       " 'transistor',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'hazelnut',\n",
       " 'transistor',\n",
       " 'metal_nut',\n",
       " 'zipper',\n",
       " 'metal_nut',\n",
       " 'carpet',\n",
       " 'carpet',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'bottle',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'bottle',\n",
       " 'tile',\n",
       " 'hazelnut',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'carpet',\n",
       " 'grid',\n",
       " 'pill',\n",
       " 'tile',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'carpet',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'leather',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'bottle',\n",
       " 'carpet',\n",
       " 'tile',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'carpet',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'carpet',\n",
       " 'capsule',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'metal_nut',\n",
       " 'pill',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'bottle',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'wood',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'carpet',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'grid',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'carpet',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'toothbrush',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'tile',\n",
       " 'metal_nut',\n",
       " 'transistor',\n",
       " 'cable',\n",
       " 'tile',\n",
       " 'leather',\n",
       " 'wood',\n",
       " 'toothbrush',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'metal_nut',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'tile',\n",
       " 'pill',\n",
       " 'wood',\n",
       " 'metal_nut',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'wood',\n",
       " 'capsule',\n",
       " 'carpet',\n",
       " 'capsule',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'grid',\n",
       " 'screw',\n",
       " 'zipper',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'toothbrush',\n",
       " 'cable',\n",
       " 'wood',\n",
       " 'grid',\n",
       " 'screw',\n",
       " 'grid',\n",
       " 'zipper',\n",
       " 'transistor',\n",
       " 'tile',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'carpet',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'grid',\n",
       " 'grid',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'wood',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'carpet',\n",
       " 'grid',\n",
       " 'zipper',\n",
       " 'pill',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'carpet',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'transistor',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'zipper',\n",
       " 'metal_nut',\n",
       " 'carpet',\n",
       " 'tile',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'leather',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'metal_nut',\n",
       " 'grid',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'hazelnut',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'carpet',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'bottle',\n",
       " 'carpet',\n",
       " 'metal_nut',\n",
       " 'toothbrush',\n",
       " 'hazelnut',\n",
       " 'bottle',\n",
       " 'leather',\n",
       " 'toothbrush',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'pill',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'bottle',\n",
       " 'carpet',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'wood',\n",
       " 'hazelnut',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'grid',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'transistor',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'transistor',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'transistor',\n",
       " 'hazelnut',\n",
       " 'tile',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'bottle',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'bottle',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'tile',\n",
       " 'leather',\n",
       " 'leather',\n",
       " 'tile',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'tile',\n",
       " 'tile',\n",
       " 'hazelnut',\n",
       " 'grid',\n",
       " 'leather',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'grid',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'bottle',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'bottle',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'cable',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'grid',\n",
       " 'carpet',\n",
       " 'wood',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'hazelnut',\n",
       " 'zipper',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'hazelnut',\n",
       " 'pill',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'bottle',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'hazelnut',\n",
       " 'metal_nut',\n",
       " 'carpet',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'wood',\n",
       " 'hazelnut',\n",
       " 'carpet',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'tile',\n",
       " 'carpet',\n",
       " 'bottle',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'cable',\n",
       " 'metal_nut',\n",
       " 'transistor',\n",
       " 'capsule',\n",
       " 'tile',\n",
       " 'zipper',\n",
       " 'bottle',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'pill',\n",
       " 'bottle',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'zipper',\n",
       " 'metal_nut',\n",
       " 'pill',\n",
       " 'wood',\n",
       " 'carpet',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'bottle',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'toothbrush',\n",
       " 'toothbrush',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'carpet',\n",
       " 'toothbrush',\n",
       " 'toothbrush',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'tile',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'wood',\n",
       " 'pill',\n",
       " 'bottle',\n",
       " 'leather',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'toothbrush',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'bottle',\n",
       " 'tile',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'transistor',\n",
       " 'carpet',\n",
       " 'grid',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'tile',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'carpet',\n",
       " 'pill',\n",
       " 'hazelnut',\n",
       " 'bottle',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'tile',\n",
       " 'transistor',\n",
       " 'leather',\n",
       " 'bottle',\n",
       " 'tile',\n",
       " 'capsule',\n",
       " 'tile',\n",
       " 'wood',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'carpet',\n",
       " 'transistor',\n",
       " 'toothbrush',\n",
       " 'cable',\n",
       " 'zipper',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'pill',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'wood',\n",
       " 'leather',\n",
       " 'leather',\n",
       " 'hazelnut',\n",
       " 'grid',\n",
       " 'capsule',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'tile',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'leather',\n",
       " 'metal_nut',\n",
       " 'transistor',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'cable',\n",
       " 'carpet',\n",
       " 'transistor',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'screw',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'pill',\n",
       " 'zipper',\n",
       " 'capsule',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'tile',\n",
       " 'wood',\n",
       " 'hazelnut',\n",
       " 'screw',\n",
       " 'tile',\n",
       " 'zipper',\n",
       " 'metal_nut',\n",
       " 'leather',\n",
       " 'cable',\n",
       " 'toothbrush',\n",
       " 'cable',\n",
       " 'toothbrush',\n",
       " 'tile',\n",
       " 'cable',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'capsule',\n",
       " 'pill',\n",
       " 'tile',\n",
       " 'leather',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'carpet',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'bottle',\n",
       " 'leather',\n",
       " 'leather',\n",
       " 'capsule',\n",
       " 'zipper',\n",
       " 'hazelnut',\n",
       " 'capsule',\n",
       " 'metal_nut',\n",
       " 'tile',\n",
       " 'grid',\n",
       " 'screw',\n",
       " 'carpet',\n",
       " 'capsule',\n",
       " 'transistor',\n",
       " 'capsule',\n",
       " 'wood',\n",
       " 'zipper',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'hazelnut',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'screw',\n",
       " 'hazelnut',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'carpet',\n",
       " 'carpet',\n",
       " 'bottle',\n",
       " 'cable',\n",
       " 'hazelnut',\n",
       " 'tile',\n",
       " 'pill',\n",
       " 'leather',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'leather',\n",
       " 'carpet',\n",
       " 'screw',\n",
       " 'transistor',\n",
       " 'pill',\n",
       " 'capsule',\n",
       " 'grid',\n",
       " 'metal_nut',\n",
       " 'screw',\n",
       " 'screw',\n",
       " 'pill',\n",
       " 'screw',\n",
       " 'capsule',\n",
       " 'cable',\n",
       " 'cable',\n",
       " 'bottle',\n",
       " 'zipper',\n",
       " 'bottle',\n",
       " 'leather',\n",
       " 'screw',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       label\n",
       "0         0        tile\n",
       "1         1        grid\n",
       "2         2  transistor\n",
       "3         3        tile\n",
       "4         4        tile\n",
       "...     ...         ...\n",
       "2149   2149        tile\n",
       "2150   2150       screw\n",
       "2151   2151        grid\n",
       "2152   2152       cable\n",
       "2153   2153      zipper\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"class_result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
