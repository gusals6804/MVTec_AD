{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file_name       class state            label  label2  class2  state2\n",
      "0  10000.png  transistor  good  transistor-good      72      12      25\n",
      "1  10001.png     capsule  good     capsule-good      15       2      25\n",
      "2  10002.png  transistor  good  transistor-good      72      12      25\n",
      "3  10003.png        wood  good        wood-good      76      13      25\n",
      "4  10004.png      bottle  good      bottle-good       3       0      25\n",
      "   index  file_name\n",
      "0      0  20000.png\n",
      "1      1  20001.png\n",
      "2      2  20002.png\n",
      "3      3  20003.png\n",
      "4      4  20004.png\n",
      "(4277, 7)\n",
      "(2154, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './open'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df2.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_y = pd.read_csv(\"./open/train_df.csv\")\n",
    "\n",
    "# train_labels = train_y[\"label\"]\n",
    "\n",
    "# label_unique = sorted(np.unique(train_labels))\n",
    "# label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "# train_labels = [label_unique[k] for k in train_labels]\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['label2'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./open/train_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(train_df.label2.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./open',\n",
    "     'Kfold':5,\n",
    "     'model_path':'label_results/',\n",
    "     'image_type':'train_1024', \n",
    "     'class_num' : class_num,\n",
    "\n",
    "     # Model parameter settings\n",
    "     'model_name':'regnety_040',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':288,\n",
    "     'batch_size':16,\n",
    "     'epochs':100,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':1e-5,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':3,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-5,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience': 7,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':True,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "     \n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return x + self.conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def coatnet_0():\n",
    "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_1():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_2():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [128, 128, 256, 512, 1026]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_3():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_4():\n",
    "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, p: float, degrees: int):\n",
    "        super(RandomRotation, self).__init__(degrees)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            fill = self.fill\n",
    "            if isinstance(img, Tensor):\n",
    "                if isinstance(fill, (int, float)):\n",
    "                    fill = [float(fill)] * F.get_image_num_channels(img)\n",
    "                else:\n",
    "                    fill = [float(f) for f in fill]\n",
    "            angle = self.get_params(self.degrees)\n",
    "\n",
    "            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.target = df['label2'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(opj('./open/train/', self.img_path[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "#         target = self.target[idx]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "        \n",
    "        image = Image.open(opj('./open/train/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "#         augmentation = random.randint(0,2)\n",
    "#             if augmentation==1:\n",
    "#                 img = img[::-1].copy()\n",
    "#             elif augmentation==2:\n",
    "#                 img = img[:,::-1].copy()\n",
    "#         img = transforms.ToTensor()(img)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "#         image = cv2.imread(opj('./open/train/', self.img_path[idx])).astype(np.float32)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "#         target = self.target[idx]\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.3),\n",
    "                transforms.RandomVerticalFlip(p=0.3),\n",
    "#                 transforms.RandomAffine((-20, 20)),\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "            args.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "        )\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "            args.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "        )\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted crossentropy loss를 위한 weight 계산 함수\n",
    "def get_class_weight():\n",
    "    return 1 / train_df['label2'].value_counts().sort_index().values\n",
    "\n",
    "class_weight = get_class_weight()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train_df2.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "#         if args.image_type is not None:\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label2'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.criterion = CutMixCrossEntropyLoss(True)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "#         self.optimizer = Lookahead(torch.optim.Adam(self.model.parameters(), lr=args.initial_lr), k=5, alpha=0.5)\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "#                 # Gradient Clipping\n",
    "#                 if args.clipping is not None:\n",
    "#                     scaler.unscale_(self.optimizer)\n",
    "#                     torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 2 and 3 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./open/sample_submission.csv')\n",
    "df_train = pd.read_csv('./open/train_df2.csv')\n",
    "df_test = pd.read_csv('./open/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 17:22:35,501 INFO: {'exp_num': '0', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 1e-05, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3421\n",
      "Dataset size:856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 17:22:35,834 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 214/214 [01:13<00:00,  2.93it/s]\n",
      "2022-04-27 17:23:52,786 INFO: Epoch:[001/100]\n",
      "2022-04-27 17:23:52,787 INFO: Train Loss:4.498 | Acc:0.0064 | F1:0.0033\n",
      "2022-04-27 17:24:10,043 INFO: val Loss:4.450 | Acc:0.0257 | F1:0.0024\n",
      "2022-04-27 17:24:10,507 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:08<00:00,  3.13it/s]\n",
      "2022-04-27 17:25:18,868 INFO: Epoch:[002/100]\n",
      "2022-04-27 17:25:18,869 INFO: Train Loss:4.470 | Acc:0.0082 | F1:0.0022\n",
      "2022-04-27 17:25:35,898 INFO: val Loss:4.401 | Acc:0.0339 | F1:0.0040\n",
      "2022-04-27 17:25:36,523 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:08<00:00,  3.14it/s]\n",
      "2022-04-27 17:26:44,590 INFO: Epoch:[003/100]\n",
      "2022-04-27 17:26:44,591 INFO: Train Loss:4.407 | Acc:0.0251 | F1:0.0068\n",
      "2022-04-27 17:27:01,421 INFO: val Loss:4.323 | Acc:0.0561 | F1:0.0051\n",
      "2022-04-27 17:27:02,072 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:27<00:00,  2.45it/s]\n",
      "2022-04-27 17:28:29,452 INFO: Epoch:[004/100]\n",
      "2022-04-27 17:28:29,453 INFO: Train Loss:4.208 | Acc:0.1482 | F1:0.0324\n",
      "2022-04-27 17:28:48,231 INFO: val Loss:3.897 | Acc:0.4614 | F1:0.0820\n",
      "2022-04-27 17:28:48,897 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.30it/s]\n",
      "2022-04-27 17:30:22,128 INFO: Epoch:[005/100]\n",
      "2022-04-27 17:30:22,129 INFO: Train Loss:3.824 | Acc:0.4715 | F1:0.0876\n",
      "2022-04-27 17:30:40,861 INFO: val Loss:3.435 | Acc:0.7722 | F1:0.1398\n",
      "2022-04-27 17:30:41,533 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 17:32:11,224 INFO: Epoch:[006/100]\n",
      "2022-04-27 17:32:11,225 INFO: Train Loss:3.424 | Acc:0.6545 | F1:0.1157\n",
      "2022-04-27 17:32:30,662 INFO: val Loss:2.928 | Acc:0.8189 | F1:0.1419\n",
      "2022-04-27 17:32:31,368 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.33it/s]\n",
      "2022-04-27 17:34:03,380 INFO: Epoch:[007/100]\n",
      "2022-04-27 17:34:03,381 INFO: Train Loss:2.990 | Acc:0.7331 | F1:0.1277\n",
      "2022-04-27 17:34:22,110 INFO: val Loss:2.383 | Acc:0.8341 | F1:0.1448\n",
      "2022-04-27 17:34:22,822 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 17:35:53,775 INFO: Epoch:[008/100]\n",
      "2022-04-27 17:35:53,776 INFO: Train Loss:2.520 | Acc:0.7700 | F1:0.1336\n",
      "2022-04-27 17:36:13,731 INFO: val Loss:1.884 | Acc:0.8341 | F1:0.1448\n",
      "2022-04-27 17:36:14,458 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 17:37:43,956 INFO: Epoch:[009/100]\n",
      "2022-04-27 17:37:43,957 INFO: Train Loss:2.142 | Acc:0.7872 | F1:0.1368\n",
      "2022-04-27 17:38:03,228 INFO: val Loss:1.420 | Acc:0.8341 | F1:0.1448\n",
      "2022-04-27 17:38:03,907 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 17:39:33,623 INFO: Epoch:[010/100]\n",
      "2022-04-27 17:39:33,623 INFO: Train Loss:1.857 | Acc:0.8044 | F1:0.1405\n",
      "2022-04-27 17:39:52,768 INFO: val Loss:1.302 | Acc:0.8446 | F1:0.1542\n",
      "2022-04-27 17:39:53,478 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 17:41:23,579 INFO: Epoch:[011/100]\n",
      "2022-04-27 17:41:23,580 INFO: Train Loss:1.667 | Acc:0.8144 | F1:0.1458\n",
      "2022-04-27 17:41:43,124 INFO: val Loss:1.100 | Acc:0.8481 | F1:0.1560\n",
      "2022-04-27 17:41:43,827 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 17:43:13,607 INFO: Epoch:[012/100]\n",
      "2022-04-27 17:43:13,607 INFO: Train Loss:1.569 | Acc:0.8261 | F1:0.1510\n",
      "2022-04-27 17:43:33,146 INFO: val Loss:0.968 | Acc:0.8481 | F1:0.1560\n",
      "2022-04-27 17:43:33,923 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 17:45:03,773 INFO: Epoch:[013/100]\n",
      "2022-04-27 17:45:03,774 INFO: Train Loss:1.448 | Acc:0.8357 | F1:0.1536\n",
      "2022-04-27 17:45:22,893 INFO: val Loss:0.896 | Acc:0.8481 | F1:0.1560\n",
      "2022-04-27 17:45:23,575 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.28it/s]\n",
      "2022-04-27 17:46:57,323 INFO: Epoch:[014/100]\n",
      "2022-04-27 17:46:57,324 INFO: Train Loss:1.359 | Acc:0.8381 | F1:0.1540\n",
      "2022-04-27 17:47:16,370 INFO: val Loss:0.822 | Acc:0.8481 | F1:0.1560\n",
      "2022-04-27 17:47:17,108 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 17:48:49,317 INFO: Epoch:[015/100]\n",
      "2022-04-27 17:48:49,318 INFO: Train Loss:1.222 | Acc:0.8398 | F1:0.1547\n",
      "2022-04-27 17:49:08,098 INFO: val Loss:0.743 | Acc:0.8481 | F1:0.1560\n",
      "2022-04-27 17:49:08,787 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 17:50:38,273 INFO: Epoch:[016/100]\n",
      "2022-04-27 17:50:38,274 INFO: Train Loss:1.119 | Acc:0.8389 | F1:0.1545\n",
      "2022-04-27 17:50:57,571 INFO: val Loss:0.667 | Acc:0.8481 | F1:0.1560\n",
      "2022-04-27 17:50:58,262 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 17:52:29,079 INFO: Epoch:[017/100]\n",
      "2022-04-27 17:52:29,079 INFO: Train Loss:1.022 | Acc:0.8404 | F1:0.1543\n",
      "2022-04-27 17:52:48,198 INFO: val Loss:0.679 | Acc:0.8505 | F1:0.1676\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 17:54:18,554 INFO: Epoch:[018/100]\n",
      "2022-04-27 17:54:18,555 INFO: Train Loss:0.907 | Acc:0.8445 | F1:0.1644\n",
      "2022-04-27 17:54:37,661 INFO: val Loss:0.603 | Acc:0.8411 | F1:0.1690\n",
      "2022-04-27 17:54:38,347 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 17:56:09,337 INFO: Epoch:[019/100]\n",
      "2022-04-27 17:56:09,338 INFO: Train Loss:0.846 | Acc:0.8436 | F1:0.1746\n",
      "2022-04-27 17:56:28,864 INFO: val Loss:0.533 | Acc:0.8481 | F1:0.1909\n",
      "2022-04-27 17:56:29,702 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 17:58:00,839 INFO: Epoch:[020/100]\n",
      "2022-04-27 17:58:00,839 INFO: Train Loss:0.761 | Acc:0.8503 | F1:0.1889\n",
      "2022-04-27 17:58:20,338 INFO: val Loss:0.488 | Acc:0.8668 | F1:0.2260\n",
      "2022-04-27 17:58:21,070 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 214/214 [01:19<00:00,  2.70it/s]\n",
      "2022-04-27 17:59:40,481 INFO: Epoch:[021/100]\n",
      "2022-04-27 17:59:40,482 INFO: Train Loss:0.691 | Acc:0.8550 | F1:0.2066\n",
      "2022-04-27 17:59:59,773 INFO: val Loss:0.484 | Acc:0.8750 | F1:0.2607\n",
      "2022-04-27 18:00:00,613 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 18:01:31,350 INFO: Epoch:[022/100]\n",
      "2022-04-27 18:01:31,351 INFO: Train Loss:0.657 | Acc:0.8562 | F1:0.2237\n",
      "2022-04-27 18:01:50,519 INFO: val Loss:0.449 | Acc:0.8750 | F1:0.2593\n",
      "2022-04-27 18:01:51,191 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 18:03:20,680 INFO: Epoch:[023/100]\n",
      "2022-04-27 18:03:20,680 INFO: Train Loss:0.627 | Acc:0.8574 | F1:0.2255\n",
      "2022-04-27 18:03:40,071 INFO: val Loss:0.395 | Acc:0.8843 | F1:0.3012\n",
      "2022-04-27 18:03:40,764 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 214/214 [01:27<00:00,  2.45it/s]\n",
      "2022-04-27 18:05:08,225 INFO: Epoch:[024/100]\n",
      "2022-04-27 18:05:08,225 INFO: Train Loss:0.589 | Acc:0.8606 | F1:0.2506\n",
      "2022-04-27 18:05:27,592 INFO: val Loss:0.404 | Acc:0.8773 | F1:0.2779\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 18:06:57,809 INFO: Epoch:[025/100]\n",
      "2022-04-27 18:06:57,809 INFO: Train Loss:0.544 | Acc:0.8661 | F1:0.2608\n",
      "2022-04-27 18:07:17,059 INFO: val Loss:0.395 | Acc:0.8808 | F1:0.2752\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 18:08:46,989 INFO: Epoch:[026/100]\n",
      "2022-04-27 18:08:46,990 INFO: Train Loss:0.537 | Acc:0.8658 | F1:0.2748\n",
      "2022-04-27 18:09:06,127 INFO: val Loss:0.406 | Acc:0.8750 | F1:0.2683\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 18:10:36,843 INFO: Epoch:[027/100]\n",
      "2022-04-27 18:10:36,844 INFO: Train Loss:0.512 | Acc:0.8670 | F1:0.3048\n",
      "2022-04-27 18:10:56,004 INFO: val Loss:0.455 | Acc:0.8516 | F1:0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 18:12:26,349 INFO: Epoch:[028/100]\n",
      "2022-04-27 18:12:26,350 INFO: Train Loss:0.486 | Acc:0.8720 | F1:0.3123\n",
      "2022-04-27 18:12:45,817 INFO: val Loss:0.361 | Acc:0.8879 | F1:0.3308\n",
      "2022-04-27 18:12:46,540 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 18:14:17,876 INFO: Epoch:[029/100]\n",
      "2022-04-27 18:14:17,877 INFO: Train Loss:0.464 | Acc:0.8690 | F1:0.3087\n",
      "2022-04-27 18:14:36,686 INFO: val Loss:0.389 | Acc:0.8867 | F1:0.3165\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.30it/s]\n",
      "2022-04-27 18:16:09,693 INFO: Epoch:[030/100]\n",
      "2022-04-27 18:16:09,693 INFO: Train Loss:0.450 | Acc:0.8772 | F1:0.3498\n",
      "2022-04-27 18:16:28,747 INFO: val Loss:0.349 | Acc:0.8995 | F1:0.3972\n",
      "2022-04-27 18:16:29,441 INFO: -----------------SAVE:30epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 18:17:58,789 INFO: Epoch:[031/100]\n",
      "2022-04-27 18:17:58,790 INFO: Train Loss:0.431 | Acc:0.8734 | F1:0.3385\n",
      "2022-04-27 18:18:17,770 INFO: val Loss:0.358 | Acc:0.8879 | F1:0.4049\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 18:19:47,263 INFO: Epoch:[032/100]\n",
      "2022-04-27 18:19:47,264 INFO: Train Loss:0.418 | Acc:0.8804 | F1:0.3775\n",
      "2022-04-27 18:20:06,454 INFO: val Loss:0.335 | Acc:0.8995 | F1:0.4257\n",
      "2022-04-27 18:20:07,125 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 18:21:36,115 INFO: Epoch:[033/100]\n",
      "2022-04-27 18:21:36,116 INFO: Train Loss:0.390 | Acc:0.8854 | F1:0.3928\n",
      "2022-04-27 18:21:55,278 INFO: val Loss:0.310 | Acc:0.9077 | F1:0.4447\n",
      "2022-04-27 18:21:56,003 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 18:23:25,667 INFO: Epoch:[034/100]\n",
      "2022-04-27 18:23:25,667 INFO: Train Loss:0.388 | Acc:0.8916 | F1:0.4293\n",
      "2022-04-27 18:23:44,617 INFO: val Loss:0.302 | Acc:0.9030 | F1:0.4369\n",
      "2022-04-27 18:23:45,290 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 18:25:14,557 INFO: Epoch:[035/100]\n",
      "2022-04-27 18:25:14,558 INFO: Train Loss:0.363 | Acc:0.8956 | F1:0.4349\n",
      "2022-04-27 18:25:33,855 INFO: val Loss:0.279 | Acc:0.9124 | F1:0.5055\n",
      "2022-04-27 18:25:34,568 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 18:27:04,300 INFO: Epoch:[036/100]\n",
      "2022-04-27 18:27:04,301 INFO: Train Loss:0.340 | Acc:0.9003 | F1:0.4779\n",
      "2022-04-27 18:27:23,944 INFO: val Loss:0.276 | Acc:0.9159 | F1:0.5114\n",
      "2022-04-27 18:27:24,632 INFO: -----------------SAVE:36epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 18:28:53,277 INFO: Epoch:[037/100]\n",
      "2022-04-27 18:28:53,278 INFO: Train Loss:0.329 | Acc:0.9053 | F1:0.5077\n",
      "2022-04-27 18:29:12,302 INFO: val Loss:0.288 | Acc:0.9147 | F1:0.4944\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 18:30:43,849 INFO: Epoch:[038/100]\n",
      "2022-04-27 18:30:43,850 INFO: Train Loss:0.319 | Acc:0.9065 | F1:0.5129\n",
      "2022-04-27 18:31:03,195 INFO: val Loss:0.266 | Acc:0.9241 | F1:0.5759\n",
      "2022-04-27 18:31:03,871 INFO: -----------------SAVE:38epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.30it/s]\n",
      "2022-04-27 18:32:36,857 INFO: Epoch:[039/100]\n",
      "2022-04-27 18:32:36,858 INFO: Train Loss:0.298 | Acc:0.9117 | F1:0.5433\n",
      "2022-04-27 18:32:56,154 INFO: val Loss:0.280 | Acc:0.9136 | F1:0.5160\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 18:34:28,433 INFO: Epoch:[040/100]\n",
      "2022-04-27 18:34:28,434 INFO: Train Loss:0.300 | Acc:0.9129 | F1:0.5614\n",
      "2022-04-27 18:34:47,917 INFO: val Loss:0.281 | Acc:0.9171 | F1:0.5333\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 18:36:18,561 INFO: Epoch:[041/100]\n",
      "2022-04-27 18:36:18,562 INFO: Train Loss:0.288 | Acc:0.9149 | F1:0.5621\n",
      "2022-04-27 18:36:37,709 INFO: val Loss:0.230 | Acc:0.9311 | F1:0.6146\n",
      "2022-04-27 18:36:38,395 INFO: -----------------SAVE:41epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 18:38:07,214 INFO: Epoch:[042/100]\n",
      "2022-04-27 18:38:07,215 INFO: Train Loss:0.273 | Acc:0.9182 | F1:0.5939\n",
      "2022-04-27 18:38:26,659 INFO: val Loss:0.236 | Acc:0.9252 | F1:0.5912\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 18:39:57,083 INFO: Epoch:[043/100]\n",
      "2022-04-27 18:39:57,084 INFO: Train Loss:0.264 | Acc:0.9217 | F1:0.6157\n",
      "2022-04-27 18:40:16,788 INFO: val Loss:0.263 | Acc:0.9276 | F1:0.5799\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 18:41:48,098 INFO: Epoch:[044/100]\n",
      "2022-04-27 18:41:48,098 INFO: Train Loss:0.255 | Acc:0.9228 | F1:0.6013\n",
      "2022-04-27 18:42:07,550 INFO: val Loss:0.250 | Acc:0.9287 | F1:0.5919\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 18:43:38,535 INFO: Epoch:[045/100]\n",
      "2022-04-27 18:43:38,536 INFO: Train Loss:0.243 | Acc:0.9243 | F1:0.6065\n",
      "2022-04-27 18:43:57,801 INFO: val Loss:0.251 | Acc:0.9229 | F1:0.6066\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 18:45:30,288 INFO: Epoch:[046/100]\n",
      "2022-04-27 18:45:30,289 INFO: Train Loss:0.245 | Acc:0.9298 | F1:0.6508\n",
      "2022-04-27 18:45:49,625 INFO: val Loss:0.262 | Acc:0.9346 | F1:0.6456\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 18:47:22,054 INFO: Epoch:[047/100]\n",
      "2022-04-27 18:47:22,055 INFO: Train Loss:0.235 | Acc:0.9301 | F1:0.6417\n",
      "2022-04-27 18:47:40,603 INFO: val Loss:0.210 | Acc:0.9381 | F1:0.6560\n",
      "2022-04-27 18:47:41,323 INFO: -----------------SAVE:47epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 18:49:13,687 INFO: Epoch:[048/100]\n",
      "2022-04-27 18:49:13,688 INFO: Train Loss:0.233 | Acc:0.9290 | F1:0.6456\n",
      "2022-04-27 18:49:33,092 INFO: val Loss:0.223 | Acc:0.9428 | F1:0.6674\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 18:51:04,583 INFO: Epoch:[049/100]\n",
      "2022-04-27 18:51:04,583 INFO: Train Loss:0.201 | Acc:0.9430 | F1:0.7153\n",
      "2022-04-27 18:51:24,147 INFO: val Loss:0.225 | Acc:0.9346 | F1:0.6475\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 18:52:55,559 INFO: Epoch:[050/100]\n",
      "2022-04-27 18:52:55,560 INFO: Train Loss:0.211 | Acc:0.9401 | F1:0.6940\n",
      "2022-04-27 18:53:14,797 INFO: val Loss:0.210 | Acc:0.9357 | F1:0.6568\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 18:54:44,240 INFO: Epoch:[051/100]\n",
      "2022-04-27 18:54:44,241 INFO: Train Loss:0.183 | Acc:0.9459 | F1:0.7245\n",
      "2022-04-27 18:55:03,379 INFO: val Loss:0.199 | Acc:0.9486 | F1:0.7008\n",
      "2022-04-27 18:55:04,055 INFO: -----------------SAVE:51epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 18:56:33,361 INFO: Epoch:[052/100]\n",
      "2022-04-27 18:56:33,362 INFO: Train Loss:0.211 | Acc:0.9418 | F1:0.7070\n",
      "2022-04-27 18:56:52,781 INFO: val Loss:0.195 | Acc:0.9521 | F1:0.7333\n",
      "2022-04-27 18:56:53,451 INFO: -----------------SAVE:52epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 18:58:23,858 INFO: Epoch:[053/100]\n",
      "2022-04-27 18:58:23,859 INFO: Train Loss:0.184 | Acc:0.9453 | F1:0.7355\n",
      "2022-04-27 18:58:42,815 INFO: val Loss:0.219 | Acc:0.9463 | F1:0.7146\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 19:00:13,966 INFO: Epoch:[054/100]\n",
      "2022-04-27 19:00:13,967 INFO: Train Loss:0.181 | Acc:0.9439 | F1:0.7341\n",
      "2022-04-27 19:00:33,487 INFO: val Loss:0.240 | Acc:0.9357 | F1:0.6780\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.29it/s]\n",
      "2022-04-27 19:02:06,805 INFO: Epoch:[055/100]\n",
      "2022-04-27 19:02:06,806 INFO: Train Loss:0.184 | Acc:0.9450 | F1:0.7368\n",
      "2022-04-27 19:02:26,142 INFO: val Loss:0.240 | Acc:0.9089 | F1:0.7322\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.30it/s]\n",
      "2022-04-27 19:03:59,087 INFO: Epoch:[056/100]\n",
      "2022-04-27 19:03:59,088 INFO: Train Loss:0.155 | Acc:0.9567 | F1:0.8014\n",
      "2022-04-27 19:04:18,814 INFO: val Loss:0.212 | Acc:0.9404 | F1:0.7096\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 19:05:48,800 INFO: Epoch:[057/100]\n",
      "2022-04-27 19:05:48,801 INFO: Train Loss:0.169 | Acc:0.9503 | F1:0.7726\n",
      "2022-04-27 19:06:08,103 INFO: val Loss:0.207 | Acc:0.9217 | F1:0.7254\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 19:07:38,975 INFO: Epoch:[058/100]\n",
      "2022-04-27 19:07:38,975 INFO: Train Loss:0.159 | Acc:0.9512 | F1:0.7653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 19:07:57,710 INFO: val Loss:0.176 | Acc:0.9486 | F1:0.7331\n",
      "2022-04-27 19:07:58,395 INFO: -----------------SAVE:58epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 19:09:27,648 INFO: Epoch:[059/100]\n",
      "2022-04-27 19:09:27,649 INFO: Train Loss:0.158 | Acc:0.9503 | F1:0.7761\n",
      "2022-04-27 19:09:46,879 INFO: val Loss:0.220 | Acc:0.9533 | F1:0.7506\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 19:11:16,908 INFO: Epoch:[060/100]\n",
      "2022-04-27 19:11:16,909 INFO: Train Loss:0.143 | Acc:0.9538 | F1:0.7763\n",
      "2022-04-27 19:11:36,100 INFO: val Loss:0.214 | Acc:0.9451 | F1:0.7138\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 19:13:05,499 INFO: Epoch:[061/100]\n",
      "2022-04-27 19:13:05,500 INFO: Train Loss:0.148 | Acc:0.9582 | F1:0.8160\n",
      "2022-04-27 19:13:25,370 INFO: val Loss:0.194 | Acc:0.9533 | F1:0.7609\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 19:14:55,212 INFO: Epoch:[062/100]\n",
      "2022-04-27 19:14:55,213 INFO: Train Loss:0.145 | Acc:0.9605 | F1:0.8009\n",
      "2022-04-27 19:15:14,237 INFO: val Loss:0.172 | Acc:0.9591 | F1:0.7529\n",
      "2022-04-27 19:15:14,901 INFO: -----------------SAVE:62epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 19:16:45,959 INFO: Epoch:[063/100]\n",
      "2022-04-27 19:16:45,960 INFO: Train Loss:0.134 | Acc:0.9585 | F1:0.8138\n",
      "2022-04-27 19:17:04,706 INFO: val Loss:0.143 | Acc:0.9614 | F1:0.7973\n",
      "2022-04-27 19:17:05,409 INFO: -----------------SAVE:63epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 19:18:37,663 INFO: Epoch:[064/100]\n",
      "2022-04-27 19:18:37,663 INFO: Train Loss:0.122 | Acc:0.9638 | F1:0.8360\n",
      "2022-04-27 19:18:56,489 INFO: val Loss:0.179 | Acc:0.9579 | F1:0.7903\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 19:20:28,767 INFO: Epoch:[065/100]\n",
      "2022-04-27 19:20:28,767 INFO: Train Loss:0.122 | Acc:0.9649 | F1:0.8398\n",
      "2022-04-27 19:20:47,923 INFO: val Loss:0.219 | Acc:0.9591 | F1:0.7979\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 19:22:18,327 INFO: Epoch:[066/100]\n",
      "2022-04-27 19:22:18,327 INFO: Train Loss:0.126 | Acc:0.9635 | F1:0.8443\n",
      "2022-04-27 19:22:37,395 INFO: val Loss:0.163 | Acc:0.9614 | F1:0.8201\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 19:24:08,337 INFO: Epoch:[067/100]\n",
      "2022-04-27 19:24:08,338 INFO: Train Loss:0.120 | Acc:0.9673 | F1:0.8416\n",
      "2022-04-27 19:24:27,989 INFO: val Loss:0.176 | Acc:0.9614 | F1:0.7962\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 19:25:57,494 INFO: Epoch:[068/100]\n",
      "2022-04-27 19:25:57,495 INFO: Train Loss:0.110 | Acc:0.9693 | F1:0.8633\n",
      "2022-04-27 19:26:16,707 INFO: val Loss:0.165 | Acc:0.9661 | F1:0.8120\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 19:27:47,058 INFO: Epoch:[069/100]\n",
      "2022-04-27 19:27:47,059 INFO: Train Loss:0.116 | Acc:0.9705 | F1:0.8743\n",
      "2022-04-27 19:28:06,318 INFO: val Loss:0.166 | Acc:0.9603 | F1:0.7775\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 19:29:36,726 INFO: Epoch:[070/100]\n",
      "2022-04-27 19:29:36,726 INFO: Train Loss:0.094 | Acc:0.9705 | F1:0.8666\n",
      "2022-04-27 19:29:55,568 INFO: val Loss:0.136 | Acc:0.9685 | F1:0.8335\n",
      "2022-04-27 19:29:56,245 INFO: -----------------SAVE:70epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 19:31:26,375 INFO: Epoch:[071/100]\n",
      "2022-04-27 19:31:26,375 INFO: Train Loss:0.090 | Acc:0.9760 | F1:0.9037\n",
      "2022-04-27 19:31:45,800 INFO: val Loss:0.178 | Acc:0.9568 | F1:0.7788\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 19:33:16,523 INFO: Epoch:[072/100]\n",
      "2022-04-27 19:33:16,524 INFO: Train Loss:0.094 | Acc:0.9746 | F1:0.8884\n",
      "2022-04-27 19:33:35,686 INFO: val Loss:0.167 | Acc:0.9708 | F1:0.8398\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.30it/s]\n",
      "2022-04-27 19:35:08,781 INFO: Epoch:[073/100]\n",
      "2022-04-27 19:35:08,782 INFO: Train Loss:0.097 | Acc:0.9746 | F1:0.8914\n",
      "2022-04-27 19:35:27,982 INFO: val Loss:0.172 | Acc:0.9661 | F1:0.8163\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 19:36:59,358 INFO: Epoch:[074/100]\n",
      "2022-04-27 19:36:59,359 INFO: Train Loss:0.099 | Acc:0.9725 | F1:0.8918\n",
      "2022-04-27 19:37:18,505 INFO: val Loss:0.171 | Acc:0.9638 | F1:0.8020\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.42it/s]\n",
      "2022-04-27 19:38:46,979 INFO: Epoch:[075/100]\n",
      "2022-04-27 19:38:46,980 INFO: Train Loss:0.078 | Acc:0.9816 | F1:0.9216\n",
      "2022-04-27 19:39:06,518 INFO: val Loss:0.148 | Acc:0.9696 | F1:0.8403\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 19:40:37,087 INFO: Epoch:[076/100]\n",
      "2022-04-27 19:40:37,088 INFO: Train Loss:0.075 | Acc:0.9816 | F1:0.9188\n",
      "2022-04-27 19:40:56,478 INFO: val Loss:0.187 | Acc:0.9696 | F1:0.8221\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 19:42:27,079 INFO: Epoch:[077/100]\n",
      "2022-04-27 19:42:27,080 INFO: Train Loss:0.073 | Acc:0.9819 | F1:0.9320\n",
      "2022-04-27 19:42:46,485 INFO: val Loss:0.155 | Acc:0.9673 | F1:0.8285\n",
      "2022-04-27 19:42:46,487 INFO: \n",
      "Best Val Epoch:70 | Val Loss:0.1363 | Val Acc:0.9685 | Val F1:0.8335\n",
      "2022-04-27 19:42:46,487 INFO: Total Process time:140.114Minute\n",
      "2022-04-27 19:42:46,492 INFO: {'exp_num': '1', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 1e-05, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3421\n",
      "Dataset size:856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 19:42:46,842 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 19:44:17,119 INFO: Epoch:[001/100]\n",
      "2022-04-27 19:44:17,120 INFO: Train Loss:4.496 | Acc:0.0050 | F1:0.0014\n",
      "2022-04-27 19:44:36,854 INFO: val Loss:4.453 | Acc:0.0339 | F1:0.0032\n",
      "2022-04-27 19:44:37,392 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 19:46:08,671 INFO: Epoch:[002/100]\n",
      "2022-04-27 19:46:08,671 INFO: Train Loss:4.475 | Acc:0.0094 | F1:0.0044\n",
      "2022-04-27 19:46:28,556 INFO: val Loss:4.398 | Acc:0.0467 | F1:0.0046\n",
      "2022-04-27 19:46:29,249 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 19:48:00,250 INFO: Epoch:[003/100]\n",
      "2022-04-27 19:48:00,251 INFO: Train Loss:4.414 | Acc:0.0222 | F1:0.0071\n",
      "2022-04-27 19:48:20,001 INFO: val Loss:4.294 | Acc:0.0888 | F1:0.0110\n",
      "2022-04-27 19:48:20,755 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:34<00:00,  2.27it/s]\n",
      "2022-04-27 19:49:54,944 INFO: Epoch:[004/100]\n",
      "2022-04-27 19:49:54,944 INFO: Train Loss:4.202 | Acc:0.1564 | F1:0.0327\n",
      "2022-04-27 19:50:15,337 INFO: val Loss:3.897 | Acc:0.5234 | F1:0.0921\n",
      "2022-04-27 19:50:16,094 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.28it/s]\n",
      "2022-04-27 19:51:49,799 INFO: Epoch:[005/100]\n",
      "2022-04-27 19:51:49,800 INFO: Train Loss:3.829 | Acc:0.4844 | F1:0.0905\n",
      "2022-04-27 19:52:09,953 INFO: val Loss:3.435 | Acc:0.8049 | F1:0.1393\n",
      "2022-04-27 19:52:10,702 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 19:53:41,311 INFO: Epoch:[006/100]\n",
      "2022-04-27 19:53:41,311 INFO: Train Loss:3.429 | Acc:0.6577 | F1:0.1150\n",
      "2022-04-27 19:54:00,646 INFO: val Loss:2.890 | Acc:0.8353 | F1:0.1449\n",
      "2022-04-27 19:54:01,359 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:34<00:00,  2.27it/s]\n",
      "2022-04-27 19:55:35,582 INFO: Epoch:[007/100]\n",
      "2022-04-27 19:55:35,583 INFO: Train Loss:3.008 | Acc:0.7293 | F1:0.1255\n",
      "2022-04-27 19:55:55,658 INFO: val Loss:2.442 | Acc:0.8353 | F1:0.1448\n",
      "2022-04-27 19:55:56,298 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.29it/s]\n",
      "2022-04-27 19:57:29,593 INFO: Epoch:[008/100]\n",
      "2022-04-27 19:57:29,594 INFO: Train Loss:2.519 | Acc:0.7685 | F1:0.1348\n",
      "2022-04-27 19:57:49,622 INFO: val Loss:1.823 | Acc:0.8353 | F1:0.1451\n",
      "2022-04-27 19:57:50,357 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 19:59:20,701 INFO: Epoch:[009/100]\n",
      "2022-04-27 19:59:20,702 INFO: Train Loss:2.118 | Acc:0.7904 | F1:0.1375\n",
      "2022-04-27 19:59:40,839 INFO: val Loss:1.442 | Acc:0.8353 | F1:0.1451\n",
      "2022-04-27 19:59:41,651 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 20:01:12,617 INFO: Epoch:[010/100]\n",
      "2022-04-27 20:01:12,617 INFO: Train Loss:1.845 | Acc:0.8088 | F1:0.1413\n",
      "2022-04-27 20:01:32,821 INFO: val Loss:1.161 | Acc:0.8493 | F1:0.1566\n",
      "2022-04-27 20:01:33,545 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 20:03:04,898 INFO: Epoch:[011/100]\n",
      "2022-04-27 20:03:04,899 INFO: Train Loss:1.678 | Acc:0.8135 | F1:0.1443\n",
      "2022-04-27 20:03:25,140 INFO: val Loss:1.064 | Acc:0.8493 | F1:0.1563\n",
      "2022-04-27 20:03:25,900 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 20:04:57,067 INFO: Epoch:[012/100]\n",
      "2022-04-27 20:04:57,068 INFO: Train Loss:1.555 | Acc:0.8313 | F1:0.1521\n",
      "2022-04-27 20:05:17,206 INFO: val Loss:0.951 | Acc:0.8493 | F1:0.1563\n",
      "2022-04-27 20:05:18,030 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 20:06:48,953 INFO: Epoch:[013/100]\n",
      "2022-04-27 20:06:48,953 INFO: Train Loss:1.471 | Acc:0.8363 | F1:0.1528\n",
      "2022-04-27 20:07:08,952 INFO: val Loss:0.892 | Acc:0.8493 | F1:0.1563\n",
      "2022-04-27 20:07:09,645 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 20:08:42,324 INFO: Epoch:[014/100]\n",
      "2022-04-27 20:08:42,325 INFO: Train Loss:1.358 | Acc:0.8386 | F1:0.1537\n",
      "2022-04-27 20:09:02,414 INFO: val Loss:0.804 | Acc:0.8493 | F1:0.1563\n",
      "2022-04-27 20:09:03,121 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.29it/s]\n",
      "2022-04-27 20:10:36,778 INFO: Epoch:[015/100]\n",
      "2022-04-27 20:10:36,778 INFO: Train Loss:1.243 | Acc:0.8401 | F1:0.1548\n",
      "2022-04-27 20:10:56,921 INFO: val Loss:0.778 | Acc:0.8493 | F1:0.1563\n",
      "2022-04-27 20:10:57,695 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 20:12:28,805 INFO: Epoch:[016/100]\n",
      "2022-04-27 20:12:28,806 INFO: Train Loss:1.110 | Acc:0.8375 | F1:0.1537\n",
      "2022-04-27 20:12:48,695 INFO: val Loss:0.697 | Acc:0.8493 | F1:0.1563\n",
      "2022-04-27 20:12:49,408 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 20:14:19,846 INFO: Epoch:[017/100]\n",
      "2022-04-27 20:14:19,847 INFO: Train Loss:1.018 | Acc:0.8424 | F1:0.1567\n",
      "2022-04-27 20:14:39,874 INFO: val Loss:0.658 | Acc:0.8423 | F1:0.1655\n",
      "2022-04-27 20:14:40,566 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 20:16:10,906 INFO: Epoch:[018/100]\n",
      "2022-04-27 20:16:10,906 INFO: Train Loss:0.905 | Acc:0.8410 | F1:0.1602\n",
      "2022-04-27 20:16:30,909 INFO: val Loss:0.559 | Acc:0.8563 | F1:0.1811\n",
      "2022-04-27 20:16:31,606 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 20:18:03,248 INFO: Epoch:[019/100]\n",
      "2022-04-27 20:18:03,249 INFO: Train Loss:0.837 | Acc:0.8451 | F1:0.1789\n",
      "2022-04-27 20:18:23,057 INFO: val Loss:0.524 | Acc:0.8563 | F1:0.1850\n",
      "2022-04-27 20:18:23,766 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 20:19:54,605 INFO: Epoch:[020/100]\n",
      "2022-04-27 20:19:54,606 INFO: Train Loss:0.761 | Acc:0.8515 | F1:0.1961\n",
      "2022-04-27 20:20:14,529 INFO: val Loss:0.476 | Acc:0.8540 | F1:0.2219\n",
      "2022-04-27 20:20:15,317 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 20:21:45,292 INFO: Epoch:[021/100]\n",
      "2022-04-27 20:21:45,293 INFO: Train Loss:0.683 | Acc:0.8521 | F1:0.2090\n",
      "2022-04-27 20:22:05,033 INFO: val Loss:0.457 | Acc:0.8598 | F1:0.2149\n",
      "2022-04-27 20:22:05,724 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 20:23:38,569 INFO: Epoch:[022/100]\n",
      "2022-04-27 20:23:38,569 INFO: Train Loss:0.665 | Acc:0.8512 | F1:0.2155\n",
      "2022-04-27 20:23:58,375 INFO: val Loss:0.478 | Acc:0.8715 | F1:0.2693\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 20:25:30,964 INFO: Epoch:[023/100]\n",
      "2022-04-27 20:25:30,965 INFO: Train Loss:0.606 | Acc:0.8617 | F1:0.2532\n",
      "2022-04-27 20:25:50,877 INFO: val Loss:0.389 | Acc:0.8832 | F1:0.2856\n",
      "2022-04-27 20:25:51,563 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 20:27:21,529 INFO: Epoch:[024/100]\n",
      "2022-04-27 20:27:21,530 INFO: Train Loss:0.588 | Acc:0.8620 | F1:0.2542\n",
      "2022-04-27 20:27:41,345 INFO: val Loss:0.373 | Acc:0.8843 | F1:0.2889\n",
      "2022-04-27 20:27:42,020 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 20:29:11,418 INFO: Epoch:[025/100]\n",
      "2022-04-27 20:29:11,418 INFO: Train Loss:0.538 | Acc:0.8629 | F1:0.2507\n",
      "2022-04-27 20:29:31,275 INFO: val Loss:0.376 | Acc:0.8820 | F1:0.2893\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 20:31:02,331 INFO: Epoch:[026/100]\n",
      "2022-04-27 20:31:02,331 INFO: Train Loss:0.529 | Acc:0.8723 | F1:0.3188\n",
      "2022-04-27 20:31:22,431 INFO: val Loss:0.362 | Acc:0.8820 | F1:0.2878\n",
      "2022-04-27 20:31:23,139 INFO: -----------------SAVE:26epoch----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 20:32:54,389 INFO: Epoch:[027/100]\n",
      "2022-04-27 20:32:54,390 INFO: Train Loss:0.515 | Acc:0.8690 | F1:0.3036\n",
      "2022-04-27 20:33:14,438 INFO: val Loss:0.342 | Acc:0.8902 | F1:0.3395\n",
      "2022-04-27 20:33:15,117 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 20:34:46,212 INFO: Epoch:[028/100]\n",
      "2022-04-27 20:34:46,213 INFO: Train Loss:0.471 | Acc:0.8714 | F1:0.3126\n",
      "2022-04-27 20:35:06,466 INFO: val Loss:0.366 | Acc:0.8925 | F1:0.3826\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 20:36:38,663 INFO: Epoch:[029/100]\n",
      "2022-04-27 20:36:38,664 INFO: Train Loss:0.444 | Acc:0.8766 | F1:0.3458\n",
      "2022-04-27 20:36:58,530 INFO: val Loss:0.340 | Acc:0.8984 | F1:0.4018\n",
      "2022-04-27 20:36:59,394 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 20:38:32,068 INFO: Epoch:[030/100]\n",
      "2022-04-27 20:38:32,069 INFO: Train Loss:0.452 | Acc:0.8831 | F1:0.3952\n",
      "2022-04-27 20:38:52,066 INFO: val Loss:0.315 | Acc:0.9042 | F1:0.4156\n",
      "2022-04-27 20:38:52,772 INFO: -----------------SAVE:30epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 20:40:24,910 INFO: Epoch:[031/100]\n",
      "2022-04-27 20:40:24,911 INFO: Train Loss:0.413 | Acc:0.8848 | F1:0.4064\n",
      "2022-04-27 20:40:44,668 INFO: val Loss:0.297 | Acc:0.9054 | F1:0.4492\n",
      "2022-04-27 20:40:45,413 INFO: -----------------SAVE:31epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 20:42:16,484 INFO: Epoch:[032/100]\n",
      "2022-04-27 20:42:16,485 INFO: Train Loss:0.400 | Acc:0.8910 | F1:0.4267\n",
      "2022-04-27 20:42:36,529 INFO: val Loss:0.281 | Acc:0.9124 | F1:0.4864\n",
      "2022-04-27 20:42:37,285 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 20:44:08,237 INFO: Epoch:[033/100]\n",
      "2022-04-27 20:44:08,238 INFO: Train Loss:0.381 | Acc:0.8930 | F1:0.4700\n",
      "2022-04-27 20:44:28,203 INFO: val Loss:0.314 | Acc:0.9112 | F1:0.4755\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 20:45:58,830 INFO: Epoch:[034/100]\n",
      "2022-04-27 20:45:58,831 INFO: Train Loss:0.384 | Acc:0.8956 | F1:0.4648\n",
      "2022-04-27 20:46:19,075 INFO: val Loss:0.272 | Acc:0.9241 | F1:0.5545\n",
      "2022-04-27 20:46:19,779 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 20:47:48,686 INFO: Epoch:[035/100]\n",
      "2022-04-27 20:47:48,687 INFO: Train Loss:0.340 | Acc:0.9027 | F1:0.5025\n",
      "2022-04-27 20:48:08,582 INFO: val Loss:0.255 | Acc:0.9194 | F1:0.5004\n",
      "2022-04-27 20:48:09,260 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 20:49:39,677 INFO: Epoch:[036/100]\n",
      "2022-04-27 20:49:39,678 INFO: Train Loss:0.338 | Acc:0.9006 | F1:0.5038\n",
      "2022-04-27 20:49:59,259 INFO: val Loss:0.236 | Acc:0.9252 | F1:0.5508\n",
      "2022-04-27 20:49:59,947 INFO: -----------------SAVE:36epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 20:51:32,499 INFO: Epoch:[037/100]\n",
      "2022-04-27 20:51:32,500 INFO: Train Loss:0.321 | Acc:0.9038 | F1:0.5084\n",
      "2022-04-27 20:51:52,273 INFO: val Loss:0.251 | Acc:0.9299 | F1:0.5944\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.28it/s]\n",
      "2022-04-27 20:53:26,022 INFO: Epoch:[038/100]\n",
      "2022-04-27 20:53:26,023 INFO: Train Loss:0.309 | Acc:0.9097 | F1:0.5417\n",
      "2022-04-27 20:53:45,893 INFO: val Loss:0.254 | Acc:0.9299 | F1:0.5794\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 20:55:17,669 INFO: Epoch:[039/100]\n",
      "2022-04-27 20:55:17,669 INFO: Train Loss:0.302 | Acc:0.9120 | F1:0.5452\n",
      "2022-04-27 20:55:37,633 INFO: val Loss:0.262 | Acc:0.9276 | F1:0.5864\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 20:57:07,723 INFO: Epoch:[040/100]\n",
      "2022-04-27 20:57:07,723 INFO: Train Loss:0.283 | Acc:0.9120 | F1:0.5401\n",
      "2022-04-27 20:57:27,790 INFO: val Loss:0.244 | Acc:0.9264 | F1:0.6042\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 20:58:59,769 INFO: Epoch:[041/100]\n",
      "2022-04-27 20:58:59,770 INFO: Train Loss:0.277 | Acc:0.9184 | F1:0.5899\n",
      "2022-04-27 20:59:20,146 INFO: val Loss:0.369 | Acc:0.9065 | F1:0.6111\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.43it/s]\n",
      "2022-04-27 21:00:48,362 INFO: Epoch:[042/100]\n",
      "2022-04-27 21:00:48,363 INFO: Train Loss:0.264 | Acc:0.9193 | F1:0.5897\n",
      "2022-04-27 21:01:08,375 INFO: val Loss:0.262 | Acc:0.9264 | F1:0.5707\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 21:02:39,252 INFO: Epoch:[043/100]\n",
      "2022-04-27 21:02:39,253 INFO: Train Loss:0.259 | Acc:0.9222 | F1:0.6216\n",
      "2022-04-27 21:02:59,155 INFO: val Loss:0.239 | Acc:0.9334 | F1:0.6079\n",
      "2022-04-27 21:02:59,156 INFO: \n",
      "Best Val Epoch:36 | Val Loss:0.2358 | Val Acc:0.9252 | Val F1:0.5508\n",
      "2022-04-27 21:02:59,157 INFO: Total Process time:80.202Minute\n",
      "2022-04-27 21:02:59,161 INFO: {'exp_num': '2', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 1e-05, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3422\n",
      "Dataset size:855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 21:02:59,542 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 21:04:30,694 INFO: Epoch:[001/100]\n",
      "2022-04-27 21:04:30,695 INFO: Train Loss:4.501 | Acc:0.0061 | F1:0.0030\n",
      "2022-04-27 21:04:50,416 INFO: val Loss:4.449 | Acc:0.0281 | F1:0.0029\n",
      "2022-04-27 21:04:50,898 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 21:06:23,242 INFO: Epoch:[002/100]\n",
      "2022-04-27 21:06:23,243 INFO: Train Loss:4.472 | Acc:0.0082 | F1:0.0022\n",
      "2022-04-27 21:06:42,611 INFO: val Loss:4.404 | Acc:0.0421 | F1:0.0041\n",
      "2022-04-27 21:06:43,332 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 21:08:15,898 INFO: Epoch:[003/100]\n",
      "2022-04-27 21:08:15,898 INFO: Train Loss:4.409 | Acc:0.0207 | F1:0.0069\n",
      "2022-04-27 21:08:35,414 INFO: val Loss:4.314 | Acc:0.0889 | F1:0.0120\n",
      "2022-04-27 21:08:36,101 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 21:10:07,821 INFO: Epoch:[004/100]\n",
      "2022-04-27 21:10:07,822 INFO: Train Loss:4.203 | Acc:0.1493 | F1:0.0297\n",
      "2022-04-27 21:10:27,647 INFO: val Loss:3.914 | Acc:0.4538 | F1:0.0782\n",
      "2022-04-27 21:10:28,328 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 21:11:58,973 INFO: Epoch:[005/100]\n",
      "2022-04-27 21:11:58,974 INFO: Train Loss:3.832 | Acc:0.4705 | F1:0.0876\n",
      "2022-04-27 21:12:18,793 INFO: val Loss:3.479 | Acc:0.7883 | F1:0.1373\n",
      "2022-04-27 21:12:19,510 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 21:13:51,606 INFO: Epoch:[006/100]\n",
      "2022-04-27 21:13:51,606 INFO: Train Loss:3.429 | Acc:0.6485 | F1:0.1149\n",
      "2022-04-27 21:14:11,018 INFO: val Loss:2.920 | Acc:0.8363 | F1:0.1466\n",
      "2022-04-27 21:14:11,680 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 21:15:42,401 INFO: Epoch:[007/100]\n",
      "2022-04-27 21:15:42,402 INFO: Train Loss:3.001 | Acc:0.7312 | F1:0.1254\n",
      "2022-04-27 21:16:02,154 INFO: val Loss:2.385 | Acc:0.8363 | F1:0.1466\n",
      "2022-04-27 21:16:02,963 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 21:17:33,853 INFO: Epoch:[008/100]\n",
      "2022-04-27 21:17:33,853 INFO: Train Loss:2.504 | Acc:0.7683 | F1:0.1330\n",
      "2022-04-27 21:17:53,508 INFO: val Loss:1.848 | Acc:0.8351 | F1:0.1448\n",
      "2022-04-27 21:17:54,165 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 21:19:24,385 INFO: Epoch:[009/100]\n",
      "2022-04-27 21:19:24,386 INFO: Train Loss:2.128 | Acc:0.7843 | F1:0.1364\n",
      "2022-04-27 21:19:44,044 INFO: val Loss:1.488 | Acc:0.8351 | F1:0.1450\n",
      "2022-04-27 21:19:44,835 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 214/214 [01:34<00:00,  2.28it/s]\n",
      "2022-04-27 21:21:18,888 INFO: Epoch:[010/100]\n",
      "2022-04-27 21:21:18,889 INFO: Train Loss:1.829 | Acc:0.8077 | F1:0.1403\n",
      "2022-04-27 21:21:38,121 INFO: val Loss:1.230 | Acc:0.8409 | F1:0.1518\n",
      "2022-04-27 21:21:38,810 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 21:23:11,401 INFO: Epoch:[011/100]\n",
      "2022-04-27 21:23:11,402 INFO: Train Loss:1.666 | Acc:0.8209 | F1:0.1473\n",
      "2022-04-27 21:23:31,314 INFO: val Loss:1.097 | Acc:0.8491 | F1:0.1562\n",
      "2022-04-27 21:23:31,999 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 21:25:03,189 INFO: Epoch:[012/100]\n",
      "2022-04-27 21:25:03,190 INFO: Train Loss:1.562 | Acc:0.8279 | F1:0.1514\n",
      "2022-04-27 21:25:23,023 INFO: val Loss:0.981 | Acc:0.8491 | F1:0.1562\n",
      "2022-04-27 21:25:23,726 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 21:26:54,985 INFO: Epoch:[013/100]\n",
      "2022-04-27 21:26:54,986 INFO: Train Loss:1.445 | Acc:0.8375 | F1:0.1540\n",
      "2022-04-27 21:27:14,890 INFO: val Loss:0.902 | Acc:0.8491 | F1:0.1562\n",
      "2022-04-27 21:27:15,571 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 21:28:47,371 INFO: Epoch:[014/100]\n",
      "2022-04-27 21:28:47,371 INFO: Train Loss:1.351 | Acc:0.8387 | F1:0.1541\n",
      "2022-04-27 21:29:07,288 INFO: val Loss:0.800 | Acc:0.8491 | F1:0.1562\n",
      "2022-04-27 21:29:07,971 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 21:30:37,941 INFO: Epoch:[015/100]\n",
      "2022-04-27 21:30:37,942 INFO: Train Loss:1.238 | Acc:0.8396 | F1:0.1544\n",
      "2022-04-27 21:30:58,017 INFO: val Loss:0.738 | Acc:0.8491 | F1:0.1562\n",
      "2022-04-27 21:30:58,726 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 21:32:30,322 INFO: Epoch:[016/100]\n",
      "2022-04-27 21:32:30,323 INFO: Train Loss:1.122 | Acc:0.8372 | F1:0.1537\n",
      "2022-04-27 21:32:50,206 INFO: val Loss:0.688 | Acc:0.8515 | F1:0.1677\n",
      "2022-04-27 21:32:50,944 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.29it/s]\n",
      "2022-04-27 21:34:24,539 INFO: Epoch:[017/100]\n",
      "2022-04-27 21:34:24,540 INFO: Train Loss:0.999 | Acc:0.8440 | F1:0.1550\n",
      "2022-04-27 21:34:44,289 INFO: val Loss:0.646 | Acc:0.8515 | F1:0.1677\n",
      "2022-04-27 21:34:44,989 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 21:36:16,554 INFO: Epoch:[018/100]\n",
      "2022-04-27 21:36:16,555 INFO: Train Loss:0.910 | Acc:0.8410 | F1:0.1712\n",
      "2022-04-27 21:36:36,475 INFO: val Loss:0.572 | Acc:0.8561 | F1:0.1816\n",
      "2022-04-27 21:36:37,214 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 21:38:08,700 INFO: Epoch:[019/100]\n",
      "2022-04-27 21:38:08,701 INFO: Train Loss:0.841 | Acc:0.8477 | F1:0.1727\n",
      "2022-04-27 21:38:28,466 INFO: val Loss:0.531 | Acc:0.8538 | F1:0.1965\n",
      "2022-04-27 21:38:29,139 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 21:40:00,450 INFO: Epoch:[020/100]\n",
      "2022-04-27 21:40:00,451 INFO: Train Loss:0.768 | Acc:0.8510 | F1:0.2012\n",
      "2022-04-27 21:40:20,209 INFO: val Loss:0.496 | Acc:0.8690 | F1:0.2362\n",
      "2022-04-27 21:40:20,917 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 21:41:50,159 INFO: Epoch:[021/100]\n",
      "2022-04-27 21:41:50,160 INFO: Train Loss:0.696 | Acc:0.8542 | F1:0.2150\n",
      "2022-04-27 21:42:10,143 INFO: val Loss:0.423 | Acc:0.8772 | F1:0.2592\n",
      "2022-04-27 21:42:10,831 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 21:43:42,191 INFO: Epoch:[022/100]\n",
      "2022-04-27 21:43:42,192 INFO: Train Loss:0.668 | Acc:0.8553 | F1:0.2298\n",
      "2022-04-27 21:44:02,338 INFO: val Loss:0.430 | Acc:0.8749 | F1:0.2532\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 21:45:32,455 INFO: Epoch:[023/100]\n",
      "2022-04-27 21:45:32,455 INFO: Train Loss:0.621 | Acc:0.8603 | F1:0.2350\n",
      "2022-04-27 21:45:51,962 INFO: val Loss:0.441 | Acc:0.8819 | F1:0.2906\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 21:47:24,394 INFO: Epoch:[024/100]\n",
      "2022-04-27 21:47:24,394 INFO: Train Loss:0.590 | Acc:0.8618 | F1:0.2635\n",
      "2022-04-27 21:47:43,795 INFO: val Loss:0.432 | Acc:0.8760 | F1:0.2631\n",
      "100%|██████████| 214/214 [01:33<00:00,  2.30it/s]\n",
      "2022-04-27 21:49:16,895 INFO: Epoch:[025/100]\n",
      "2022-04-27 21:49:16,896 INFO: Train Loss:0.527 | Acc:0.8673 | F1:0.2744\n",
      "2022-04-27 21:49:37,032 INFO: val Loss:0.400 | Acc:0.8854 | F1:0.3102\n",
      "2022-04-27 21:49:37,743 INFO: -----------------SAVE:25epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 21:51:08,906 INFO: Epoch:[026/100]\n",
      "2022-04-27 21:51:08,906 INFO: Train Loss:0.531 | Acc:0.8627 | F1:0.2706\n",
      "2022-04-27 21:51:28,764 INFO: val Loss:0.370 | Acc:0.8936 | F1:0.3723\n",
      "2022-04-27 21:51:29,441 INFO: -----------------SAVE:26epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 21:53:00,052 INFO: Epoch:[027/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 21:53:00,053 INFO: Train Loss:0.522 | Acc:0.8694 | F1:0.3090\n",
      "2022-04-27 21:53:19,579 INFO: val Loss:0.361 | Acc:0.8877 | F1:0.3335\n",
      "2022-04-27 21:53:20,272 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 21:54:48,928 INFO: Epoch:[028/100]\n",
      "2022-04-27 21:54:48,929 INFO: Train Loss:0.490 | Acc:0.8720 | F1:0.3272\n",
      "2022-04-27 21:55:09,036 INFO: val Loss:0.343 | Acc:0.9029 | F1:0.4088\n",
      "2022-04-27 21:55:09,756 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 21:56:39,461 INFO: Epoch:[029/100]\n",
      "2022-04-27 21:56:39,462 INFO: Train Loss:0.452 | Acc:0.8741 | F1:0.3400\n",
      "2022-04-27 21:56:59,391 INFO: val Loss:0.399 | Acc:0.8912 | F1:0.3658\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 21:58:29,595 INFO: Epoch:[030/100]\n",
      "2022-04-27 21:58:29,595 INFO: Train Loss:0.451 | Acc:0.8793 | F1:0.3693\n",
      "2022-04-27 21:58:49,056 INFO: val Loss:0.394 | Acc:0.8877 | F1:0.3571\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 22:00:19,353 INFO: Epoch:[031/100]\n",
      "2022-04-27 22:00:19,354 INFO: Train Loss:0.416 | Acc:0.8822 | F1:0.3841\n",
      "2022-04-27 22:00:39,425 INFO: val Loss:0.368 | Acc:0.8901 | F1:0.3573\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 22:02:10,593 INFO: Epoch:[032/100]\n",
      "2022-04-27 22:02:10,594 INFO: Train Loss:0.398 | Acc:0.8939 | F1:0.4412\n",
      "2022-04-27 22:02:29,959 INFO: val Loss:0.312 | Acc:0.9018 | F1:0.4053\n",
      "2022-04-27 22:02:30,675 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 214/214 [01:34<00:00,  2.27it/s]\n",
      "2022-04-27 22:04:04,993 INFO: Epoch:[033/100]\n",
      "2022-04-27 22:04:04,994 INFO: Train Loss:0.402 | Acc:0.8901 | F1:0.4330\n",
      "2022-04-27 22:04:24,549 INFO: val Loss:0.341 | Acc:0.9006 | F1:0.4303\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 22:05:56,188 INFO: Epoch:[034/100]\n",
      "2022-04-27 22:05:56,188 INFO: Train Loss:0.374 | Acc:0.8951 | F1:0.4488\n",
      "2022-04-27 22:06:16,051 INFO: val Loss:0.311 | Acc:0.9099 | F1:0.4620\n",
      "2022-04-27 22:06:16,759 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 22:07:47,839 INFO: Epoch:[035/100]\n",
      "2022-04-27 22:07:47,840 INFO: Train Loss:0.370 | Acc:0.8933 | F1:0.4434\n",
      "2022-04-27 22:08:07,454 INFO: val Loss:0.275 | Acc:0.9099 | F1:0.4366\n",
      "2022-04-27 22:08:08,180 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 22:09:38,293 INFO: Epoch:[036/100]\n",
      "2022-04-27 22:09:38,294 INFO: Train Loss:0.353 | Acc:0.8989 | F1:0.4721\n",
      "2022-04-27 22:09:57,974 INFO: val Loss:0.256 | Acc:0.9181 | F1:0.5092\n",
      "2022-04-27 22:09:58,645 INFO: -----------------SAVE:36epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 22:11:30,330 INFO: Epoch:[037/100]\n",
      "2022-04-27 22:11:30,331 INFO: Train Loss:0.309 | Acc:0.9094 | F1:0.5231\n",
      "2022-04-27 22:11:50,233 INFO: val Loss:0.284 | Acc:0.9216 | F1:0.5391\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 22:13:19,370 INFO: Epoch:[038/100]\n",
      "2022-04-27 22:13:19,371 INFO: Train Loss:0.325 | Acc:0.9050 | F1:0.5102\n",
      "2022-04-27 22:13:38,765 INFO: val Loss:0.268 | Acc:0.9251 | F1:0.5594\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 22:15:07,974 INFO: Epoch:[039/100]\n",
      "2022-04-27 22:15:07,975 INFO: Train Loss:0.310 | Acc:0.9062 | F1:0.5114\n",
      "2022-04-27 22:15:27,901 INFO: val Loss:0.256 | Acc:0.9251 | F1:0.5225\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 22:16:58,025 INFO: Epoch:[040/100]\n",
      "2022-04-27 22:16:58,026 INFO: Train Loss:0.289 | Acc:0.9155 | F1:0.5730\n",
      "2022-04-27 22:17:17,575 INFO: val Loss:0.267 | Acc:0.9298 | F1:0.5996\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.30it/s]\n",
      "2022-04-27 22:18:50,448 INFO: Epoch:[041/100]\n",
      "2022-04-27 22:18:50,449 INFO: Train Loss:0.284 | Acc:0.9173 | F1:0.5643\n",
      "2022-04-27 22:19:10,226 INFO: val Loss:0.245 | Acc:0.9298 | F1:0.5664\n",
      "2022-04-27 22:19:10,938 INFO: -----------------SAVE:41epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 22:20:43,473 INFO: Epoch:[042/100]\n",
      "2022-04-27 22:20:43,473 INFO: Train Loss:0.297 | Acc:0.9120 | F1:0.5660\n",
      "2022-04-27 22:21:02,996 INFO: val Loss:0.225 | Acc:0.9333 | F1:0.6101\n",
      "2022-04-27 22:21:03,715 INFO: -----------------SAVE:42epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 22:22:33,306 INFO: Epoch:[043/100]\n",
      "2022-04-27 22:22:33,306 INFO: Train Loss:0.279 | Acc:0.9217 | F1:0.6209\n",
      "2022-04-27 22:22:52,983 INFO: val Loss:0.222 | Acc:0.9368 | F1:0.6363\n",
      "2022-04-27 22:22:54,117 INFO: -----------------SAVE:43epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 22:24:24,733 INFO: Epoch:[044/100]\n",
      "2022-04-27 22:24:24,734 INFO: Train Loss:0.242 | Acc:0.9264 | F1:0.6319\n",
      "2022-04-27 22:24:44,986 INFO: val Loss:0.261 | Acc:0.9158 | F1:0.5676\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 22:26:15,145 INFO: Epoch:[045/100]\n",
      "2022-04-27 22:26:15,146 INFO: Train Loss:0.245 | Acc:0.9272 | F1:0.6280\n",
      "2022-04-27 22:26:34,158 INFO: val Loss:0.209 | Acc:0.9333 | F1:0.6322\n",
      "2022-04-27 22:26:34,883 INFO: -----------------SAVE:45epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 22:28:03,815 INFO: Epoch:[046/100]\n",
      "2022-04-27 22:28:03,815 INFO: Train Loss:0.247 | Acc:0.9246 | F1:0.6308\n",
      "2022-04-27 22:28:23,475 INFO: val Loss:0.206 | Acc:0.9368 | F1:0.6360\n",
      "2022-04-27 22:28:24,191 INFO: -----------------SAVE:46epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 22:29:53,528 INFO: Epoch:[047/100]\n",
      "2022-04-27 22:29:53,529 INFO: Train Loss:0.240 | Acc:0.9243 | F1:0.6334\n",
      "2022-04-27 22:30:13,266 INFO: val Loss:0.181 | Acc:0.9509 | F1:0.7115\n",
      "2022-04-27 22:30:13,988 INFO: -----------------SAVE:47epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 22:31:44,211 INFO: Epoch:[048/100]\n",
      "2022-04-27 22:31:44,212 INFO: Train Loss:0.230 | Acc:0.9342 | F1:0.6766\n",
      "2022-04-27 22:32:03,607 INFO: val Loss:0.181 | Acc:0.9520 | F1:0.7062\n",
      "2022-04-27 22:32:04,283 INFO: -----------------SAVE:48epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.30it/s]\n",
      "2022-04-27 22:33:37,232 INFO: Epoch:[049/100]\n",
      "2022-04-27 22:33:37,233 INFO: Train Loss:0.220 | Acc:0.9337 | F1:0.6761\n",
      "2022-04-27 22:33:56,808 INFO: val Loss:0.163 | Acc:0.9532 | F1:0.7213\n",
      "2022-04-27 22:33:57,479 INFO: -----------------SAVE:49epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 22:35:28,970 INFO: Epoch:[050/100]\n",
      "2022-04-27 22:35:28,971 INFO: Train Loss:0.213 | Acc:0.9348 | F1:0.6772\n",
      "2022-04-27 22:35:48,614 INFO: val Loss:0.217 | Acc:0.9415 | F1:0.6500\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 22:37:19,400 INFO: Epoch:[051/100]\n",
      "2022-04-27 22:37:19,401 INFO: Train Loss:0.203 | Acc:0.9401 | F1:0.6992\n",
      "2022-04-27 22:37:39,297 INFO: val Loss:0.201 | Acc:0.9450 | F1:0.6853\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 22:39:09,951 INFO: Epoch:[052/100]\n",
      "2022-04-27 22:39:09,952 INFO: Train Loss:0.201 | Acc:0.9439 | F1:0.7306\n",
      "2022-04-27 22:39:29,497 INFO: val Loss:0.200 | Acc:0.9556 | F1:0.7180\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 22:41:00,637 INFO: Epoch:[053/100]\n",
      "2022-04-27 22:41:00,638 INFO: Train Loss:0.198 | Acc:0.9439 | F1:0.7341\n",
      "2022-04-27 22:41:20,585 INFO: val Loss:0.181 | Acc:0.9556 | F1:0.7235\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 22:42:51,047 INFO: Epoch:[054/100]\n",
      "2022-04-27 22:42:51,047 INFO: Train Loss:0.193 | Acc:0.9421 | F1:0.7219\n",
      "2022-04-27 22:43:11,207 INFO: val Loss:0.173 | Acc:0.9509 | F1:0.6988\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.41it/s]\n",
      "2022-04-27 22:44:39,896 INFO: Epoch:[055/100]\n",
      "2022-04-27 22:44:39,897 INFO: Train Loss:0.182 | Acc:0.9456 | F1:0.7419\n",
      "2022-04-27 22:45:00,212 INFO: val Loss:0.138 | Acc:0.9602 | F1:0.7549\n",
      "2022-04-27 22:45:00,887 INFO: -----------------SAVE:55epoch----------------\n",
      "100%|██████████| 214/214 [01:28<00:00,  2.42it/s]\n",
      "2022-04-27 22:46:29,502 INFO: Epoch:[056/100]\n",
      "2022-04-27 22:46:29,503 INFO: Train Loss:0.174 | Acc:0.9506 | F1:0.7670\n",
      "2022-04-27 22:46:49,145 INFO: val Loss:0.138 | Acc:0.9520 | F1:0.7132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 22:48:19,399 INFO: Epoch:[057/100]\n",
      "2022-04-27 22:48:19,400 INFO: Train Loss:0.175 | Acc:0.9524 | F1:0.7843\n",
      "2022-04-27 22:48:39,247 INFO: val Loss:0.141 | Acc:0.9614 | F1:0.7808\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 22:50:10,708 INFO: Epoch:[058/100]\n",
      "2022-04-27 22:50:10,709 INFO: Train Loss:0.166 | Acc:0.9509 | F1:0.7655\n",
      "2022-04-27 22:50:30,037 INFO: val Loss:0.195 | Acc:0.9626 | F1:0.7848\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 22:52:02,693 INFO: Epoch:[059/100]\n",
      "2022-04-27 22:52:02,694 INFO: Train Loss:0.148 | Acc:0.9573 | F1:0.7971\n",
      "2022-04-27 22:52:22,108 INFO: val Loss:0.209 | Acc:0.9485 | F1:0.7110\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 22:53:53,921 INFO: Epoch:[060/100]\n",
      "2022-04-27 22:53:53,922 INFO: Train Loss:0.156 | Acc:0.9541 | F1:0.7810\n",
      "2022-04-27 22:54:13,379 INFO: val Loss:0.191 | Acc:0.9532 | F1:0.7456\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 22:55:43,103 INFO: Epoch:[061/100]\n",
      "2022-04-27 22:55:43,104 INFO: Train Loss:0.151 | Acc:0.9568 | F1:0.8147\n",
      "2022-04-27 22:56:02,650 INFO: val Loss:0.143 | Acc:0.9567 | F1:0.7695\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 22:57:31,977 INFO: Epoch:[062/100]\n",
      "2022-04-27 22:57:31,978 INFO: Train Loss:0.161 | Acc:0.9559 | F1:0.8118\n",
      "2022-04-27 22:57:51,769 INFO: val Loss:0.164 | Acc:0.9602 | F1:0.7689\n",
      "2022-04-27 22:57:51,770 INFO: \n",
      "Best Val Epoch:55 | Val Loss:0.1375 | Val Acc:0.9602 | Val F1:0.7549\n",
      "2022-04-27 22:57:51,771 INFO: Total Process time:114.867Minute\n",
      "2022-04-27 22:57:51,776 INFO: {'exp_num': '3', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 288, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 1e-05, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3422\n",
      "Dataset size:855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 22:57:52,200 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 22:59:21,676 INFO: Epoch:[001/100]\n",
      "2022-04-27 22:59:21,676 INFO: Train Loss:4.498 | Acc:0.0061 | F1:0.0021\n",
      "2022-04-27 22:59:41,022 INFO: val Loss:4.442 | Acc:0.0339 | F1:0.0032\n",
      "2022-04-27 22:59:41,547 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.40it/s]\n",
      "2022-04-27 23:01:10,870 INFO: Epoch:[002/100]\n",
      "2022-04-27 23:01:10,871 INFO: Train Loss:4.465 | Acc:0.0082 | F1:0.0039\n",
      "2022-04-27 23:01:30,345 INFO: val Loss:4.409 | Acc:0.0409 | F1:0.0044\n",
      "2022-04-27 23:01:31,008 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 23:03:01,544 INFO: Epoch:[003/100]\n",
      "2022-04-27 23:03:01,545 INFO: Train Loss:4.412 | Acc:0.0184 | F1:0.0044\n",
      "2022-04-27 23:03:20,820 INFO: val Loss:4.307 | Acc:0.0959 | F1:0.0121\n",
      "2022-04-27 23:03:21,509 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 23:04:51,322 INFO: Epoch:[004/100]\n",
      "2022-04-27 23:04:51,323 INFO: Train Loss:4.204 | Acc:0.1426 | F1:0.0297\n",
      "2022-04-27 23:05:10,796 INFO: val Loss:3.901 | Acc:0.4994 | F1:0.0838\n",
      "2022-04-27 23:05:11,478 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 23:06:41,001 INFO: Epoch:[005/100]\n",
      "2022-04-27 23:06:41,001 INFO: Train Loss:3.828 | Acc:0.4757 | F1:0.0889\n",
      "2022-04-27 23:07:00,367 INFO: val Loss:3.447 | Acc:0.7883 | F1:0.1340\n",
      "2022-04-27 23:07:01,030 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.33it/s]\n",
      "2022-04-27 23:08:32,945 INFO: Epoch:[006/100]\n",
      "2022-04-27 23:08:32,946 INFO: Train Loss:3.433 | Acc:0.6473 | F1:0.1144\n",
      "2022-04-27 23:08:52,275 INFO: val Loss:2.887 | Acc:0.8339 | F1:0.1447\n",
      "2022-04-27 23:08:53,040 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.32it/s]\n",
      "2022-04-27 23:10:25,199 INFO: Epoch:[007/100]\n",
      "2022-04-27 23:10:25,199 INFO: Train Loss:2.976 | Acc:0.7276 | F1:0.1254\n",
      "2022-04-27 23:10:44,535 INFO: val Loss:2.422 | Acc:0.8339 | F1:0.1447\n",
      "2022-04-27 23:10:45,232 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.36it/s]\n",
      "2022-04-27 23:12:15,827 INFO: Epoch:[008/100]\n",
      "2022-04-27 23:12:15,827 INFO: Train Loss:2.487 | Acc:0.7671 | F1:0.1330\n",
      "2022-04-27 23:12:35,006 INFO: val Loss:1.842 | Acc:0.8339 | F1:0.1449\n",
      "2022-04-27 23:12:35,688 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.39it/s]\n",
      "2022-04-27 23:14:05,332 INFO: Epoch:[009/100]\n",
      "2022-04-27 23:14:05,333 INFO: Train Loss:2.105 | Acc:0.7881 | F1:0.1374\n",
      "2022-04-27 23:14:25,386 INFO: val Loss:1.393 | Acc:0.8409 | F1:0.1529\n",
      "2022-04-27 23:14:26,097 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 23:15:56,431 INFO: Epoch:[010/100]\n",
      "2022-04-27 23:15:56,432 INFO: Train Loss:1.852 | Acc:0.8042 | F1:0.1405\n",
      "2022-04-27 23:16:15,964 INFO: val Loss:1.236 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:16:16,714 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.38it/s]\n",
      "2022-04-27 23:17:46,766 INFO: Epoch:[011/100]\n",
      "2022-04-27 23:17:46,767 INFO: Train Loss:1.648 | Acc:0.8235 | F1:0.1478\n",
      "2022-04-27 23:18:06,409 INFO: val Loss:1.024 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:18:07,125 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 23:19:37,516 INFO: Epoch:[012/100]\n",
      "2022-04-27 23:19:37,516 INFO: Train Loss:1.539 | Acc:0.8293 | F1:0.1513\n",
      "2022-04-27 23:19:57,213 INFO: val Loss:0.918 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:19:58,023 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 23:21:28,218 INFO: Epoch:[013/100]\n",
      "2022-04-27 23:21:28,219 INFO: Train Loss:1.441 | Acc:0.8328 | F1:0.1522\n",
      "2022-04-27 23:21:48,048 INFO: val Loss:0.895 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:21:48,791 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.35it/s]\n",
      "2022-04-27 23:23:19,792 INFO: Epoch:[014/100]\n",
      "2022-04-27 23:23:19,793 INFO: Train Loss:1.373 | Acc:0.8366 | F1:0.1539\n",
      "2022-04-27 23:23:39,378 INFO: val Loss:0.822 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:23:40,128 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 23:25:11,233 INFO: Epoch:[015/100]\n",
      "2022-04-27 23:25:11,234 INFO: Train Loss:1.226 | Acc:0.8364 | F1:0.1530\n",
      "2022-04-27 23:25:30,758 INFO: val Loss:0.766 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:25:31,640 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 23:27:04,248 INFO: Epoch:[016/100]\n",
      "2022-04-27 23:27:04,249 INFO: Train Loss:1.103 | Acc:0.8396 | F1:0.1544\n",
      "2022-04-27 23:27:23,715 INFO: val Loss:0.680 | Acc:0.8480 | F1:0.1561\n",
      "2022-04-27 23:27:24,437 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.35it/s]\n",
      "2022-04-27 23:28:55,562 INFO: Epoch:[017/100]\n",
      "2022-04-27 23:28:55,562 INFO: Train Loss:1.009 | Acc:0.8393 | F1:0.1586\n",
      "2022-04-27 23:29:15,251 INFO: val Loss:0.633 | Acc:0.8515 | F1:0.1678\n",
      "2022-04-27 23:29:15,980 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 214/214 [01:31<00:00,  2.34it/s]\n",
      "2022-04-27 23:30:47,628 INFO: Epoch:[018/100]\n",
      "2022-04-27 23:30:47,629 INFO: Train Loss:0.911 | Acc:0.8440 | F1:0.1677\n",
      "2022-04-27 23:31:07,720 INFO: val Loss:0.571 | Acc:0.8550 | F1:0.1872\n",
      "2022-04-27 23:31:08,419 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 23:32:38,554 INFO: Epoch:[019/100]\n",
      "2022-04-27 23:32:38,555 INFO: Train Loss:0.847 | Acc:0.8431 | F1:0.1748\n",
      "2022-04-27 23:32:58,499 INFO: val Loss:0.551 | Acc:0.8620 | F1:0.2019\n",
      "2022-04-27 23:32:59,200 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 214/214 [01:32<00:00,  2.31it/s]\n",
      "2022-04-27 23:34:31,959 INFO: Epoch:[020/100]\n",
      "2022-04-27 23:34:31,960 INFO: Train Loss:0.770 | Acc:0.8504 | F1:0.1800\n",
      "2022-04-27 23:34:52,058 INFO: val Loss:0.474 | Acc:0.8690 | F1:0.2288\n",
      "2022-04-27 23:34:52,759 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 214/214 [01:30<00:00,  2.37it/s]\n",
      "2022-04-27 23:36:23,208 INFO: Epoch:[021/100]\n",
      "2022-04-27 23:36:23,209 INFO: Train Loss:0.711 | Acc:0.8533 | F1:0.2181\n",
      "2022-04-27 23:36:42,619 INFO: val Loss:0.447 | Acc:0.8737 | F1:0.2308\n",
      "2022-04-27 23:36:43,344 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 214/214 [01:29<00:00,  2.38it/s]\n",
      "2022-04-27 23:38:13,218 INFO: Epoch:[022/100]\n",
      "2022-04-27 23:38:13,219 INFO: Train Loss:0.665 | Acc:0.8580 | F1:0.2261\n"
     ]
    }
   ],
   "source": [
    "args.step = 0\n",
    "models_path = []\n",
    "for s_fold in range(5): # 5fold\n",
    "    args.fold = s_fold\n",
    "    args.exp_num = str(s_fold)\n",
    "    save_path = main(args)\n",
    "    models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 288\n",
    "\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_path = ['./class_results/000', './class_results/001', './class_results/002', './class_results/003', './class_results/004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_040', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble_5fold(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pred = ensemble.argmax(axis=1).tolist()\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./open/train_df.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"label_result.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
