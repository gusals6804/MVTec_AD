{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "import torch.cuda.amp as amp\n",
    "from adamp import AdamP, SGDP\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  file_name       class state            label  encoder_label\n",
      "0           0  10000.png  transistor  good  transistor-good             72\n",
      "1           1  10001.png     capsule  good     capsule-good             15\n",
      "2           2  10002.png  transistor  good  transistor-good             72\n",
      "3           3  10003.png        wood  good        wood-good             76\n",
      "4           4  10004.png      bottle  good      bottle-good              3\n",
      "   index  file_name\n",
      "0      0  20000.png\n",
      "1      1  20001.png\n",
      "2      2  20002.png\n",
      "3      3  20003.png\n",
      "4      4  20004.png\n",
      "(13997, 6)\n",
      "(2154, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './open'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df_add_data.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hazelnut-good              391\n",
       "screw-good                 320\n",
       "carpet-good                280\n",
       "pill-good                  267\n",
       "grid-good                  264\n",
       "                          ... \n",
       "transistor-cut_lead         80\n",
       "transistor-damaged_case     80\n",
       "transistor-misplaced        80\n",
       "wood-color                  64\n",
       "toothbrush-good             60\n",
       "Name: label, Length: 88, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['transistor-good', 'capsule-good', 'wood-good', 'bottle-good',\n",
       "       'screw-good', 'cable-bent_wire', 'carpet-hole', 'hazelnut-good',\n",
       "       'pill-pill_type', 'cable-good', 'metal_nut-scratch', 'pill-good',\n",
       "       'screw-thread_side', 'zipper-fabric_border', 'leather-good',\n",
       "       'pill-scratch', 'toothbrush-good', 'hazelnut-crack',\n",
       "       'screw-manipulated_front', 'zipper-good', 'tile-good',\n",
       "       'carpet-good', 'metal_nut-good', 'bottle-contamination',\n",
       "       'grid-good', 'zipper-split_teeth', 'pill-crack', 'wood-combined',\n",
       "       'pill-color', 'screw-thread_top', 'cable-missing_cable',\n",
       "       'capsule-squeeze', 'zipper-rough', 'capsule-crack', 'capsule-poke',\n",
       "       'metal_nut-flip', 'carpet-metal_contamination', 'metal_nut-color',\n",
       "       'transistor-bent_lead', 'zipper-fabric_interior', 'leather-fold',\n",
       "       'tile-glue_strip', 'screw-scratch_neck', 'screw-scratch_head',\n",
       "       'hazelnut-cut', 'bottle-broken_large', 'bottle-broken_small',\n",
       "       'leather-cut', 'cable-cut_outer_insulation',\n",
       "       'zipper-squeezed_teeth', 'toothbrush-defective',\n",
       "       'cable-cut_inner_insulation', 'pill-contamination',\n",
       "       'cable-missing_wire', 'carpet-thread', 'grid-broken',\n",
       "       'pill-faulty_imprint', 'hazelnut-hole', 'leather-glue',\n",
       "       'leather-poke', 'transistor-damaged_case', 'wood-scratch',\n",
       "       'tile-gray_stroke', 'capsule-faulty_imprint', 'grid-glue',\n",
       "       'zipper-combined', 'carpet-color', 'grid-bent', 'pill-combined',\n",
       "       'hazelnut-print', 'cable-combined', 'capsule-scratch',\n",
       "       'metal_nut-bent', 'zipper-broken_teeth', 'tile-oil',\n",
       "       'transistor-misplaced', 'grid-thread', 'grid-metal_contamination',\n",
       "       'carpet-cut', 'wood-color', 'cable-cable_swap', 'tile-crack',\n",
       "       'leather-color', 'cable-poke_insulation', 'transistor-cut_lead',\n",
       "       'wood-hole', 'tile-rough', 'wood-liquid'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2,3\"  # Set the GPUs 2 and 3 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(train_df.encoder_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./open',\n",
    "     'Kfold':5,\n",
    "     'model_path':'label_results_coatnet_4/',\n",
    "     'image_type':'train_1024', \n",
    "     'class_num' : class_num,\n",
    "\n",
    "     # Model parameter settings\n",
    "     'model_name':'coatnet_4',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':224,\n",
    "     'batch_size':32,\n",
    "     'epochs':100,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-4,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'Reduce',\n",
    "     'warm_epoch':3,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-5,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience': 5,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':True,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "     \n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, p: float, degrees: int):\n",
    "        super(RandomRotation, self).__init__(degrees)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            fill = self.fill\n",
    "            if isinstance(img, Tensor):\n",
    "                if isinstance(fill, (int, float)):\n",
    "                    fill = [float(fill)] * F.get_image_num_channels(img)\n",
    "                else:\n",
    "                    fill = [float(f) for f in fill]\n",
    "            angle = self.get_params(self.degrees)\n",
    "\n",
    "            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.target = df['encoder_label'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = Image.open(opj('./open/train_add_data/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \n",
    "                                 pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    if ver == 2:\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.3),\n",
    "                transforms.RandomVerticalFlip(p=0.3),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "                transforms.RandomAffine((-20,20)),\n",
    "                RandomRotation(0.5, degrees=5),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "#                 transforms.CenterCrop(224), \n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return x + self.conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def coatnet_0():\n",
    "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_1():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_2():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [128, 128, 256, 512, 1026]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_3():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_4():\n",
    "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=88)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "#         self.model_ft = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "#             args.model_name,\n",
    "#             pretrained=True,\n",
    "#             num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "#         )\n",
    "\n",
    "        self.model_ft = coatnet_4()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "#         self.model_ft = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "#             args.model_name,\n",
    "#             pretrained=True,\n",
    "#             num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "#         )\n",
    "\n",
    "        self.model_ft = coatnet_4()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight: tensor([2.4437, 2.2216, 2.2216, 1.8708, 3.4911, 4.0729, 4.0729, 3.4911, 4.8875,\n",
      "        1.7455, 4.0729, 4.8875, 4.8875, 2.0365, 2.2216, 1.7854, 2.2216, 2.0365,\n",
      "        2.4437, 2.4437, 2.7153, 1.3964, 2.7153, 2.7153, 2.4437, 4.0729, 4.0729,\n",
      "        4.0729, 1.4811, 4.0729, 4.0729, 2.7153, 2.7153, 1.0000, 2.7153, 2.7153,\n",
      "        2.4437, 2.4437, 2.7153, 2.4437, 1.5959, 2.7153, 1.8798, 2.2216, 2.0365,\n",
      "        1.7773, 2.0365, 1.8798, 2.7153, 2.2216, 1.8798, 2.4437, 1.4644, 4.8875,\n",
      "        2.0365, 1.2219, 2.0365, 2.0365, 1.8798, 2.0365, 2.0365, 2.7153, 2.7153,\n",
      "        1.7000, 3.0547, 2.7153, 3.0547, 1.6292, 6.5167, 4.8875, 4.8875, 4.8875,\n",
      "        1.8357, 4.8875, 6.1094, 4.0729, 1.5830, 4.8875, 4.8875, 2.2216, 2.4437,\n",
      "        3.0547, 2.7153, 3.0547, 1.6292, 2.7153, 2.7153, 3.0547],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # weighted crossentropy loss를 위한 weight 계산 함수\n",
    "\n",
    "class_num = train_df.groupby([\"encoder_label\"])[\"encoder_label\"].count().tolist()\n",
    "class_weight = torch.tensor(np.max(class_num) / class_num).to(\"cuda\", dtype=torch.float)\n",
    "print(f\"class_weight: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://dacon.io/competitions/official/235585/codeshare/1796\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train_df_add_data.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "#         if args.image_type is not None:\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['encoder_label'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "\n",
    "        # Loss\n",
    "#         self.criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "        self.criterion = FocalLoss()\n",
    "#         self.criterion = CutMixCrossEntropyLoss(True)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "#         self.optimizer = Lookahead(torch.optim.Adam(self.model.parameters(), lr=args.initial_lr), k=5, alpha=0.5)\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=3, factor=0.2, mode=\"max\", verbose=True)\n",
    "            \n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        \n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "    \n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                    \n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./open/sample_submission.csv')\n",
    "df_train = pd.read_csv('./open/train_df_add_data.csv')\n",
    "df_test = pd.read_csv('./open/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'coatnet_4', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def make_pseudo_df(train_df, test_df, ensemble, step, threshold = 0.9, z_sample = 500): \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    test_df_copy['disease'] = np.nan\n",
    "    test_df_copy['disease_code'] = ensemble.argmax(axis=1)\n",
    "    pseudo_test_df = test_df_copy.iloc[np.where(ensemble > threshold)[0]].reset_index(drop=True)\n",
    "    z_idx  = pseudo_test_df[pseudo_test_df['disease_code'] == 0].sample(n=z_sample, random_state=42).index.tolist()\n",
    "    ot_idx = pseudo_test_df[pseudo_test_df['disease_code'].isin([*range(1,8)])].index.tolist()\n",
    "    pseudo_test_df = pseudo_test_df.iloc[z_idx + ot_idx]\n",
    "\n",
    "    train_df_copy = train_df_copy.append(pseudo_test_df, ignore_index=True).reset_index(drop=True) # reset_index\n",
    "    # print(f'Make train_{step}step.csv')\n",
    "    train_df_copy.to_csv(f'../data/train_{step}step.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 16:00:30,648 INFO: {'exp_num': '0', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_coatnet_4/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'coatnet_4', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 32, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11197\n",
      "Dataset size:2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [04:06<00:00,  1.42it/s]\n",
      "2022-05-03 16:04:39,390 INFO: Epoch:[001/100]\n",
      "2022-05-03 16:04:39,391 INFO: Train Loss:3.141 | Acc:0.1713 | F1:0.1090\n",
      "2022-05-03 16:05:08,072 INFO: val Loss:1.432 | Acc:0.2889 | F1:0.1433\n",
      "2022-05-03 16:05:12,832 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 350/350 [04:00<00:00,  1.45it/s]\n",
      "2022-05-03 16:09:13,851 INFO: Epoch:[002/100]\n",
      "2022-05-03 16:09:13,851 INFO: Train Loss:1.441 | Acc:0.3015 | F1:0.2541\n",
      "2022-05-03 16:09:44,549 INFO: val Loss:1.380 | Acc:0.3386 | F1:0.2081\n",
      "2022-05-03 16:09:49,507 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 350/350 [03:58<00:00,  1.47it/s]\n",
      "2022-05-03 16:13:47,752 INFO: Epoch:[003/100]\n",
      "2022-05-03 16:13:47,753 INFO: Train Loss:1.371 | Acc:0.3433 | F1:0.3110\n",
      "2022-05-03 16:14:18,550 INFO: val Loss:1.302 | Acc:0.4232 | F1:0.2960\n",
      "2022-05-03 16:14:23,680 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 350/350 [04:10<00:00,  1.40it/s]\n",
      "2022-05-03 16:18:33,783 INFO: Epoch:[004/100]\n",
      "2022-05-03 16:18:33,784 INFO: Train Loss:1.204 | Acc:0.3916 | F1:0.3598\n",
      "2022-05-03 16:19:03,627 INFO: val Loss:1.079 | Acc:0.4957 | F1:0.3896\n",
      "2022-05-03 16:19:08,792 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 350/350 [04:13<00:00,  1.38it/s]\n",
      "2022-05-03 16:23:21,841 INFO: Epoch:[005/100]\n",
      "2022-05-03 16:23:21,842 INFO: Train Loss:0.978 | Acc:0.4742 | F1:0.4485\n",
      "2022-05-03 16:23:51,474 INFO: val Loss:0.931 | Acc:0.5104 | F1:0.4216\n",
      "2022-05-03 16:23:56,471 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 350/350 [04:17<00:00,  1.36it/s]\n",
      "2022-05-03 16:28:14,249 INFO: Epoch:[006/100]\n",
      "2022-05-03 16:28:14,250 INFO: Train Loss:0.815 | Acc:0.5388 | F1:0.5232\n",
      "2022-05-03 16:28:43,157 INFO: val Loss:0.649 | Acc:0.5957 | F1:0.5278\n",
      "2022-05-03 16:28:48,137 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 350/350 [03:58<00:00,  1.47it/s]\n",
      "2022-05-03 16:32:46,295 INFO: Epoch:[007/100]\n",
      "2022-05-03 16:32:46,295 INFO: Train Loss:0.674 | Acc:0.5946 | F1:0.5862\n",
      "2022-05-03 16:33:14,852 INFO: val Loss:0.628 | Acc:0.6364 | F1:0.5930\n",
      "2022-05-03 16:33:19,743 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 350/350 [04:11<00:00,  1.39it/s]\n",
      "2022-05-03 16:37:31,648 INFO: Epoch:[008/100]\n",
      "2022-05-03 16:37:31,649 INFO: Train Loss:0.570 | Acc:0.6417 | F1:0.6375\n",
      "2022-05-03 16:38:01,621 INFO: val Loss:0.440 | Acc:0.7039 | F1:0.6838\n",
      "2022-05-03 16:38:06,851 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 350/350 [04:12<00:00,  1.39it/s]\n",
      "2022-05-03 16:42:19,261 INFO: Epoch:[009/100]\n",
      "2022-05-03 16:42:19,261 INFO: Train Loss:0.484 | Acc:0.6896 | F1:0.6954\n",
      "2022-05-03 16:42:49,667 INFO: val Loss:0.410 | Acc:0.7457 | F1:0.7345\n",
      "2022-05-03 16:42:54,507 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 350/350 [04:18<00:00,  1.36it/s]\n",
      "2022-05-03 16:47:12,544 INFO: Epoch:[010/100]\n",
      "2022-05-03 16:47:12,545 INFO: Train Loss:0.411 | Acc:0.7314 | F1:0.7407\n",
      "2022-05-03 16:47:42,303 INFO: val Loss:0.358 | Acc:0.7657 | F1:0.7602\n",
      "2022-05-03 16:47:47,475 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 350/350 [04:12<00:00,  1.39it/s]\n",
      "2022-05-03 16:51:59,860 INFO: Epoch:[011/100]\n",
      "2022-05-03 16:51:59,861 INFO: Train Loss:0.344 | Acc:0.7637 | F1:0.7739\n",
      "2022-05-03 16:52:30,322 INFO: val Loss:0.317 | Acc:0.7943 | F1:0.7814\n",
      "2022-05-03 16:52:35,072 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 350/350 [04:11<00:00,  1.39it/s]\n",
      "2022-05-03 16:56:46,642 INFO: Epoch:[012/100]\n",
      "2022-05-03 16:56:46,643 INFO: Train Loss:0.290 | Acc:0.7959 | F1:0.8047\n",
      "2022-05-03 16:57:15,004 INFO: val Loss:0.350 | Acc:0.7900 | F1:0.7900\n",
      "100%|██████████| 350/350 [04:07<00:00,  1.41it/s]\n",
      "2022-05-03 17:01:22,570 INFO: Epoch:[013/100]\n",
      "2022-05-03 17:01:22,571 INFO: Train Loss:0.256 | Acc:0.8171 | F1:0.8276\n",
      "2022-05-03 17:01:52,611 INFO: val Loss:0.203 | Acc:0.8668 | F1:0.8595\n",
      "2022-05-03 17:01:58,015 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 350/350 [04:10<00:00,  1.40it/s]\n",
      "2022-05-03 17:06:08,862 INFO: Epoch:[014/100]\n",
      "2022-05-03 17:06:08,863 INFO: Train Loss:0.223 | Acc:0.8426 | F1:0.8513\n",
      "2022-05-03 17:06:36,819 INFO: val Loss:0.152 | Acc:0.8779 | F1:0.8771\n",
      "2022-05-03 17:06:41,590 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 350/350 [04:04<00:00,  1.43it/s]\n",
      "2022-05-03 17:10:46,451 INFO: Epoch:[015/100]\n",
      "2022-05-03 17:10:46,451 INFO: Train Loss:0.187 | Acc:0.8631 | F1:0.8695\n",
      "2022-05-03 17:11:14,991 INFO: val Loss:0.176 | Acc:0.8729 | F1:0.8679\n",
      "100%|██████████| 350/350 [04:09<00:00,  1.40it/s]\n",
      "2022-05-03 17:15:24,124 INFO: Epoch:[016/100]\n",
      "2022-05-03 17:15:24,125 INFO: Train Loss:0.170 | Acc:0.8712 | F1:0.8785\n",
      "2022-05-03 17:15:53,556 INFO: val Loss:0.089 | Acc:0.9221 | F1:0.9265\n",
      "2022-05-03 17:15:58,597 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 350/350 [04:04<00:00,  1.43it/s]\n",
      "2022-05-03 17:20:03,382 INFO: Epoch:[017/100]\n",
      "2022-05-03 17:20:03,383 INFO: Train Loss:0.148 | Acc:0.8855 | F1:0.8930\n",
      "2022-05-03 17:20:31,963 INFO: val Loss:0.102 | Acc:0.9168 | F1:0.9155\n",
      "100%|██████████| 350/350 [03:57<00:00,  1.48it/s]\n",
      "2022-05-03 17:24:29,060 INFO: Epoch:[018/100]\n",
      "2022-05-03 17:24:29,061 INFO: Train Loss:0.138 | Acc:0.8958 | F1:0.9012\n",
      "2022-05-03 17:24:57,250 INFO: val Loss:0.093 | Acc:0.9168 | F1:0.9199\n",
      "100%|██████████| 350/350 [04:03<00:00,  1.44it/s]\n",
      "2022-05-03 17:29:00,347 INFO: Epoch:[019/100]\n",
      "2022-05-03 17:29:00,348 INFO: Train Loss:0.122 | Acc:0.9035 | F1:0.9094\n",
      "2022-05-03 17:29:30,224 INFO: val Loss:0.107 | Acc:0.9182 | F1:0.9171\n",
      "100%|██████████| 350/350 [04:06<00:00,  1.42it/s]\n",
      "2022-05-03 17:33:36,554 INFO: Epoch:[020/100]\n",
      "2022-05-03 17:33:36,554 INFO: Train Loss:0.119 | Acc:0.9069 | F1:0.9122\n",
      "2022-05-03 17:34:06,944 INFO: val Loss:0.064 | Acc:0.9482 | F1:0.9522\n",
      "2022-05-03 17:34:11,882 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 350/350 [04:01<00:00,  1.45it/s]\n",
      "2022-05-03 17:38:13,113 INFO: Epoch:[021/100]\n",
      "2022-05-03 17:38:13,113 INFO: Train Loss:0.100 | Acc:0.9189 | F1:0.9239\n",
      "2022-05-03 17:38:41,546 INFO: val Loss:0.066 | Acc:0.9425 | F1:0.9499\n",
      "100%|██████████| 350/350 [04:04<00:00,  1.43it/s]\n",
      "2022-05-03 17:42:46,255 INFO: Epoch:[022/100]\n",
      "2022-05-03 17:42:46,256 INFO: Train Loss:0.092 | Acc:0.9286 | F1:0.9326\n",
      "2022-05-03 17:43:15,283 INFO: val Loss:0.062 | Acc:0.9461 | F1:0.9458\n",
      "2022-05-03 17:43:20,223 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 350/350 [04:03<00:00,  1.44it/s]\n",
      "2022-05-03 17:47:23,608 INFO: Epoch:[023/100]\n",
      "2022-05-03 17:47:23,609 INFO: Train Loss:0.086 | Acc:0.9303 | F1:0.9343\n",
      "2022-05-03 17:47:52,656 INFO: val Loss:0.089 | Acc:0.9457 | F1:0.9445\n",
      "100%|██████████| 350/350 [04:07<00:00,  1.41it/s]\n",
      "2022-05-03 17:52:00,352 INFO: Epoch:[024/100]\n",
      "2022-05-03 17:52:00,353 INFO: Train Loss:0.080 | Acc:0.9347 | F1:0.9392\n",
      "2022-05-03 17:52:29,372 INFO: val Loss:0.083 | Acc:0.9332 | F1:0.9348\n",
      "100%|██████████| 350/350 [04:18<00:00,  1.35it/s]\n",
      "2022-05-03 17:56:47,822 INFO: Epoch:[025/100]\n",
      "2022-05-03 17:56:47,822 INFO: Train Loss:0.071 | Acc:0.9436 | F1:0.9472\n",
      "2022-05-03 17:57:17,619 INFO: val Loss:0.100 | Acc:0.9436 | F1:0.9437\n",
      "100%|██████████| 350/350 [04:15<00:00,  1.37it/s]\n",
      "2022-05-03 18:01:33,181 INFO: Epoch:[026/100]\n",
      "2022-05-03 18:01:33,182 INFO: Train Loss:0.079 | Acc:0.9430 | F1:0.9467\n",
      "2022-05-03 18:02:02,882 INFO: val Loss:0.055 | Acc:0.9539 | F1:0.9570\n",
      "2022-05-03 18:02:08,142 INFO: -----------------SAVE:26epoch----------------\n",
      "100%|██████████| 350/350 [04:14<00:00,  1.37it/s]\n",
      "2022-05-03 18:06:22,732 INFO: Epoch:[027/100]\n",
      "2022-05-03 18:06:22,733 INFO: Train Loss:0.066 | Acc:0.9457 | F1:0.9507\n",
      "2022-05-03 18:06:53,190 INFO: val Loss:0.039 | Acc:0.9636 | F1:0.9680\n",
      "2022-05-03 18:06:58,529 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 350/350 [04:11<00:00,  1.39it/s]\n",
      "2022-05-03 18:11:10,367 INFO: Epoch:[028/100]\n",
      "2022-05-03 18:11:10,368 INFO: Train Loss:0.061 | Acc:0.9513 | F1:0.9546\n",
      "2022-05-03 18:11:38,843 INFO: val Loss:0.068 | Acc:0.9543 | F1:0.9551\n",
      "100%|██████████| 350/350 [04:11<00:00,  1.39it/s]\n",
      "2022-05-03 18:15:50,703 INFO: Epoch:[029/100]\n",
      "2022-05-03 18:15:50,704 INFO: Train Loss:0.059 | Acc:0.9537 | F1:0.9583\n",
      "2022-05-03 18:16:20,873 INFO: val Loss:0.036 | Acc:0.9664 | F1:0.9702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 18:16:25,920 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 350/350 [04:12<00:00,  1.38it/s]\n",
      "2022-05-03 18:20:38,783 INFO: Epoch:[030/100]\n",
      "2022-05-03 18:20:38,783 INFO: Train Loss:0.055 | Acc:0.9571 | F1:0.9607\n",
      "2022-05-03 18:21:08,361 INFO: val Loss:0.042 | Acc:0.9629 | F1:0.9643\n",
      "100%|██████████| 350/350 [04:08<00:00,  1.41it/s]\n",
      "2022-05-03 18:25:17,123 INFO: Epoch:[031/100]\n",
      "2022-05-03 18:25:17,124 INFO: Train Loss:0.049 | Acc:0.9594 | F1:0.9626\n",
      "2022-05-03 18:25:46,427 INFO: val Loss:0.050 | Acc:0.9621 | F1:0.9646\n",
      "100%|██████████| 350/350 [04:06<00:00,  1.42it/s]\n",
      "2022-05-03 18:29:53,309 INFO: Epoch:[032/100]\n",
      "2022-05-03 18:29:53,310 INFO: Train Loss:0.052 | Acc:0.9570 | F1:0.9603\n",
      "2022-05-03 18:30:23,342 INFO: val Loss:0.032 | Acc:0.9675 | F1:0.9711\n",
      "2022-05-03 18:30:27,599 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 350/350 [04:04<00:00,  1.43it/s]\n",
      "2022-05-03 18:34:31,989 INFO: Epoch:[033/100]\n",
      "2022-05-03 18:34:31,990 INFO: Train Loss:0.048 | Acc:0.9624 | F1:0.9655\n",
      "2022-05-03 18:35:01,257 INFO: val Loss:0.036 | Acc:0.9664 | F1:0.9710\n",
      "100%|██████████| 350/350 [04:12<00:00,  1.39it/s]\n",
      "2022-05-03 18:39:13,611 INFO: Epoch:[034/100]\n",
      "2022-05-03 18:39:13,612 INFO: Train Loss:0.042 | Acc:0.9611 | F1:0.9647\n",
      "2022-05-03 18:39:44,548 INFO: val Loss:0.033 | Acc:0.9682 | F1:0.9702\n",
      "100%|██████████| 350/350 [04:17<00:00,  1.36it/s]\n",
      "2022-05-03 18:44:02,067 INFO: Epoch:[035/100]\n",
      "2022-05-03 18:44:02,068 INFO: Train Loss:0.043 | Acc:0.9684 | F1:0.9704\n",
      "2022-05-03 18:44:32,496 INFO: val Loss:0.057 | Acc:0.9575 | F1:0.9534\n",
      "100%|██████████| 350/350 [04:15<00:00,  1.37it/s]\n",
      "2022-05-03 18:48:48,230 INFO: Epoch:[036/100]\n",
      "2022-05-03 18:48:48,231 INFO: Train Loss:0.042 | Acc:0.9686 | F1:0.9714\n",
      "2022-05-03 18:49:18,219 INFO: val Loss:0.026 | Acc:0.9789 | F1:0.9826\n",
      "2022-05-03 18:49:23,656 INFO: -----------------SAVE:36epoch----------------\n",
      "100%|██████████| 350/350 [04:12<00:00,  1.38it/s]\n",
      "2022-05-03 18:53:36,609 INFO: Epoch:[037/100]\n",
      "2022-05-03 18:53:36,610 INFO: Train Loss:0.045 | Acc:0.9659 | F1:0.9705\n",
      "2022-05-03 18:54:06,432 INFO: val Loss:0.011 | Acc:0.9904 | F1:0.9903\n",
      "2022-05-03 18:54:11,838 INFO: -----------------SAVE:37epoch----------------\n",
      "100%|██████████| 350/350 [04:06<00:00,  1.42it/s]\n",
      "2022-05-03 18:58:18,108 INFO: Epoch:[038/100]\n",
      "2022-05-03 18:58:18,109 INFO: Train Loss:0.038 | Acc:0.9670 | F1:0.9700\n",
      "2022-05-03 18:58:47,184 INFO: val Loss:0.016 | Acc:0.9829 | F1:0.9847\n",
      "100%|██████████| 350/350 [04:14<00:00,  1.38it/s]\n",
      "2022-05-03 19:03:01,480 INFO: Epoch:[039/100]\n",
      "2022-05-03 19:03:01,481 INFO: Train Loss:0.042 | Acc:0.9698 | F1:0.9713\n",
      "2022-05-03 19:03:30,735 INFO: val Loss:0.020 | Acc:0.9854 | F1:0.9862\n",
      "100%|██████████| 350/350 [04:10<00:00,  1.40it/s]\n",
      "2022-05-03 19:07:40,842 INFO: Epoch:[040/100]\n",
      "2022-05-03 19:07:40,843 INFO: Train Loss:0.032 | Acc:0.9737 | F1:0.9769\n",
      "2022-05-03 19:08:09,960 INFO: val Loss:0.042 | Acc:0.9721 | F1:0.9734\n",
      "100%|██████████| 350/350 [03:57<00:00,  1.48it/s]\n",
      "2022-05-03 19:12:07,119 INFO: Epoch:[041/100]\n",
      "2022-05-03 19:12:07,120 INFO: Train Loss:0.038 | Acc:0.9719 | F1:0.9735\n",
      "2022-05-03 19:12:38,076 INFO: val Loss:0.024 | Acc:0.9832 | F1:0.9831\n",
      "100%|██████████| 350/350 [04:04<00:00,  1.43it/s]\n",
      "2022-05-03 19:16:43,008 INFO: Epoch:[042/100]\n",
      "2022-05-03 19:16:43,008 INFO: Train Loss:0.034 | Acc:0.9730 | F1:0.9755\n",
      "2022-05-03 19:17:12,198 INFO: val Loss:0.019 | Acc:0.9821 | F1:0.9827\n",
      "2022-05-03 19:17:12,200 INFO: \n",
      "Best Val Epoch:37 | Val Loss:0.0112 | Val Acc:0.9904 | Val F1:0.9903\n",
      "2022-05-03 19:17:12,201 INFO: Total Process time:196.663Minute\n",
      "2022-05-03 19:17:12,216 INFO: {'exp_num': '1', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results_coatnet_4/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'coatnet_4', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 32, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 0.0005, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'Reduce', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 5, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:11197\n",
      "Dataset size:2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [04:09<00:00,  1.40it/s]\n",
      "2022-05-03 19:21:23,792 INFO: Epoch:[001/100]\n",
      "2022-05-03 19:21:23,793 INFO: Train Loss:3.141 | Acc:0.1674 | F1:0.1082\n",
      "2022-05-03 19:21:52,517 INFO: val Loss:1.423 | Acc:0.3300 | F1:0.1777\n",
      "2022-05-03 19:21:57,679 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 350/350 [03:56<00:00,  1.48it/s]\n",
      "2022-05-03 19:25:54,569 INFO: Epoch:[002/100]\n",
      "2022-05-03 19:25:54,569 INFO: Train Loss:1.461 | Acc:0.2944 | F1:0.2483\n",
      "2022-05-03 19:26:22,646 INFO: val Loss:1.234 | Acc:0.4054 | F1:0.2632\n",
      "2022-05-03 19:26:27,474 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 350/350 [04:02<00:00,  1.44it/s]\n",
      "2022-05-03 19:30:30,321 INFO: Epoch:[003/100]\n",
      "2022-05-03 19:30:30,322 INFO: Train Loss:1.362 | Acc:0.3464 | F1:0.3136\n",
      "2022-05-03 19:30:59,688 INFO: val Loss:1.302 | Acc:0.4239 | F1:0.2842\n",
      "100%|██████████| 350/350 [04:05<00:00,  1.42it/s]\n",
      "2022-05-03 19:35:05,373 INFO: Epoch:[004/100]\n",
      "2022-05-03 19:35:05,374 INFO: Train Loss:1.202 | Acc:0.3918 | F1:0.3621\n",
      "2022-05-03 19:35:35,063 INFO: val Loss:1.137 | Acc:0.4421 | F1:0.3217\n",
      "2022-05-03 19:35:40,019 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 350/350 [04:00<00:00,  1.46it/s]\n",
      "2022-05-03 19:39:40,052 INFO: Epoch:[005/100]\n",
      "2022-05-03 19:39:40,052 INFO: Train Loss:0.992 | Acc:0.4653 | F1:0.4431\n",
      "2022-05-03 19:40:08,445 INFO: val Loss:0.881 | Acc:0.5407 | F1:0.4484\n",
      "2022-05-03 19:40:13,376 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 350/350 [04:11<00:00,  1.39it/s]\n",
      "2022-05-03 19:44:25,027 INFO: Epoch:[006/100]\n",
      "2022-05-03 19:44:25,028 INFO: Train Loss:0.803 | Acc:0.5355 | F1:0.5200\n",
      "2022-05-03 19:44:55,050 INFO: val Loss:0.632 | Acc:0.6289 | F1:0.5785\n"
     ]
    }
   ],
   "source": [
    "args.step = 0\n",
    "models_path = []\n",
    "for s_fold in range(5): # 5fold\n",
    "    args.fold = s_fold\n",
    "    args.exp_num = str(s_fold)\n",
    "    save_path = main(args)\n",
    "    models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_path = ['./class_results/000', './class_results/001', './class_results/002', './class_results/003', './class_results/004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ensemble_5fold(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pred = ensemble.argmax(axis=1).tolist()\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./open/train_df_add_data.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./submission/label_result_add_0503_coatnet_4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
