{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import easydict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as F\n",
    "import torch.cuda.amp as amp\n",
    "from adamp import AdamP, SGDP\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  file_name       class state            label  label2  class2  \\\n",
      "0           0  10000.png  transistor  good  transistor-good      72      12   \n",
      "1           1  10001.png     capsule  good     capsule-good      15       2   \n",
      "2           2  10002.png  transistor  good  transistor-good      72      12   \n",
      "3           3  10003.png        wood  good        wood-good      76      13   \n",
      "4           4  10004.png      bottle  good      bottle-good       3       0   \n",
      "\n",
      "   state2  \n",
      "0      25  \n",
      "1      25  \n",
      "2      25  \n",
      "3      25  \n",
      "4      25  \n",
      "   index  file_name\n",
      "0      0  20000.png\n",
      "1      1  20001.png\n",
      "2      2  20002.png\n",
      "3      3  20003.png\n",
      "4      4  20004.png\n",
      "(4925, 8)\n",
      "(2154, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = './open'\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train_df_add2.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_DIR, 'test_df.csv'))\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['transistor-good', 'capsule-good', 'wood-good', 'bottle-good',\n",
       "       'screw-good', 'cable-bent_wire', 'carpet-hole', 'hazelnut-good',\n",
       "       'pill-pill_type', 'cable-good', 'metal_nut-scratch', 'pill-good',\n",
       "       'screw-thread_side', 'zipper-fabric_border', 'leather-good',\n",
       "       'pill-scratch', 'toothbrush-good', 'hazelnut-crack',\n",
       "       'screw-manipulated_front', 'zipper-good', 'tile-good',\n",
       "       'carpet-good', 'metal_nut-good', 'bottle-contamination',\n",
       "       'grid-good', 'zipper-split_teeth', 'pill-crack', 'wood-combined',\n",
       "       'pill-color', 'screw-thread_top', 'cable-missing_cable',\n",
       "       'capsule-squeeze', 'zipper-rough', 'capsule-crack', 'capsule-poke',\n",
       "       'metal_nut-flip', 'carpet-metal_contamination', 'metal_nut-color',\n",
       "       'transistor-bent_lead', 'zipper-fabric_interior', 'leather-fold',\n",
       "       'tile-glue_strip', 'screw-scratch_neck', 'screw-scratch_head',\n",
       "       'hazelnut-cut', 'bottle-broken_large', 'bottle-broken_small',\n",
       "       'leather-cut', 'cable-cut_outer_insulation',\n",
       "       'zipper-squeezed_teeth', 'toothbrush-defective',\n",
       "       'cable-cut_inner_insulation', 'pill-contamination',\n",
       "       'cable-missing_wire', 'carpet-thread', 'grid-broken',\n",
       "       'pill-faulty_imprint', 'hazelnut-hole', 'leather-glue',\n",
       "       'leather-poke', 'transistor-damaged_case', 'wood-scratch',\n",
       "       'tile-gray_stroke', 'capsule-faulty_imprint', 'grid-glue',\n",
       "       'zipper-combined', 'carpet-color', 'grid-bent', 'pill-combined',\n",
       "       'hazelnut-print', 'cable-combined', 'capsule-scratch',\n",
       "       'metal_nut-bent', 'zipper-broken_teeth', 'tile-oil',\n",
       "       'transistor-misplaced', 'grid-thread', 'grid-metal_contamination',\n",
       "       'carpet-cut', 'wood-color', 'cable-cable_swap', 'tile-crack',\n",
       "       'leather-color', 'cable-poke_insulation', 'transistor-cut_lead',\n",
       "       'wood-hole', 'tile-rough', 'wood-liquid'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(img, angle):\n",
    "    angle = int(random.uniform(-angle, angle))\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((int(w/2), int(h/2)), angle, 1)\n",
    "    img = cv2.warpAffine(img, M, (w, h)) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# oslabel = list(train_df['label'].unique())\n",
    "\n",
    "# for label in tqdm(oslabel):\n",
    "#     if 'good' not in label:\n",
    "#         print(label)\n",
    "#         idx = 0\n",
    "#         one_sample = train_df[train_df['label'] == label].reset_index(drop=True)\n",
    "#         images_list = natsorted(one_sample['file_name'])\n",
    "#         print(images_list)\n",
    "#         for _, image_name in enumerate(images_list):\n",
    "#             image = np.array(Image.open(opj('./open/train_add/', image_name)).convert('RGB'))\n",
    "#             image = cv2.resize(image, dsize=(1024, 1024))\n",
    "#             aug_img = rotation(image, 30)\n",
    "#             save_path = opj('./open/train_add', f'{label}_{idx}.png')\n",
    "#             save_name = f'{label}_{idx}.png'\n",
    "#             idx += 1\n",
    "#             cv2.imwrite(save_path, aug_img)\n",
    "#             train_df.loc[len(train_df)] = [save_name] + one_sample.iloc[0][1:].values.tolist()\n",
    "\n",
    "# train_df.to_csv('./open/train_df_add.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = pd.read_csv(\"./open/train_df_add.csv\")\n",
    "\n",
    "# train_labels = train_y[\"label\"]\n",
    "\n",
    "# label_unique = sorted(np.unique(train_labels))\n",
    "# label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "# train_labels = [label_unique[k] for k in train_labels]\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['label2'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./open/train_df_add2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPUs 2 and 3 to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = len(train_df.label2.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     'data_path':'./open',\n",
    "     'Kfold':5,\n",
    "     'model_path':'label_results/',\n",
    "     'image_type':'train_1024', \n",
    "     'class_num' : class_num,\n",
    "\n",
    "     # Model parameter settings\n",
    "     'model_name':'regnety_040',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':512,\n",
    "     'batch_size':16,\n",
    "     'epochs':100,\n",
    "     'optimizer':'Lamb',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':3,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-5,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience': 7,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':True,\n",
    "     'logging':False,\n",
    "     'num_workers':4,\n",
    "     'seed':42\n",
    "     \n",
    "     \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, p: float, degrees: int):\n",
    "        super(RandomRotation, self).__init__(degrees)\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            fill = self.fill\n",
    "            if isinstance(img, Tensor):\n",
    "                if isinstance(fill, (int, float)):\n",
    "                    fill = [float(fill)] * F.get_image_num_channels(img)\n",
    "                else:\n",
    "                    fill = [float(f) for f in fill]\n",
    "            angle = self.get_params(self.degrees)\n",
    "\n",
    "            img = F.rotate(img, angle, self.resample, self.expand, self.center, fill)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.target = df['label2'].values \n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(opj('./open/train_add/', self.img_path[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        \n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "        \n",
    "#         image = Image.open(opj('./open/train/', self.img_path[idx])).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "#         augmentation = random.randint(0,2)\n",
    "#             if augmentation==1:\n",
    "#                 img = img[::-1].copy()\n",
    "#             elif augmentation==2:\n",
    "#                 img = img[:,::-1].copy()\n",
    "#         img = transforms.ToTensor()(img)\n",
    "#         target = self.target[idx]\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.img_path = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.img_path)}')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = cv2.imread(opj('./open/test/', self.img_path[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "#         image = Image.open(opj('./open/test/', self.img_path[idx])).convert('RGB')\n",
    "#         image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "def get_loader(df, phase: str, batch_size, shuffle,\n",
    "               num_workers, transform):\n",
    "    if phase == 'test':\n",
    "        dataset = Test_dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "    else:\n",
    "        dataset = Train_Dataset(df, transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \n",
    "                                 pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver==1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "#                 transforms.ToTensor(),\n",
    "                \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "    if ver == 2:\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "\n",
    "                transforms.RandomHorizontalFlip(p=0.3),\n",
    "                transforms.RandomVerticalFlip(p=0.3),\n",
    "#                 transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "#                 transforms.RandomAffine((-20, 20)),\n",
    "#                 RandomRotation(0.7, degrees=5),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "#                 transforms.ToTensor(), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "            args.model_name,\n",
    "            pretrained=True,\n",
    "            num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "        )\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.model_ft = timm.create_model( # timm ImageNet pre-trained 모델 load\n",
    "            encoder_name,\n",
    "            pretrained=True,\n",
    "            num_classes = 88, drop_path_rate=args.drop_path_rate\n",
    "        )\n",
    "\n",
    "#         self.model_ft = coatnet_3()\n",
    "#         num_ftrs = self.model_ft.fc.in_features\n",
    "#         self.model_ft.fc = nn.Linear(num_ftrs, args.class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model_ft(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight: tensor([19.5500, 17.7727, 17.7727,  1.8708, 27.9286, 32.5833, 32.5833, 27.9286,\n",
      "        39.1000,  1.7455, 32.5833, 39.1000, 39.1000, 16.2917, 17.7727,  1.7854,\n",
      "        17.7727, 16.2917, 19.5500, 19.5500, 21.7222,  1.3964, 21.7222, 21.7222,\n",
      "        19.5500, 32.5833, 32.5833, 32.5833,  1.4811, 32.5833, 32.5833, 21.7222,\n",
      "        21.7222,  1.0000, 21.7222, 21.7222, 19.5500, 19.5500, 21.7222, 19.5500,\n",
      "         1.5959, 21.7222, 15.0385, 17.7727, 16.2917,  1.7773, 16.2917, 15.0385,\n",
      "        21.7222, 17.7727, 15.0385, 19.5500,  1.4644, 39.1000, 16.2917,  1.2219,\n",
      "        16.2917, 16.2917, 15.0385, 16.2917, 16.2917, 21.7222, 21.7222,  1.7000,\n",
      "        24.4375, 21.7222, 24.4375, 13.0333,  6.5167, 39.1000, 39.1000, 39.1000,\n",
      "         1.8357, 39.1000, 48.8750, 32.5833,  1.5830, 39.1000, 39.1000, 17.7727,\n",
      "        19.5500, 24.4375, 21.7222, 24.4375,  1.6292, 21.7222, 21.7222, 24.4375],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # weighted crossentropy loss를 위한 weight 계산 함수\n",
    "# def get_class_weight():\n",
    "#     return 1 / train_df['label2'].value_counts().sort_index().values\n",
    "\n",
    "# class_weight = get_class_weight()\n",
    "\n",
    "\n",
    "class_num = train_df.groupby([\"label2\"])[\"label2\"].count().tolist()\n",
    "class_weight = torch.tensor(np.max(class_num) / class_num).to(\"cuda\", dtype=torch.float)\n",
    "print(f\"class_weight: {class_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from torch.optim import Optimizer\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group[\"counter\"] = 0\n",
    "    \n",
    "    def update(self, group):\n",
    "        for fast in group[\"params\"]:\n",
    "            param_state = self.state[fast]\n",
    "            if \"slow_param\" not in param_state:\n",
    "                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n",
    "                param_state[\"slow_param\"].copy_(fast.data)\n",
    "            slow = param_state[\"slow_param\"]\n",
    "            slow += (fast.data - slow) * self.alpha\n",
    "            fast.data.copy_(slow)\n",
    "    \n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            if group[\"counter\"] == 0:\n",
    "                self.update(group)\n",
    "            group[\"counter\"] += 1\n",
    "            if group[\"counter\"] >= self.k:\n",
    "                group[\"counter\"] = 0\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict[\"state\"]\n",
    "        param_groups = fast_state_dict[\"param_groups\"]\n",
    "        return {\n",
    "            \"fast_state\": fast_state,\n",
    "            \"slow_state\": slow_state,\n",
    "            \"param_groups\": param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        slow_state_dict = {\n",
    "            \"state\": state_dict[\"slow_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        fast_state_dict = {\n",
    "            \"state\": state_dict[\"fast_state\"],\n",
    "            \"param_groups\": state_dict[\"param_groups\"],\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "\n",
    "    def add_param_group(self, param_group):\n",
    "        param_group[\"counter\"] = 0\n",
    "        self.optimizer.add_param_group(param_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://dacon.io/competitions/official/235585/codeshare/1796\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "        if args.step == 0 :\n",
    "            df_train = pd.read_csv(opj(args.data_path, 'train_df_add2.csv'))\n",
    "        else :\n",
    "            df_train = pd.read_csv(opj(args.data_path, f'train_{args.step}step.csv'))\n",
    "\n",
    "#         if args.image_type is not None:\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('train_imgs', args.image_type))\n",
    "#             df_train['img_path'] = df_train['img_path'].apply(lambda x:x.replace('test_imgs', 'test_1024'))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label2'])):\n",
    "            df_train.loc[val_idx, 'fold'] = fold\n",
    "        val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "        df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "        df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "\n",
    "        # TrainLoader\n",
    "        self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "#         self.criterion = FocalLoss()\n",
    "#         self.criterion = CutMixCrossEntropyLoss(True)\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "#         self.optimizer = Lookahead(torch.optim.Adam(self.model.parameters(), lr=args.initial_lr), k=5, alpha=0.5)\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr)\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5, mode=\"max\", verbose=True)\n",
    "            \n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        \n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "    \n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                    \n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sub = pd.read_csv('./open/sample_submission.csv')\n",
    "df_train = pd.read_csv('./open/train_df_add2.csv')\n",
    "df_test = pd.read_csv('./open/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_040', test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "def make_pseudo_df(train_df, test_df, ensemble, step, threshold = 0.9, z_sample = 500): \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "\n",
    "    test_df_copy['disease'] = np.nan\n",
    "    test_df_copy['disease_code'] = ensemble.argmax(axis=1)\n",
    "    pseudo_test_df = test_df_copy.iloc[np.where(ensemble > threshold)[0]].reset_index(drop=True)\n",
    "    z_idx  = pseudo_test_df[pseudo_test_df['disease_code'] == 0].sample(n=z_sample, random_state=42).index.tolist()\n",
    "    ot_idx = pseudo_test_df[pseudo_test_df['disease_code'].isin([*range(1,8)])].index.tolist()\n",
    "    pseudo_test_df = pseudo_test_df.iloc[z_idx + ot_idx]\n",
    "\n",
    "    train_df_copy = train_df_copy.append(pseudo_test_df, ignore_index=True).reset_index(drop=True) # reset_index\n",
    "    # print(f'Make train_{step}step.csv')\n",
    "    train_df_copy.to_csv(f'../data/train_{step}step.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 16:47:55,104 INFO: {'exp_num': '0', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3940\n",
      "Dataset size:985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 16:47:55,425 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 247/247 [01:29<00:00,  2.74it/s]\n",
      "2022-04-29 16:49:25,624 INFO: Epoch:[001/100]\n",
      "2022-04-29 16:49:25,624 INFO: Train Loss:4.503 | Acc:0.0129 | F1:0.0055\n",
      "2022-04-29 16:49:36,117 INFO: val Loss:4.491 | Acc:0.0497 | F1:0.0078\n",
      "2022-04-29 16:49:36,785 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 16:50:57,560 INFO: Epoch:[002/100]\n",
      "2022-04-29 16:50:57,560 INFO: Train Loss:4.510 | Acc:0.0119 | F1:0.0043\n",
      "2022-04-29 16:51:07,803 INFO: val Loss:4.474 | Acc:0.0487 | F1:0.0080\n",
      "2022-04-29 16:51:08,441 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 16:52:31,657 INFO: Epoch:[003/100]\n",
      "2022-04-29 16:52:31,657 INFO: Train Loss:4.499 | Acc:0.0119 | F1:0.0045\n",
      "2022-04-29 16:52:42,005 INFO: val Loss:4.456 | Acc:0.0518 | F1:0.0118\n",
      "2022-04-29 16:52:42,611 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 16:54:05,385 INFO: Epoch:[004/100]\n",
      "2022-04-29 16:54:05,386 INFO: Train Loss:4.461 | Acc:0.0244 | F1:0.0072\n",
      "2022-04-29 16:54:15,778 INFO: val Loss:4.315 | Acc:0.0650 | F1:0.0174\n",
      "2022-04-29 16:54:16,369 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 16:55:39,883 INFO: Epoch:[005/100]\n",
      "2022-04-29 16:55:39,884 INFO: Train Loss:4.371 | Acc:0.0655 | F1:0.0218\n",
      "2022-04-29 16:55:50,015 INFO: val Loss:4.152 | Acc:0.2751 | F1:0.0954\n",
      "2022-04-29 16:55:50,568 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.07it/s]\n",
      "2022-04-29 16:57:11,150 INFO: Epoch:[006/100]\n",
      "2022-04-29 16:57:11,151 INFO: Train Loss:4.271 | Acc:0.1990 | F1:0.0643\n",
      "2022-04-29 16:57:21,409 INFO: val Loss:3.986 | Acc:0.5076 | F1:0.1480\n",
      "2022-04-29 16:57:22,015 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 16:58:45,631 INFO: Epoch:[007/100]\n",
      "2022-04-29 16:58:45,632 INFO: Train Loss:4.127 | Acc:0.3685 | F1:0.1067\n",
      "2022-04-29 16:58:55,810 INFO: val Loss:3.747 | Acc:0.7015 | F1:0.1970\n",
      "2022-04-29 16:58:56,423 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 17:00:20,524 INFO: Epoch:[008/100]\n",
      "2022-04-29 17:00:20,525 INFO: Train Loss:3.971 | Acc:0.5246 | F1:0.1427\n",
      "2022-04-29 17:00:30,895 INFO: val Loss:3.519 | Acc:0.7381 | F1:0.1761\n",
      "2022-04-29 17:00:31,572 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 17:01:55,222 INFO: Epoch:[009/100]\n",
      "2022-04-29 17:01:55,223 INFO: Train Loss:3.778 | Acc:0.6378 | F1:0.1599\n",
      "2022-04-29 17:02:05,337 INFO: val Loss:3.274 | Acc:0.7391 | F1:0.1757\n",
      "2022-04-29 17:02:05,953 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 17:03:27,422 INFO: Epoch:[010/100]\n",
      "2022-04-29 17:03:27,423 INFO: Train Loss:3.529 | Acc:0.6690 | F1:0.1664\n",
      "2022-04-29 17:03:37,746 INFO: val Loss:3.071 | Acc:0.7472 | F1:0.1697\n",
      "2022-04-29 17:03:38,277 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 17:05:01,945 INFO: Epoch:[011/100]\n",
      "2022-04-29 17:05:01,945 INFO: Train Loss:3.296 | Acc:0.6802 | F1:0.1844\n",
      "2022-04-29 17:05:12,395 INFO: val Loss:2.814 | Acc:0.7086 | F1:0.1705\n",
      "2022-04-29 17:05:12,992 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 17:06:36,097 INFO: Epoch:[012/100]\n",
      "2022-04-29 17:06:36,097 INFO: Train Loss:3.081 | Acc:0.6736 | F1:0.1995\n",
      "2022-04-29 17:06:46,630 INFO: val Loss:2.502 | Acc:0.6548 | F1:0.1756\n",
      "2022-04-29 17:06:47,292 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:08:10,887 INFO: Epoch:[013/100]\n",
      "2022-04-29 17:08:10,888 INFO: Train Loss:2.876 | Acc:0.6503 | F1:0.2031\n",
      "2022-04-29 17:08:21,261 INFO: val Loss:2.256 | Acc:0.7137 | F1:0.1944\n",
      "2022-04-29 17:08:21,893 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 17:09:44,803 INFO: Epoch:[014/100]\n",
      "2022-04-29 17:09:44,804 INFO: Train Loss:2.649 | Acc:0.6421 | F1:0.2224\n",
      "2022-04-29 17:09:55,223 INFO: val Loss:2.181 | Acc:0.6670 | F1:0.2134\n",
      "2022-04-29 17:09:55,842 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 17:11:17,445 INFO: Epoch:[015/100]\n",
      "2022-04-29 17:11:17,446 INFO: Train Loss:2.533 | Acc:0.6266 | F1:0.2311\n",
      "2022-04-29 17:11:27,712 INFO: val Loss:2.034 | Acc:0.7117 | F1:0.2302\n",
      "2022-04-29 17:11:28,312 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.01it/s]\n",
      "2022-04-29 17:12:50,432 INFO: Epoch:[016/100]\n",
      "2022-04-29 17:12:50,432 INFO: Train Loss:2.383 | Acc:0.6360 | F1:0.2562\n",
      "2022-04-29 17:13:00,806 INFO: val Loss:1.920 | Acc:0.6467 | F1:0.2237\n",
      "2022-04-29 17:13:01,405 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:14:24,889 INFO: Epoch:[017/100]\n",
      "2022-04-29 17:14:24,890 INFO: Train Loss:2.302 | Acc:0.6206 | F1:0.2536\n",
      "2022-04-29 17:14:35,063 INFO: val Loss:1.840 | Acc:0.6284 | F1:0.2444\n",
      "2022-04-29 17:14:35,673 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 17:15:57,427 INFO: Epoch:[018/100]\n",
      "2022-04-29 17:15:57,427 INFO: Train Loss:2.201 | Acc:0.6266 | F1:0.2624\n",
      "2022-04-29 17:16:07,624 INFO: val Loss:1.728 | Acc:0.6924 | F1:0.2520\n",
      "2022-04-29 17:16:08,222 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 17:17:29,649 INFO: Epoch:[019/100]\n",
      "2022-04-29 17:17:29,650 INFO: Train Loss:2.102 | Acc:0.6541 | F1:0.2834\n",
      "2022-04-29 17:17:40,056 INFO: val Loss:1.699 | Acc:0.6142 | F1:0.2570\n",
      "2022-04-29 17:17:40,650 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 17:19:03,753 INFO: Epoch:[020/100]\n",
      "2022-04-29 17:19:03,753 INFO: Train Loss:2.043 | Acc:0.6660 | F1:0.3114\n",
      "2022-04-29 17:19:14,102 INFO: val Loss:1.604 | Acc:0.6497 | F1:0.2778\n",
      "2022-04-29 17:19:14,778 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:20:38,293 INFO: Epoch:[021/100]\n",
      "2022-04-29 17:20:38,293 INFO: Train Loss:1.956 | Acc:0.6561 | F1:0.3193\n",
      "2022-04-29 17:20:48,821 INFO: val Loss:1.517 | Acc:0.6873 | F1:0.2910\n",
      "2022-04-29 17:20:49,440 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 17:22:13,278 INFO: Epoch:[022/100]\n",
      "2022-04-29 17:22:13,279 INFO: Train Loss:1.849 | Acc:0.6685 | F1:0.3515\n",
      "2022-04-29 17:22:23,608 INFO: val Loss:1.451 | Acc:0.7574 | F1:0.3646\n",
      "2022-04-29 17:22:24,212 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 17:23:47,079 INFO: Epoch:[023/100]\n",
      "2022-04-29 17:23:47,079 INFO: Train Loss:1.772 | Acc:0.6629 | F1:0.3755\n",
      "2022-04-29 17:23:57,403 INFO: val Loss:1.367 | Acc:0.7574 | F1:0.4211\n",
      "2022-04-29 17:23:57,999 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 17:25:21,057 INFO: Epoch:[024/100]\n",
      "2022-04-29 17:25:21,058 INFO: Train Loss:1.694 | Acc:0.7063 | F1:0.3975\n",
      "2022-04-29 17:25:31,339 INFO: val Loss:1.293 | Acc:0.7320 | F1:0.3741\n",
      "2022-04-29 17:25:31,947 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 17:26:52,582 INFO: Epoch:[025/100]\n",
      "2022-04-29 17:26:52,583 INFO: Train Loss:1.656 | Acc:0.6947 | F1:0.4064\n",
      "2022-04-29 17:27:03,071 INFO: val Loss:1.356 | Acc:0.7117 | F1:0.3853\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 17:28:24,484 INFO: Epoch:[026/100]\n",
      "2022-04-29 17:28:24,485 INFO: Train Loss:1.551 | Acc:0.7063 | F1:0.4367\n",
      "2022-04-29 17:28:34,879 INFO: val Loss:1.216 | Acc:0.7411 | F1:0.4306\n",
      "2022-04-29 17:28:35,488 INFO: -----------------SAVE:26epoch----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 17:29:58,744 INFO: Epoch:[027/100]\n",
      "2022-04-29 17:29:58,745 INFO: Train Loss:1.439 | Acc:0.7292 | F1:0.4633\n",
      "2022-04-29 17:30:09,116 INFO: val Loss:1.125 | Acc:0.7503 | F1:0.4346\n",
      "2022-04-29 17:30:09,728 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 17:31:30,489 INFO: Epoch:[028/100]\n",
      "2022-04-29 17:31:30,490 INFO: Train Loss:1.400 | Acc:0.7299 | F1:0.4825\n",
      "2022-04-29 17:31:40,754 INFO: val Loss:1.098 | Acc:0.7168 | F1:0.4822\n",
      "2022-04-29 17:31:41,359 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 17:33:02,116 INFO: Epoch:[029/100]\n",
      "2022-04-29 17:33:02,116 INFO: Train Loss:1.350 | Acc:0.7234 | F1:0.4917\n",
      "2022-04-29 17:33:12,373 INFO: val Loss:1.039 | Acc:0.8335 | F1:0.5153\n",
      "2022-04-29 17:33:12,968 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.05it/s]\n",
      "2022-04-29 17:34:33,948 INFO: Epoch:[030/100]\n",
      "2022-04-29 17:34:33,948 INFO: Train Loss:1.241 | Acc:0.7602 | F1:0.5452\n",
      "2022-04-29 17:34:44,268 INFO: val Loss:0.991 | Acc:0.7868 | F1:0.5080\n",
      "2022-04-29 17:34:44,876 INFO: -----------------SAVE:30epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 17:36:06,379 INFO: Epoch:[031/100]\n",
      "2022-04-29 17:36:06,380 INFO: Train Loss:1.190 | Acc:0.7614 | F1:0.5625\n",
      "2022-04-29 17:36:16,574 INFO: val Loss:1.012 | Acc:0.8335 | F1:0.5586\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 17:37:40,952 INFO: Epoch:[032/100]\n",
      "2022-04-29 17:37:40,953 INFO: Train Loss:1.134 | Acc:0.7855 | F1:0.5869\n",
      "2022-04-29 17:37:51,303 INFO: val Loss:0.915 | Acc:0.8355 | F1:0.5740\n",
      "2022-04-29 17:37:51,911 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:39:15,484 INFO: Epoch:[033/100]\n",
      "2022-04-29 17:39:15,485 INFO: Train Loss:1.057 | Acc:0.7741 | F1:0.5965\n",
      "2022-04-29 17:39:25,768 INFO: val Loss:0.909 | Acc:0.8487 | F1:0.5904\n",
      "2022-04-29 17:39:26,369 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 17:40:49,469 INFO: Epoch:[034/100]\n",
      "2022-04-29 17:40:49,469 INFO: Train Loss:1.000 | Acc:0.7992 | F1:0.6288\n",
      "2022-04-29 17:40:59,940 INFO: val Loss:0.877 | Acc:0.8457 | F1:0.6157\n",
      "2022-04-29 17:41:00,551 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 17:42:23,108 INFO: Epoch:[035/100]\n",
      "2022-04-29 17:42:23,109 INFO: Train Loss:0.956 | Acc:0.7959 | F1:0.6508\n",
      "2022-04-29 17:42:33,251 INFO: val Loss:0.810 | Acc:0.8873 | F1:0.6629\n",
      "2022-04-29 17:42:33,856 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 17:43:56,602 INFO: Epoch:[036/100]\n",
      "2022-04-29 17:43:56,603 INFO: Train Loss:0.865 | Acc:0.8305 | F1:0.6807\n",
      "2022-04-29 17:44:06,865 INFO: val Loss:0.859 | Acc:0.8223 | F1:0.6254\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:45:30,324 INFO: Epoch:[037/100]\n",
      "2022-04-29 17:45:30,325 INFO: Train Loss:0.840 | Acc:0.8241 | F1:0.6885\n",
      "2022-04-29 17:45:40,573 INFO: val Loss:0.730 | Acc:0.8376 | F1:0.7061\n",
      "2022-04-29 17:45:41,187 INFO: -----------------SAVE:37epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 17:47:02,385 INFO: Epoch:[038/100]\n",
      "2022-04-29 17:47:02,385 INFO: Train Loss:0.742 | Acc:0.8398 | F1:0.7189\n",
      "2022-04-29 17:47:12,757 INFO: val Loss:0.642 | Acc:0.8650 | F1:0.7049\n",
      "2022-04-29 17:47:13,352 INFO: -----------------SAVE:38epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 17:48:36,279 INFO: Epoch:[039/100]\n",
      "2022-04-29 17:48:36,280 INFO: Train Loss:0.741 | Acc:0.8305 | F1:0.7029\n",
      "2022-04-29 17:48:46,524 INFO: val Loss:0.586 | Acc:0.8477 | F1:0.7222\n",
      "2022-04-29 17:48:47,148 INFO: -----------------SAVE:39epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:50:10,660 INFO: Epoch:[040/100]\n",
      "2022-04-29 17:50:10,661 INFO: Train Loss:0.703 | Acc:0.8553 | F1:0.7396\n",
      "2022-04-29 17:50:21,062 INFO: val Loss:0.523 | Acc:0.8609 | F1:0.7261\n",
      "2022-04-29 17:50:21,668 INFO: -----------------SAVE:40epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.91it/s]\n",
      "2022-04-29 17:51:46,498 INFO: Epoch:[041/100]\n",
      "2022-04-29 17:51:46,499 INFO: Train Loss:0.634 | Acc:0.8675 | F1:0.7597\n",
      "2022-04-29 17:51:56,733 INFO: val Loss:0.839 | Acc:0.8071 | F1:0.7158\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 17:53:17,533 INFO: Epoch:[042/100]\n",
      "2022-04-29 17:53:17,533 INFO: Train Loss:0.632 | Acc:0.8683 | F1:0.7662\n",
      "2022-04-29 17:53:27,876 INFO: val Loss:0.627 | Acc:0.8609 | F1:0.7286\n",
      "100%|██████████| 247/247 [01:19<00:00,  3.09it/s]\n",
      "2022-04-29 17:54:47,856 INFO: Epoch:[043/100]\n",
      "2022-04-29 17:54:47,857 INFO: Train Loss:0.625 | Acc:0.8624 | F1:0.7539\n",
      "2022-04-29 17:54:58,247 INFO: val Loss:0.486 | Acc:0.8802 | F1:0.7745\n",
      "2022-04-29 17:54:58,841 INFO: -----------------SAVE:43epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 17:56:19,685 INFO: Epoch:[044/100]\n",
      "2022-04-29 17:56:19,685 INFO: Train Loss:0.557 | Acc:0.8614 | F1:0.7722\n",
      "2022-04-29 17:56:29,938 INFO: val Loss:0.455 | Acc:0.8995 | F1:0.7856\n",
      "2022-04-29 17:56:30,641 INFO: -----------------SAVE:44epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 17:57:54,135 INFO: Epoch:[045/100]\n",
      "2022-04-29 17:57:54,136 INFO: Train Loss:0.539 | Acc:0.8744 | F1:0.7812\n",
      "2022-04-29 17:58:04,395 INFO: val Loss:0.563 | Acc:0.8640 | F1:0.7789\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.07it/s]\n",
      "2022-04-29 17:59:24,912 INFO: Epoch:[046/100]\n",
      "2022-04-29 17:59:24,913 INFO: Train Loss:0.488 | Acc:0.8860 | F1:0.8036\n",
      "2022-04-29 17:59:35,383 INFO: val Loss:0.424 | Acc:0.8792 | F1:0.8141\n",
      "2022-04-29 17:59:36,001 INFO: -----------------SAVE:46epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.98it/s]\n",
      "2022-04-29 18:00:59,024 INFO: Epoch:[047/100]\n",
      "2022-04-29 18:00:59,025 INFO: Train Loss:0.474 | Acc:0.8985 | F1:0.8221\n",
      "2022-04-29 18:01:09,524 INFO: val Loss:0.410 | Acc:0.8670 | F1:0.8134\n",
      "2022-04-29 18:01:10,127 INFO: -----------------SAVE:47epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 18:02:32,692 INFO: Epoch:[048/100]\n",
      "2022-04-29 18:02:32,693 INFO: Train Loss:0.438 | Acc:0.8952 | F1:0.8248\n",
      "2022-04-29 18:02:43,009 INFO: val Loss:0.528 | Acc:0.9188 | F1:0.8207\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 18:04:06,109 INFO: Epoch:[049/100]\n",
      "2022-04-29 18:04:06,110 INFO: Train Loss:0.407 | Acc:0.9104 | F1:0.8431\n",
      "2022-04-29 18:04:16,384 INFO: val Loss:0.410 | Acc:0.9015 | F1:0.8295\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 18:05:39,527 INFO: Epoch:[050/100]\n",
      "2022-04-29 18:05:39,527 INFO: Train Loss:0.403 | Acc:0.9150 | F1:0.8500\n",
      "2022-04-29 18:05:49,832 INFO: val Loss:0.394 | Acc:0.8914 | F1:0.8362\n",
      "2022-04-29 18:05:50,465 INFO: -----------------SAVE:50epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.98it/s]\n",
      "2022-04-29 18:07:13,495 INFO: Epoch:[051/100]\n",
      "2022-04-29 18:07:13,496 INFO: Train Loss:0.374 | Acc:0.9135 | F1:0.8530\n",
      "2022-04-29 18:07:23,879 INFO: val Loss:0.371 | Acc:0.9482 | F1:0.8781\n",
      "2022-04-29 18:07:24,526 INFO: -----------------SAVE:51epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 18:08:47,514 INFO: Epoch:[052/100]\n",
      "2022-04-29 18:08:47,515 INFO: Train Loss:0.351 | Acc:0.9096 | F1:0.8484\n",
      "2022-04-29 18:08:57,778 INFO: val Loss:0.402 | Acc:0.9178 | F1:0.8466\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 18:10:20,417 INFO: Epoch:[053/100]\n",
      "2022-04-29 18:10:20,417 INFO: Train Loss:0.374 | Acc:0.9124 | F1:0.8546\n",
      "2022-04-29 18:10:30,719 INFO: val Loss:0.503 | Acc:0.9442 | F1:0.8498\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 18:11:53,707 INFO: Epoch:[054/100]\n",
      "2022-04-29 18:11:53,707 INFO: Train Loss:0.333 | Acc:0.9353 | F1:0.8841\n",
      "2022-04-29 18:12:03,917 INFO: val Loss:0.425 | Acc:0.8893 | F1:0.8561\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 18:13:27,413 INFO: Epoch:[055/100]\n",
      "2022-04-29 18:13:27,414 INFO: Train Loss:0.309 | Acc:0.9231 | F1:0.8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 18:13:37,741 INFO: val Loss:0.553 | Acc:0.9117 | F1:0.8690\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 18:15:01,640 INFO: Epoch:[056/100]\n",
      "2022-04-29 18:15:01,641 INFO: Train Loss:0.294 | Acc:0.9355 | F1:0.8938\n",
      "2022-04-29 18:15:11,816 INFO: val Loss:0.310 | Acc:0.9259 | F1:0.8863\n",
      "2022-04-29 18:15:12,410 INFO: -----------------SAVE:56epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 18:16:34,875 INFO: Epoch:[057/100]\n",
      "2022-04-29 18:16:34,876 INFO: Train Loss:0.275 | Acc:0.9256 | F1:0.8903\n",
      "2022-04-29 18:16:45,019 INFO: val Loss:0.368 | Acc:0.9391 | F1:0.8988\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 18:18:08,609 INFO: Epoch:[058/100]\n",
      "2022-04-29 18:18:08,610 INFO: Train Loss:0.287 | Acc:0.9386 | F1:0.8957\n",
      "2022-04-29 18:18:18,640 INFO: val Loss:0.319 | Acc:0.9360 | F1:0.8825\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 18:19:42,104 INFO: Epoch:[059/100]\n",
      "2022-04-29 18:19:42,104 INFO: Train Loss:0.245 | Acc:0.9325 | F1:0.9006\n",
      "2022-04-29 18:19:52,367 INFO: val Loss:0.288 | Acc:0.9421 | F1:0.8875\n",
      "2022-04-29 18:19:52,994 INFO: -----------------SAVE:59epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.01it/s]\n",
      "2022-04-29 18:21:15,076 INFO: Epoch:[060/100]\n",
      "2022-04-29 18:21:15,077 INFO: Train Loss:0.243 | Acc:0.9360 | F1:0.9006\n",
      "2022-04-29 18:21:25,436 INFO: val Loss:0.275 | Acc:0.9482 | F1:0.8978\n",
      "2022-04-29 18:21:25,996 INFO: -----------------SAVE:60epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 18:22:49,309 INFO: Epoch:[061/100]\n",
      "2022-04-29 18:22:49,310 INFO: Train Loss:0.245 | Acc:0.9350 | F1:0.8977\n",
      "2022-04-29 18:22:59,653 INFO: val Loss:0.237 | Acc:0.9503 | F1:0.8988\n",
      "2022-04-29 18:23:00,249 INFO: -----------------SAVE:61epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 18:24:23,786 INFO: Epoch:[062/100]\n",
      "2022-04-29 18:24:23,787 INFO: Train Loss:0.222 | Acc:0.9437 | F1:0.9041\n",
      "2022-04-29 18:24:33,969 INFO: val Loss:0.227 | Acc:0.9036 | F1:0.9018\n",
      "2022-04-29 18:24:34,577 INFO: -----------------SAVE:62epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 18:25:55,226 INFO: Epoch:[063/100]\n",
      "2022-04-29 18:25:55,227 INFO: Train Loss:0.212 | Acc:0.9452 | F1:0.9110\n",
      "2022-04-29 18:26:05,631 INFO: val Loss:0.359 | Acc:0.9046 | F1:0.9044\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 18:27:30,192 INFO: Epoch:[064/100]\n",
      "2022-04-29 18:27:30,193 INFO: Train Loss:0.197 | Acc:0.9487 | F1:0.9210\n",
      "2022-04-29 18:27:40,487 INFO: val Loss:0.314 | Acc:0.9685 | F1:0.9146\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 18:29:03,817 INFO: Epoch:[065/100]\n",
      "2022-04-29 18:29:03,817 INFO: Train Loss:0.177 | Acc:0.9525 | F1:0.9242\n",
      "2022-04-29 18:29:13,994 INFO: val Loss:0.300 | Acc:0.9482 | F1:0.8980\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 18:30:35,508 INFO: Epoch:[066/100]\n",
      "2022-04-29 18:30:35,509 INFO: Train Loss:0.202 | Acc:0.9571 | F1:0.9209\n",
      "2022-04-29 18:30:45,746 INFO: val Loss:0.285 | Acc:0.9492 | F1:0.9207\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.05it/s]\n",
      "2022-04-29 18:32:06,832 INFO: Epoch:[067/100]\n",
      "2022-04-29 18:32:06,832 INFO: Train Loss:0.206 | Acc:0.9523 | F1:0.9206\n",
      "2022-04-29 18:32:17,092 INFO: val Loss:0.249 | Acc:0.9452 | F1:0.9152\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 18:33:41,282 INFO: Epoch:[068/100]\n",
      "2022-04-29 18:33:41,283 INFO: Train Loss:0.143 | Acc:0.9561 | F1:0.9332\n",
      "2022-04-29 18:33:51,560 INFO: val Loss:0.287 | Acc:0.9036 | F1:0.8861\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:35:15,310 INFO: Epoch:[069/100]\n",
      "2022-04-29 18:35:15,311 INFO: Train Loss:0.139 | Acc:0.9594 | F1:0.9377\n",
      "2022-04-29 18:35:25,680 INFO: val Loss:0.243 | Acc:0.9604 | F1:0.9408\n",
      "2022-04-29 18:35:25,681 INFO: \n",
      "Best Val Epoch:62 | Val Loss:0.2273 | Val Acc:0.9036 | Val F1:0.9018\n",
      "2022-04-29 18:35:25,682 INFO: Total Process time:107.501Minute\n",
      "2022-04-29 18:35:25,685 INFO: {'exp_num': '1', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3940\n",
      "Dataset size:985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 18:35:26,010 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:36:50,089 INFO: Epoch:[001/100]\n",
      "2022-04-29 18:36:50,090 INFO: Train Loss:4.504 | Acc:0.0132 | F1:0.0054\n",
      "2022-04-29 18:37:00,577 INFO: val Loss:4.517 | Acc:0.0528 | F1:0.0134\n",
      "2022-04-29 18:37:01,199 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 18:38:25,495 INFO: Epoch:[002/100]\n",
      "2022-04-29 18:38:25,496 INFO: Train Loss:4.503 | Acc:0.0142 | F1:0.0072\n",
      "2022-04-29 18:38:35,818 INFO: val Loss:4.496 | Acc:0.0457 | F1:0.0089\n",
      "2022-04-29 18:38:36,447 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 18:39:59,291 INFO: Epoch:[003/100]\n",
      "2022-04-29 18:39:59,292 INFO: Train Loss:4.504 | Acc:0.0137 | F1:0.0053\n",
      "2022-04-29 18:40:09,860 INFO: val Loss:4.484 | Acc:0.0609 | F1:0.0120\n",
      "2022-04-29 18:40:10,480 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 18:41:33,975 INFO: Epoch:[004/100]\n",
      "2022-04-29 18:41:33,976 INFO: Train Loss:4.448 | Acc:0.0279 | F1:0.0089\n",
      "2022-04-29 18:41:44,349 INFO: val Loss:4.336 | Acc:0.0964 | F1:0.0276\n",
      "2022-04-29 18:41:44,999 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:43:08,748 INFO: Epoch:[005/100]\n",
      "2022-04-29 18:43:08,748 INFO: Train Loss:4.368 | Acc:0.0832 | F1:0.0267\n",
      "2022-04-29 18:43:19,169 INFO: val Loss:4.191 | Acc:0.3036 | F1:0.0923\n",
      "2022-04-29 18:43:19,814 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.01it/s]\n",
      "2022-04-29 18:44:41,982 INFO: Epoch:[006/100]\n",
      "2022-04-29 18:44:41,983 INFO: Train Loss:4.268 | Acc:0.2401 | F1:0.0760\n",
      "2022-04-29 18:44:52,445 INFO: val Loss:4.012 | Acc:0.5533 | F1:0.1393\n",
      "2022-04-29 18:44:53,083 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:46:16,885 INFO: Epoch:[007/100]\n",
      "2022-04-29 18:46:16,886 INFO: Train Loss:4.136 | Acc:0.4155 | F1:0.1243\n",
      "2022-04-29 18:46:27,653 INFO: val Loss:3.803 | Acc:0.7269 | F1:0.1915\n",
      "2022-04-29 18:46:28,323 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:47:52,036 INFO: Epoch:[008/100]\n",
      "2022-04-29 18:47:52,036 INFO: Train Loss:3.980 | Acc:0.5406 | F1:0.1504\n",
      "2022-04-29 18:48:02,736 INFO: val Loss:3.595 | Acc:0.7421 | F1:0.1791\n",
      "2022-04-29 18:48:03,353 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 18:49:26,631 INFO: Epoch:[009/100]\n",
      "2022-04-29 18:49:26,632 INFO: Train Loss:3.783 | Acc:0.6327 | F1:0.1711\n",
      "2022-04-29 18:49:37,359 INFO: val Loss:3.342 | Acc:0.7462 | F1:0.1780\n",
      "2022-04-29 18:49:37,983 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:51:01,707 INFO: Epoch:[010/100]\n",
      "2022-04-29 18:51:01,708 INFO: Train Loss:3.527 | Acc:0.6665 | F1:0.1838\n",
      "2022-04-29 18:51:12,372 INFO: val Loss:3.099 | Acc:0.7137 | F1:0.1812\n",
      "2022-04-29 18:51:12,997 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.05it/s]\n",
      "2022-04-29 18:52:34,101 INFO: Epoch:[011/100]\n",
      "2022-04-29 18:52:34,101 INFO: Train Loss:3.300 | Acc:0.6678 | F1:0.1793\n",
      "2022-04-29 18:52:44,525 INFO: val Loss:2.831 | Acc:0.7452 | F1:0.1898\n",
      "2022-04-29 18:52:45,210 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 18:54:06,933 INFO: Epoch:[012/100]\n",
      "2022-04-29 18:54:06,934 INFO: Train Loss:3.052 | Acc:0.6728 | F1:0.1988\n",
      "2022-04-29 18:54:17,571 INFO: val Loss:2.535 | Acc:0.7299 | F1:0.1932\n",
      "2022-04-29 18:54:18,218 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 18:55:39,517 INFO: Epoch:[013/100]\n",
      "2022-04-29 18:55:39,517 INFO: Train Loss:2.849 | Acc:0.6716 | F1:0.2080\n",
      "2022-04-29 18:55:49,939 INFO: val Loss:2.341 | Acc:0.7147 | F1:0.2018\n",
      "2022-04-29 18:55:50,573 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 18:57:14,258 INFO: Epoch:[014/100]\n",
      "2022-04-29 18:57:14,259 INFO: Train Loss:2.641 | Acc:0.6330 | F1:0.2283\n",
      "2022-04-29 18:57:24,855 INFO: val Loss:2.214 | Acc:0.6254 | F1:0.1903\n",
      "2022-04-29 18:57:25,523 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 18:58:49,872 INFO: Epoch:[015/100]\n",
      "2022-04-29 18:58:49,873 INFO: Train Loss:2.543 | Acc:0.6500 | F1:0.2462\n",
      "2022-04-29 18:59:00,351 INFO: val Loss:2.083 | Acc:0.6690 | F1:0.2123\n",
      "2022-04-29 18:59:01,025 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 19:00:25,506 INFO: Epoch:[016/100]\n",
      "2022-04-29 19:00:25,507 INFO: Train Loss:2.357 | Acc:0.6226 | F1:0.2308\n",
      "2022-04-29 19:00:36,199 INFO: val Loss:1.987 | Acc:0.5868 | F1:0.1951\n",
      "2022-04-29 19:00:36,856 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 19:01:58,011 INFO: Epoch:[017/100]\n",
      "2022-04-29 19:01:58,012 INFO: Train Loss:2.276 | Acc:0.6376 | F1:0.2597\n",
      "2022-04-29 19:02:08,644 INFO: val Loss:1.877 | Acc:0.5939 | F1:0.2166\n",
      "2022-04-29 19:02:09,308 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 19:03:30,573 INFO: Epoch:[018/100]\n",
      "2022-04-29 19:03:30,574 INFO: Train Loss:2.176 | Acc:0.6231 | F1:0.2705\n",
      "2022-04-29 19:03:41,084 INFO: val Loss:1.767 | Acc:0.7218 | F1:0.2601\n",
      "2022-04-29 19:03:41,682 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 19:05:05,203 INFO: Epoch:[019/100]\n",
      "2022-04-29 19:05:05,204 INFO: Train Loss:2.123 | Acc:0.6449 | F1:0.2803\n",
      "2022-04-29 19:05:15,898 INFO: val Loss:1.701 | Acc:0.6619 | F1:0.2661\n",
      "2022-04-29 19:05:16,586 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 19:06:40,020 INFO: Epoch:[020/100]\n",
      "2022-04-29 19:06:40,021 INFO: Train Loss:2.105 | Acc:0.6612 | F1:0.2885\n",
      "2022-04-29 19:06:50,599 INFO: val Loss:1.679 | Acc:0.7442 | F1:0.3085\n",
      "2022-04-29 19:06:51,211 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 19:08:13,976 INFO: Epoch:[021/100]\n",
      "2022-04-29 19:08:13,977 INFO: Train Loss:1.996 | Acc:0.6452 | F1:0.3105\n",
      "2022-04-29 19:08:24,503 INFO: val Loss:1.556 | Acc:0.6426 | F1:0.2832\n",
      "2022-04-29 19:08:25,135 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 19:09:49,142 INFO: Epoch:[022/100]\n",
      "2022-04-29 19:09:49,143 INFO: Train Loss:1.878 | Acc:0.6690 | F1:0.3274\n",
      "2022-04-29 19:09:59,412 INFO: val Loss:1.438 | Acc:0.6051 | F1:0.3154\n",
      "2022-04-29 19:09:59,976 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.05it/s]\n",
      "2022-04-29 19:11:20,973 INFO: Epoch:[023/100]\n",
      "2022-04-29 19:11:20,974 INFO: Train Loss:1.854 | Acc:0.6675 | F1:0.3404\n",
      "2022-04-29 19:11:31,407 INFO: val Loss:1.313 | Acc:0.8000 | F1:0.3533\n",
      "2022-04-29 19:11:32,028 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 19:12:56,549 INFO: Epoch:[024/100]\n",
      "2022-04-29 19:12:56,550 INFO: Train Loss:1.764 | Acc:0.6919 | F1:0.3748\n",
      "2022-04-29 19:13:06,932 INFO: val Loss:1.265 | Acc:0.8173 | F1:0.3879\n",
      "2022-04-29 19:13:07,559 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 19:14:31,347 INFO: Epoch:[025/100]\n",
      "2022-04-29 19:14:31,347 INFO: Train Loss:1.671 | Acc:0.6906 | F1:0.3825\n",
      "2022-04-29 19:14:41,794 INFO: val Loss:1.185 | Acc:0.7695 | F1:0.4279\n",
      "2022-04-29 19:14:42,420 INFO: -----------------SAVE:25epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 19:16:06,980 INFO: Epoch:[026/100]\n",
      "2022-04-29 19:16:06,980 INFO: Train Loss:1.578 | Acc:0.7099 | F1:0.4305\n",
      "2022-04-29 19:16:17,469 INFO: val Loss:1.295 | Acc:0.7604 | F1:0.4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 19:17:38,852 INFO: Epoch:[027/100]\n",
      "2022-04-29 19:17:38,853 INFO: Train Loss:1.517 | Acc:0.7256 | F1:0.4532\n",
      "2022-04-29 19:17:49,490 INFO: val Loss:1.180 | Acc:0.7411 | F1:0.4574\n",
      "2022-04-29 19:17:50,109 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 19:19:10,938 INFO: Epoch:[028/100]\n",
      "2022-04-29 19:19:10,939 INFO: Train Loss:1.418 | Acc:0.7190 | F1:0.4650\n",
      "2022-04-29 19:19:21,503 INFO: val Loss:1.083 | Acc:0.7645 | F1:0.4708\n",
      "2022-04-29 19:19:22,127 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 19:20:46,152 INFO: Epoch:[029/100]\n",
      "2022-04-29 19:20:46,153 INFO: Train Loss:1.330 | Acc:0.7472 | F1:0.5047\n",
      "2022-04-29 19:20:56,487 INFO: val Loss:0.956 | Acc:0.7797 | F1:0.5525\n",
      "2022-04-29 19:20:57,163 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 19:22:21,138 INFO: Epoch:[030/100]\n",
      "2022-04-29 19:22:21,139 INFO: Train Loss:1.268 | Acc:0.7429 | F1:0.5108\n",
      "2022-04-29 19:22:31,623 INFO: val Loss:0.905 | Acc:0.8345 | F1:0.5414\n",
      "2022-04-29 19:22:32,331 INFO: -----------------SAVE:30epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 19:23:55,673 INFO: Epoch:[031/100]\n",
      "2022-04-29 19:23:55,674 INFO: Train Loss:1.201 | Acc:0.7741 | F1:0.5495\n",
      "2022-04-29 19:24:06,178 INFO: val Loss:0.959 | Acc:0.8223 | F1:0.5782\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 19:25:28,909 INFO: Epoch:[032/100]\n",
      "2022-04-29 19:25:28,909 INFO: Train Loss:1.141 | Acc:0.7739 | F1:0.5651\n",
      "2022-04-29 19:25:39,398 INFO: val Loss:0.851 | Acc:0.7858 | F1:0.5610\n",
      "2022-04-29 19:25:40,031 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 19:27:01,415 INFO: Epoch:[033/100]\n",
      "2022-04-29 19:27:01,415 INFO: Train Loss:1.085 | Acc:0.7840 | F1:0.5930\n",
      "2022-04-29 19:27:11,870 INFO: val Loss:0.911 | Acc:0.8294 | F1:0.5818\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 19:28:36,097 INFO: Epoch:[034/100]\n",
      "2022-04-29 19:28:36,097 INFO: Train Loss:1.015 | Acc:0.7921 | F1:0.6213\n",
      "2022-04-29 19:28:46,497 INFO: val Loss:0.736 | Acc:0.8345 | F1:0.6430\n",
      "2022-04-29 19:28:47,151 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 19:30:11,617 INFO: Epoch:[035/100]\n",
      "2022-04-29 19:30:11,617 INFO: Train Loss:0.987 | Acc:0.7954 | F1:0.6389\n",
      "2022-04-29 19:30:22,169 INFO: val Loss:0.763 | Acc:0.8609 | F1:0.6663\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 19:31:46,011 INFO: Epoch:[036/100]\n",
      "2022-04-29 19:31:46,012 INFO: Train Loss:0.928 | Acc:0.8003 | F1:0.6562\n",
      "2022-04-29 19:31:56,640 INFO: val Loss:0.669 | Acc:0.8254 | F1:0.6644\n",
      "2022-04-29 19:31:57,277 INFO: -----------------SAVE:36epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 19:33:20,032 INFO: Epoch:[037/100]\n",
      "2022-04-29 19:33:20,033 INFO: Train Loss:0.861 | Acc:0.8228 | F1:0.6711\n",
      "2022-04-29 19:33:30,571 INFO: val Loss:0.636 | Acc:0.7777 | F1:0.6723\n",
      "2022-04-29 19:33:31,191 INFO: -----------------SAVE:37epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 19:34:52,792 INFO: Epoch:[038/100]\n",
      "2022-04-29 19:34:52,793 INFO: Train Loss:0.814 | Acc:0.8236 | F1:0.6947\n",
      "2022-04-29 19:35:03,322 INFO: val Loss:0.808 | Acc:0.7746 | F1:0.6211\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 19:36:24,075 INFO: Epoch:[039/100]\n",
      "2022-04-29 19:36:24,075 INFO: Train Loss:0.789 | Acc:0.8264 | F1:0.7014\n",
      "2022-04-29 19:36:34,671 INFO: val Loss:0.712 | Acc:0.8964 | F1:0.7000\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 19:37:56,588 INFO: Epoch:[040/100]\n",
      "2022-04-29 19:37:56,589 INFO: Train Loss:0.727 | Acc:0.8480 | F1:0.7357\n",
      "2022-04-29 19:38:06,957 INFO: val Loss:0.494 | Acc:0.8640 | F1:0.7366\n",
      "2022-04-29 19:38:07,583 INFO: -----------------SAVE:40epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 19:39:29,963 INFO: Epoch:[041/100]\n",
      "2022-04-29 19:39:29,963 INFO: Train Loss:0.723 | Acc:0.8500 | F1:0.7326\n",
      "2022-04-29 19:39:40,407 INFO: val Loss:0.634 | Acc:0.8802 | F1:0.7471\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 19:41:03,561 INFO: Epoch:[042/100]\n",
      "2022-04-29 19:41:03,562 INFO: Train Loss:0.666 | Acc:0.8553 | F1:0.7496\n",
      "2022-04-29 19:41:14,020 INFO: val Loss:0.493 | Acc:0.9147 | F1:0.7733\n",
      "2022-04-29 19:41:14,669 INFO: -----------------SAVE:42epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 19:42:37,645 INFO: Epoch:[043/100]\n",
      "2022-04-29 19:42:37,645 INFO: Train Loss:0.651 | Acc:0.8533 | F1:0.7538\n",
      "2022-04-29 19:42:48,107 INFO: val Loss:0.517 | Acc:0.9239 | F1:0.7935\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.07it/s]\n",
      "2022-04-29 19:44:08,529 INFO: Epoch:[044/100]\n",
      "2022-04-29 19:44:08,529 INFO: Train Loss:0.604 | Acc:0.8736 | F1:0.7794\n",
      "2022-04-29 19:44:19,058 INFO: val Loss:0.555 | Acc:0.8234 | F1:0.7766\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 19:45:42,889 INFO: Epoch:[045/100]\n",
      "2022-04-29 19:45:42,890 INFO: Train Loss:0.587 | Acc:0.8591 | F1:0.7772\n",
      "2022-04-29 19:45:53,709 INFO: val Loss:0.689 | Acc:0.8396 | F1:0.7585\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 19:47:16,579 INFO: Epoch:[046/100]\n",
      "2022-04-29 19:47:16,580 INFO: Train Loss:0.533 | Acc:0.8850 | F1:0.8050\n",
      "2022-04-29 19:47:27,068 INFO: val Loss:0.415 | Acc:0.8964 | F1:0.8337\n",
      "2022-04-29 19:47:27,693 INFO: -----------------SAVE:46epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 19:48:49,412 INFO: Epoch:[047/100]\n",
      "2022-04-29 19:48:49,413 INFO: Train Loss:0.482 | Acc:0.8853 | F1:0.8103\n",
      "2022-04-29 19:48:59,807 INFO: val Loss:0.483 | Acc:0.9015 | F1:0.8039\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 19:50:22,809 INFO: Epoch:[048/100]\n",
      "2022-04-29 19:50:22,810 INFO: Train Loss:0.494 | Acc:0.8896 | F1:0.8133\n",
      "2022-04-29 19:50:33,275 INFO: val Loss:0.356 | Acc:0.8558 | F1:0.8303\n",
      "2022-04-29 19:50:33,894 INFO: -----------------SAVE:48epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 19:51:57,106 INFO: Epoch:[049/100]\n",
      "2022-04-29 19:51:57,107 INFO: Train Loss:0.410 | Acc:0.8896 | F1:0.8267\n",
      "2022-04-29 19:52:07,441 INFO: val Loss:0.395 | Acc:0.9107 | F1:0.8617\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 19:53:31,140 INFO: Epoch:[050/100]\n",
      "2022-04-29 19:53:31,141 INFO: Train Loss:0.415 | Acc:0.8926 | F1:0.8370\n",
      "2022-04-29 19:53:41,646 INFO: val Loss:0.436 | Acc:0.8832 | F1:0.8267\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 19:55:04,087 INFO: Epoch:[051/100]\n",
      "2022-04-29 19:55:04,088 INFO: Train Loss:0.397 | Acc:0.9010 | F1:0.8351\n",
      "2022-04-29 19:55:14,620 INFO: val Loss:0.312 | Acc:0.9005 | F1:0.8445\n",
      "2022-04-29 19:55:15,278 INFO: -----------------SAVE:51epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 19:56:39,196 INFO: Epoch:[052/100]\n",
      "2022-04-29 19:56:39,197 INFO: Train Loss:0.427 | Acc:0.9104 | F1:0.8466\n",
      "2022-04-29 19:56:49,705 INFO: val Loss:0.290 | Acc:0.9431 | F1:0.8831\n",
      "2022-04-29 19:56:50,355 INFO: -----------------SAVE:52epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 19:58:13,851 INFO: Epoch:[053/100]\n",
      "2022-04-29 19:58:13,851 INFO: Train Loss:0.357 | Acc:0.9180 | F1:0.8657\n",
      "2022-04-29 19:58:24,219 INFO: val Loss:0.329 | Acc:0.9310 | F1:0.8693\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 19:59:48,040 INFO: Epoch:[054/100]\n",
      "2022-04-29 19:59:48,040 INFO: Train Loss:0.308 | Acc:0.9221 | F1:0.8734\n",
      "2022-04-29 19:59:58,540 INFO: val Loss:0.288 | Acc:0.9523 | F1:0.8748\n",
      "2022-04-29 19:59:59,192 INFO: -----------------SAVE:54epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 20:01:20,420 INFO: Epoch:[055/100]\n",
      "2022-04-29 20:01:20,421 INFO: Train Loss:0.277 | Acc:0.9302 | F1:0.8861\n",
      "2022-04-29 20:01:30,959 INFO: val Loss:0.296 | Acc:0.9005 | F1:0.8541\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 20:02:53,383 INFO: Epoch:[056/100]\n",
      "2022-04-29 20:02:53,384 INFO: Train Loss:0.278 | Acc:0.9297 | F1:0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 20:03:04,026 INFO: val Loss:0.285 | Acc:0.9147 | F1:0.8594\n",
      "2022-04-29 20:03:04,670 INFO: -----------------SAVE:56epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 20:04:28,599 INFO: Epoch:[057/100]\n",
      "2022-04-29 20:04:28,599 INFO: Train Loss:0.318 | Acc:0.9284 | F1:0.8779\n",
      "2022-04-29 20:04:39,134 INFO: val Loss:0.274 | Acc:0.9452 | F1:0.8807\n",
      "2022-04-29 20:04:39,737 INFO: -----------------SAVE:57epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 20:06:04,097 INFO: Epoch:[058/100]\n",
      "2022-04-29 20:06:04,097 INFO: Train Loss:0.292 | Acc:0.9365 | F1:0.8880\n",
      "2022-04-29 20:06:14,546 INFO: val Loss:0.287 | Acc:0.9482 | F1:0.8811\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 20:07:36,101 INFO: Epoch:[059/100]\n",
      "2022-04-29 20:07:36,101 INFO: Train Loss:0.269 | Acc:0.9396 | F1:0.8925\n",
      "2022-04-29 20:07:46,614 INFO: val Loss:0.276 | Acc:0.9066 | F1:0.8758\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 20:09:10,587 INFO: Epoch:[060/100]\n",
      "2022-04-29 20:09:10,588 INFO: Train Loss:0.235 | Acc:0.9368 | F1:0.9016\n",
      "2022-04-29 20:09:21,206 INFO: val Loss:0.284 | Acc:0.9381 | F1:0.8762\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 20:10:45,250 INFO: Epoch:[061/100]\n",
      "2022-04-29 20:10:45,251 INFO: Train Loss:0.224 | Acc:0.9376 | F1:0.9006\n",
      "2022-04-29 20:10:55,662 INFO: val Loss:0.236 | Acc:0.9574 | F1:0.9052\n",
      "2022-04-29 20:10:56,261 INFO: -----------------SAVE:61epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 20:12:19,417 INFO: Epoch:[062/100]\n",
      "2022-04-29 20:12:19,418 INFO: Train Loss:0.260 | Acc:0.9211 | F1:0.8845\n",
      "2022-04-29 20:12:30,034 INFO: val Loss:0.234 | Acc:0.9513 | F1:0.9077\n",
      "2022-04-29 20:12:30,722 INFO: -----------------SAVE:62epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 20:13:53,097 INFO: Epoch:[063/100]\n",
      "2022-04-29 20:13:53,098 INFO: Train Loss:0.202 | Acc:0.9480 | F1:0.9207\n",
      "2022-04-29 20:14:03,825 INFO: val Loss:0.230 | Acc:0.9086 | F1:0.9022\n",
      "2022-04-29 20:14:04,441 INFO: -----------------SAVE:63epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 20:15:28,370 INFO: Epoch:[064/100]\n",
      "2022-04-29 20:15:28,370 INFO: Train Loss:0.196 | Acc:0.9520 | F1:0.9160\n",
      "2022-04-29 20:15:38,976 INFO: val Loss:0.329 | Acc:0.9523 | F1:0.8894\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 20:17:01,934 INFO: Epoch:[065/100]\n",
      "2022-04-29 20:17:01,935 INFO: Train Loss:0.169 | Acc:0.9520 | F1:0.9268\n",
      "2022-04-29 20:17:12,513 INFO: val Loss:0.268 | Acc:0.9188 | F1:0.8913\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 20:18:35,736 INFO: Epoch:[066/100]\n",
      "2022-04-29 20:18:35,737 INFO: Train Loss:0.150 | Acc:0.9569 | F1:0.9343\n",
      "2022-04-29 20:18:46,048 INFO: val Loss:0.278 | Acc:0.9036 | F1:0.8864\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 20:20:09,487 INFO: Epoch:[067/100]\n",
      "2022-04-29 20:20:09,487 INFO: Train Loss:0.162 | Acc:0.9551 | F1:0.9268\n",
      "2022-04-29 20:20:20,154 INFO: val Loss:0.287 | Acc:0.9239 | F1:0.8954\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 20:21:43,952 INFO: Epoch:[068/100]\n",
      "2022-04-29 20:21:43,953 INFO: Train Loss:0.153 | Acc:0.9609 | F1:0.9432\n",
      "2022-04-29 20:21:54,459 INFO: val Loss:0.239 | Acc:0.9320 | F1:0.8968\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 20:23:18,332 INFO: Epoch:[069/100]\n",
      "2022-04-29 20:23:18,332 INFO: Train Loss:0.140 | Acc:0.9541 | F1:0.9352\n",
      "2022-04-29 20:23:28,831 INFO: val Loss:0.294 | Acc:0.9635 | F1:0.9173\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 20:24:52,115 INFO: Epoch:[070/100]\n",
      "2022-04-29 20:24:52,116 INFO: Train Loss:0.129 | Acc:0.9617 | F1:0.9436\n",
      "2022-04-29 20:25:02,709 INFO: val Loss:0.326 | Acc:0.9594 | F1:0.9170\n",
      "2022-04-29 20:25:02,710 INFO: \n",
      "Best Val Epoch:63 | Val Loss:0.2304 | Val Acc:0.9086 | Val F1:0.9022\n",
      "2022-04-29 20:25:02,711 INFO: Total Process time:109.607Minute\n",
      "2022-04-29 20:25:02,715 INFO: {'exp_num': '2', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3940\n",
      "Dataset size:985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 20:25:03,047 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 20:26:24,663 INFO: Epoch:[001/100]\n",
      "2022-04-29 20:26:24,663 INFO: Train Loss:4.502 | Acc:0.0099 | F1:0.0038\n",
      "2022-04-29 20:26:35,105 INFO: val Loss:4.491 | Acc:0.0518 | F1:0.0130\n",
      "2022-04-29 20:26:35,717 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 20:27:59,308 INFO: Epoch:[002/100]\n",
      "2022-04-29 20:27:59,309 INFO: Train Loss:4.515 | Acc:0.0117 | F1:0.0057\n",
      "2022-04-29 20:28:09,718 INFO: val Loss:4.481 | Acc:0.0518 | F1:0.0081\n",
      "2022-04-29 20:28:10,400 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 20:29:34,342 INFO: Epoch:[003/100]\n",
      "2022-04-29 20:29:34,343 INFO: Train Loss:4.497 | Acc:0.0150 | F1:0.0070\n",
      "2022-04-29 20:29:44,801 INFO: val Loss:4.466 | Acc:0.0477 | F1:0.0120\n",
      "2022-04-29 20:29:45,412 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 20:31:09,551 INFO: Epoch:[004/100]\n",
      "2022-04-29 20:31:09,552 INFO: Train Loss:4.463 | Acc:0.0279 | F1:0.0093\n",
      "2022-04-29 20:31:20,163 INFO: val Loss:4.324 | Acc:0.1117 | F1:0.0250\n",
      "2022-04-29 20:31:20,765 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 20:32:45,115 INFO: Epoch:[005/100]\n",
      "2022-04-29 20:32:45,116 INFO: Train Loss:4.365 | Acc:0.0848 | F1:0.0254\n",
      "2022-04-29 20:32:55,470 INFO: val Loss:4.175 | Acc:0.3360 | F1:0.1046\n",
      "2022-04-29 20:32:56,086 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 20:34:19,938 INFO: Epoch:[006/100]\n",
      "2022-04-29 20:34:19,938 INFO: Train Loss:4.271 | Acc:0.2393 | F1:0.0781\n",
      "2022-04-29 20:34:30,299 INFO: val Loss:4.001 | Acc:0.5838 | F1:0.1573\n",
      "2022-04-29 20:34:31,018 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 20:35:53,943 INFO: Epoch:[007/100]\n",
      "2022-04-29 20:35:53,943 INFO: Train Loss:4.140 | Acc:0.4231 | F1:0.1202\n",
      "2022-04-29 20:36:04,449 INFO: val Loss:3.795 | Acc:0.6812 | F1:0.1636\n",
      "2022-04-29 20:36:05,085 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 20:37:28,479 INFO: Epoch:[008/100]\n",
      "2022-04-29 20:37:28,479 INFO: Train Loss:3.984 | Acc:0.5414 | F1:0.1495\n",
      "2022-04-29 20:37:38,729 INFO: val Loss:3.569 | Acc:0.7218 | F1:0.1777\n",
      "2022-04-29 20:37:39,380 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 20:39:02,790 INFO: Epoch:[009/100]\n",
      "2022-04-29 20:39:02,791 INFO: Train Loss:3.768 | Acc:0.6470 | F1:0.1678\n",
      "2022-04-29 20:39:13,200 INFO: val Loss:3.270 | Acc:0.7157 | F1:0.1600\n",
      "2022-04-29 20:39:13,825 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 20:40:35,198 INFO: Epoch:[010/100]\n",
      "2022-04-29 20:40:35,199 INFO: Train Loss:3.560 | Acc:0.6759 | F1:0.1696\n",
      "2022-04-29 20:40:45,643 INFO: val Loss:3.151 | Acc:0.7431 | F1:0.1662\n",
      "2022-04-29 20:40:46,278 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 20:42:09,461 INFO: Epoch:[011/100]\n",
      "2022-04-29 20:42:09,461 INFO: Train Loss:3.290 | Acc:0.6782 | F1:0.1827\n",
      "2022-04-29 20:42:20,172 INFO: val Loss:2.819 | Acc:0.7239 | F1:0.1798\n",
      "2022-04-29 20:42:20,787 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 20:43:44,898 INFO: Epoch:[012/100]\n",
      "2022-04-29 20:43:44,898 INFO: Train Loss:3.081 | Acc:0.6695 | F1:0.1871\n",
      "2022-04-29 20:43:54,995 INFO: val Loss:2.546 | Acc:0.6335 | F1:0.1771\n",
      "2022-04-29 20:43:55,619 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 20:45:16,459 INFO: Epoch:[013/100]\n",
      "2022-04-29 20:45:16,460 INFO: Train Loss:2.883 | Acc:0.6650 | F1:0.2084\n",
      "2022-04-29 20:45:26,863 INFO: val Loss:2.343 | Acc:0.6904 | F1:0.1790\n",
      "2022-04-29 20:45:27,485 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 20:46:50,720 INFO: Epoch:[014/100]\n",
      "2022-04-29 20:46:50,720 INFO: Train Loss:2.663 | Acc:0.6251 | F1:0.2146\n",
      "2022-04-29 20:47:01,247 INFO: val Loss:2.201 | Acc:0.6822 | F1:0.1994\n",
      "2022-04-29 20:47:01,866 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 20:48:26,391 INFO: Epoch:[015/100]\n",
      "2022-04-29 20:48:26,391 INFO: Train Loss:2.543 | Acc:0.6056 | F1:0.2345\n",
      "2022-04-29 20:48:36,696 INFO: val Loss:2.026 | Acc:0.6274 | F1:0.1997\n",
      "2022-04-29 20:48:37,351 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 20:50:01,324 INFO: Epoch:[016/100]\n",
      "2022-04-29 20:50:01,325 INFO: Train Loss:2.375 | Acc:0.6297 | F1:0.2397\n",
      "2022-04-29 20:50:11,750 INFO: val Loss:1.908 | Acc:0.6294 | F1:0.2030\n",
      "2022-04-29 20:50:12,398 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 20:51:36,914 INFO: Epoch:[017/100]\n",
      "2022-04-29 20:51:36,914 INFO: Train Loss:2.327 | Acc:0.6046 | F1:0.2435\n",
      "2022-04-29 20:51:47,073 INFO: val Loss:1.878 | Acc:0.6761 | F1:0.2761\n",
      "2022-04-29 20:51:47,724 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 20:53:12,373 INFO: Epoch:[018/100]\n",
      "2022-04-29 20:53:12,374 INFO: Train Loss:2.188 | Acc:0.6112 | F1:0.2720\n",
      "2022-04-29 20:53:22,761 INFO: val Loss:1.745 | Acc:0.5665 | F1:0.2462\n",
      "2022-04-29 20:53:23,387 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 20:54:46,379 INFO: Epoch:[019/100]\n",
      "2022-04-29 20:54:46,380 INFO: Train Loss:2.157 | Acc:0.6294 | F1:0.2568\n",
      "2022-04-29 20:54:56,742 INFO: val Loss:1.624 | Acc:0.7198 | F1:0.2835\n",
      "2022-04-29 20:54:57,370 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 20:56:21,836 INFO: Epoch:[020/100]\n",
      "2022-04-29 20:56:21,837 INFO: Train Loss:2.105 | Acc:0.6320 | F1:0.2817\n",
      "2022-04-29 20:56:32,567 INFO: val Loss:1.617 | Acc:0.6680 | F1:0.2710\n",
      "2022-04-29 20:56:33,174 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 20:57:56,407 INFO: Epoch:[021/100]\n",
      "2022-04-29 20:57:56,408 INFO: Train Loss:1.992 | Acc:0.6447 | F1:0.3022\n",
      "2022-04-29 20:58:06,854 INFO: val Loss:1.617 | Acc:0.6995 | F1:0.2876\n",
      "2022-04-29 20:58:07,507 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 20:59:28,365 INFO: Epoch:[022/100]\n",
      "2022-04-29 20:59:28,365 INFO: Train Loss:1.882 | Acc:0.6447 | F1:0.3141\n",
      "2022-04-29 20:59:38,871 INFO: val Loss:1.487 | Acc:0.7371 | F1:0.3503\n",
      "2022-04-29 20:59:39,484 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 21:01:03,418 INFO: Epoch:[023/100]\n",
      "2022-04-29 21:01:03,419 INFO: Train Loss:1.855 | Acc:0.6561 | F1:0.3387\n",
      "2022-04-29 21:01:13,718 INFO: val Loss:1.367 | Acc:0.7736 | F1:0.3798\n",
      "2022-04-29 21:01:14,331 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 21:02:38,866 INFO: Epoch:[024/100]\n",
      "2022-04-29 21:02:38,866 INFO: Train Loss:1.759 | Acc:0.6772 | F1:0.3737\n",
      "2022-04-29 21:02:49,361 INFO: val Loss:1.355 | Acc:0.7584 | F1:0.3661\n",
      "2022-04-29 21:02:49,967 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 21:04:12,459 INFO: Epoch:[025/100]\n",
      "2022-04-29 21:04:12,459 INFO: Train Loss:1.714 | Acc:0.6916 | F1:0.3717\n",
      "2022-04-29 21:04:22,696 INFO: val Loss:1.371 | Acc:0.6792 | F1:0.3741\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 21:05:45,795 INFO: Epoch:[026/100]\n",
      "2022-04-29 21:05:45,795 INFO: Train Loss:1.606 | Acc:0.6832 | F1:0.3985\n",
      "2022-04-29 21:05:56,173 INFO: val Loss:1.222 | Acc:0.7431 | F1:0.4225\n",
      "2022-04-29 21:05:56,797 INFO: -----------------SAVE:26epoch----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 21:07:17,503 INFO: Epoch:[027/100]\n",
      "2022-04-29 21:07:17,503 INFO: Train Loss:1.506 | Acc:0.7183 | F1:0.4488\n",
      "2022-04-29 21:07:27,764 INFO: val Loss:1.236 | Acc:0.6761 | F1:0.4196\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 21:08:50,634 INFO: Epoch:[028/100]\n",
      "2022-04-29 21:08:50,634 INFO: Train Loss:1.431 | Acc:0.7216 | F1:0.4668\n",
      "2022-04-29 21:09:01,009 INFO: val Loss:1.052 | Acc:0.7807 | F1:0.4814\n",
      "2022-04-29 21:09:01,621 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 21:10:25,031 INFO: Epoch:[029/100]\n",
      "2022-04-29 21:10:25,031 INFO: Train Loss:1.371 | Acc:0.7343 | F1:0.4772\n",
      "2022-04-29 21:10:35,338 INFO: val Loss:1.062 | Acc:0.7462 | F1:0.5383\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.08it/s]\n",
      "2022-04-29 21:11:55,569 INFO: Epoch:[030/100]\n",
      "2022-04-29 21:11:55,569 INFO: Train Loss:1.312 | Acc:0.7454 | F1:0.5074\n",
      "2022-04-29 21:12:05,818 INFO: val Loss:1.163 | Acc:0.7208 | F1:0.4931\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.06it/s]\n",
      "2022-04-29 21:13:26,672 INFO: Epoch:[031/100]\n",
      "2022-04-29 21:13:26,673 INFO: Train Loss:1.244 | Acc:0.7632 | F1:0.5528\n",
      "2022-04-29 21:13:37,020 INFO: val Loss:1.039 | Acc:0.7919 | F1:0.5071\n",
      "2022-04-29 21:13:37,639 INFO: -----------------SAVE:31epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 21:15:01,695 INFO: Epoch:[032/100]\n",
      "2022-04-29 21:15:01,695 INFO: Train Loss:1.164 | Acc:0.7695 | F1:0.5509\n",
      "2022-04-29 21:15:12,128 INFO: val Loss:0.977 | Acc:0.7624 | F1:0.5199\n",
      "2022-04-29 21:15:12,802 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 21:16:35,129 INFO: Epoch:[033/100]\n",
      "2022-04-29 21:16:35,130 INFO: Train Loss:1.109 | Acc:0.7789 | F1:0.5697\n",
      "2022-04-29 21:16:45,354 INFO: val Loss:0.879 | Acc:0.8132 | F1:0.5784\n",
      "2022-04-29 21:16:45,989 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:18:10,409 INFO: Epoch:[034/100]\n",
      "2022-04-29 21:18:10,409 INFO: Train Loss:1.063 | Acc:0.7779 | F1:0.5871\n",
      "2022-04-29 21:18:21,008 INFO: val Loss:0.979 | Acc:0.8518 | F1:0.6151\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 21:19:44,275 INFO: Epoch:[035/100]\n",
      "2022-04-29 21:19:44,276 INFO: Train Loss:1.000 | Acc:0.7904 | F1:0.6064\n",
      "2022-04-29 21:19:54,903 INFO: val Loss:0.788 | Acc:0.7817 | F1:0.6170\n",
      "2022-04-29 21:19:55,503 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:21:19,778 INFO: Epoch:[036/100]\n",
      "2022-04-29 21:21:19,779 INFO: Train Loss:0.953 | Acc:0.7937 | F1:0.6343\n",
      "2022-04-29 21:21:30,283 INFO: val Loss:0.675 | Acc:0.8761 | F1:0.6734\n",
      "2022-04-29 21:21:30,972 INFO: -----------------SAVE:36epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 21:22:54,942 INFO: Epoch:[037/100]\n",
      "2022-04-29 21:22:54,943 INFO: Train Loss:0.926 | Acc:0.8005 | F1:0.6421\n",
      "2022-04-29 21:23:05,285 INFO: val Loss:0.846 | Acc:0.8487 | F1:0.6572\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 21:24:27,709 INFO: Epoch:[038/100]\n",
      "2022-04-29 21:24:27,709 INFO: Train Loss:0.834 | Acc:0.8259 | F1:0.6821\n",
      "2022-04-29 21:24:38,322 INFO: val Loss:0.550 | Acc:0.9005 | F1:0.7259\n",
      "2022-04-29 21:24:38,955 INFO: -----------------SAVE:38epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:26:03,146 INFO: Epoch:[039/100]\n",
      "2022-04-29 21:26:03,147 INFO: Train Loss:0.787 | Acc:0.8388 | F1:0.6990\n",
      "2022-04-29 21:26:13,633 INFO: val Loss:0.701 | Acc:0.7787 | F1:0.6884\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.01it/s]\n",
      "2022-04-29 21:27:35,595 INFO: Epoch:[040/100]\n",
      "2022-04-29 21:27:35,595 INFO: Train Loss:0.775 | Acc:0.8317 | F1:0.6953\n",
      "2022-04-29 21:27:46,115 INFO: val Loss:0.575 | Acc:0.8142 | F1:0.7277\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 21:29:08,494 INFO: Epoch:[041/100]\n",
      "2022-04-29 21:29:08,495 INFO: Train Loss:0.741 | Acc:0.8454 | F1:0.7134\n",
      "2022-04-29 21:29:18,908 INFO: val Loss:0.624 | Acc:0.9117 | F1:0.7428\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 21:30:42,297 INFO: Epoch:[042/100]\n",
      "2022-04-29 21:30:42,297 INFO: Train Loss:0.713 | Acc:0.8409 | F1:0.7195\n",
      "2022-04-29 21:30:52,854 INFO: val Loss:0.523 | Acc:0.8386 | F1:0.7482\n",
      "2022-04-29 21:30:53,508 INFO: -----------------SAVE:42epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 21:32:17,074 INFO: Epoch:[043/100]\n",
      "2022-04-29 21:32:17,075 INFO: Train Loss:0.627 | Acc:0.8503 | F1:0.7401\n",
      "2022-04-29 21:32:27,484 INFO: val Loss:0.498 | Acc:0.8975 | F1:0.7403\n",
      "2022-04-29 21:32:28,104 INFO: -----------------SAVE:43epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.01it/s]\n",
      "2022-04-29 21:33:50,271 INFO: Epoch:[044/100]\n",
      "2022-04-29 21:33:50,272 INFO: Train Loss:0.638 | Acc:0.8561 | F1:0.7573\n",
      "2022-04-29 21:34:00,606 INFO: val Loss:0.431 | Acc:0.9168 | F1:0.8137\n",
      "2022-04-29 21:34:01,229 INFO: -----------------SAVE:44epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 21:35:24,949 INFO: Epoch:[045/100]\n",
      "2022-04-29 21:35:24,950 INFO: Train Loss:0.617 | Acc:0.8741 | F1:0.7664\n",
      "2022-04-29 21:35:35,331 INFO: val Loss:0.475 | Acc:0.8904 | F1:0.7787\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 21:36:56,898 INFO: Epoch:[046/100]\n",
      "2022-04-29 21:36:56,898 INFO: Train Loss:0.583 | Acc:0.8746 | F1:0.7700\n",
      "2022-04-29 21:37:07,278 INFO: val Loss:0.478 | Acc:0.8416 | F1:0.7498\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:38:31,656 INFO: Epoch:[047/100]\n",
      "2022-04-29 21:38:31,657 INFO: Train Loss:0.497 | Acc:0.8822 | F1:0.8113\n",
      "2022-04-29 21:38:42,265 INFO: val Loss:0.286 | Acc:0.9411 | F1:0.8638\n",
      "2022-04-29 21:38:42,901 INFO: -----------------SAVE:47epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 21:40:06,478 INFO: Epoch:[048/100]\n",
      "2022-04-29 21:40:06,478 INFO: Train Loss:0.470 | Acc:0.8987 | F1:0.8210\n",
      "2022-04-29 21:40:17,089 INFO: val Loss:0.374 | Acc:0.9269 | F1:0.8590\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:41:41,308 INFO: Epoch:[049/100]\n",
      "2022-04-29 21:41:41,309 INFO: Train Loss:0.508 | Acc:0.8845 | F1:0.7973\n",
      "2022-04-29 21:41:51,825 INFO: val Loss:0.524 | Acc:0.9036 | F1:0.8257\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 21:43:13,038 INFO: Epoch:[050/100]\n",
      "2022-04-29 21:43:13,038 INFO: Train Loss:0.441 | Acc:0.8977 | F1:0.8304\n",
      "2022-04-29 21:43:23,439 INFO: val Loss:0.296 | Acc:0.9442 | F1:0.8681\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 21:44:45,860 INFO: Epoch:[051/100]\n",
      "2022-04-29 21:44:45,860 INFO: Train Loss:0.391 | Acc:0.9008 | F1:0.8392\n",
      "2022-04-29 21:44:56,079 INFO: val Loss:0.324 | Acc:0.9401 | F1:0.8440\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:46:20,515 INFO: Epoch:[052/100]\n",
      "2022-04-29 21:46:20,516 INFO: Train Loss:0.448 | Acc:0.9048 | F1:0.8340\n",
      "2022-04-29 21:46:30,937 INFO: val Loss:0.520 | Acc:0.8761 | F1:0.8166\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 21:47:54,447 INFO: Epoch:[053/100]\n",
      "2022-04-29 21:47:54,448 INFO: Train Loss:0.392 | Acc:0.9129 | F1:0.8521\n",
      "2022-04-29 21:48:04,810 INFO: val Loss:0.321 | Acc:0.9472 | F1:0.8618\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 21:49:29,082 INFO: Epoch:[054/100]\n",
      "2022-04-29 21:49:29,083 INFO: Train Loss:0.366 | Acc:0.9221 | F1:0.8565\n",
      "2022-04-29 21:49:39,281 INFO: val Loss:0.247 | Acc:0.9574 | F1:0.9015\n",
      "2022-04-29 21:49:39,977 INFO: -----------------SAVE:54epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-29 21:51:02,869 INFO: Epoch:[055/100]\n",
      "2022-04-29 21:51:02,870 INFO: Train Loss:0.330 | Acc:0.9188 | F1:0.8675\n",
      "2022-04-29 21:51:13,353 INFO: val Loss:0.223 | Acc:0.9259 | F1:0.8868\n",
      "2022-04-29 21:51:13,968 INFO: -----------------SAVE:55epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 21:52:36,191 INFO: Epoch:[056/100]\n",
      "2022-04-29 21:52:36,192 INFO: Train Loss:0.319 | Acc:0.9254 | F1:0.8660\n",
      "2022-04-29 21:52:46,679 INFO: val Loss:0.288 | Acc:0.9320 | F1:0.8639\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 21:54:09,402 INFO: Epoch:[057/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 21:54:09,403 INFO: Train Loss:0.299 | Acc:0.9391 | F1:0.8902\n",
      "2022-04-29 21:54:19,853 INFO: val Loss:0.214 | Acc:0.9624 | F1:0.9088\n",
      "2022-04-29 21:54:20,526 INFO: -----------------SAVE:57epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 21:55:44,388 INFO: Epoch:[058/100]\n",
      "2022-04-29 21:55:44,389 INFO: Train Loss:0.277 | Acc:0.9391 | F1:0.8921\n",
      "2022-04-29 21:55:54,754 INFO: val Loss:0.215 | Acc:0.9533 | F1:0.8919\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 21:57:18,553 INFO: Epoch:[059/100]\n",
      "2022-04-29 21:57:18,554 INFO: Train Loss:0.254 | Acc:0.9360 | F1:0.8902\n",
      "2022-04-29 21:57:28,863 INFO: val Loss:0.202 | Acc:0.9604 | F1:0.8836\n",
      "2022-04-29 21:57:29,484 INFO: -----------------SAVE:59epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 21:58:51,967 INFO: Epoch:[060/100]\n",
      "2022-04-29 21:58:51,967 INFO: Train Loss:0.263 | Acc:0.9404 | F1:0.8891\n",
      "2022-04-29 21:59:02,500 INFO: val Loss:0.284 | Acc:0.9533 | F1:0.8792\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 22:00:26,999 INFO: Epoch:[061/100]\n",
      "2022-04-29 22:00:27,000 INFO: Train Loss:0.261 | Acc:0.9454 | F1:0.8981\n",
      "2022-04-29 22:00:37,506 INFO: val Loss:0.203 | Acc:0.9503 | F1:0.9102\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.98it/s]\n",
      "2022-04-29 22:02:00,522 INFO: Epoch:[062/100]\n",
      "2022-04-29 22:02:00,523 INFO: Train Loss:0.237 | Acc:0.9444 | F1:0.8987\n",
      "2022-04-29 22:02:11,117 INFO: val Loss:0.186 | Acc:0.9685 | F1:0.9158\n",
      "2022-04-29 22:02:11,810 INFO: -----------------SAVE:62epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 22:03:35,061 INFO: Epoch:[063/100]\n",
      "2022-04-29 22:03:35,062 INFO: Train Loss:0.228 | Acc:0.9462 | F1:0.9072\n",
      "2022-04-29 22:03:45,472 INFO: val Loss:0.192 | Acc:0.9360 | F1:0.9171\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 22:05:09,527 INFO: Epoch:[064/100]\n",
      "2022-04-29 22:05:09,528 INFO: Train Loss:0.216 | Acc:0.9477 | F1:0.9030\n",
      "2022-04-29 22:05:20,027 INFO: val Loss:0.193 | Acc:0.9584 | F1:0.9212\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 22:06:43,670 INFO: Epoch:[065/100]\n",
      "2022-04-29 22:06:43,671 INFO: Train Loss:0.191 | Acc:0.9556 | F1:0.9240\n",
      "2022-04-29 22:06:54,263 INFO: val Loss:0.213 | Acc:0.9746 | F1:0.9297\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.05it/s]\n",
      "2022-04-29 22:08:15,256 INFO: Epoch:[066/100]\n",
      "2022-04-29 22:08:15,257 INFO: Train Loss:0.182 | Acc:0.9586 | F1:0.9264\n",
      "2022-04-29 22:08:26,018 INFO: val Loss:0.145 | Acc:0.9706 | F1:0.9481\n",
      "2022-04-29 22:08:26,622 INFO: -----------------SAVE:66epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-29 22:09:48,179 INFO: Epoch:[067/100]\n",
      "2022-04-29 22:09:48,179 INFO: Train Loss:0.188 | Acc:0.9505 | F1:0.9098\n",
      "2022-04-29 22:09:58,609 INFO: val Loss:0.159 | Acc:0.9462 | F1:0.9400\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.91it/s]\n",
      "2022-04-29 22:11:23,555 INFO: Epoch:[068/100]\n",
      "2022-04-29 22:11:23,556 INFO: Train Loss:0.174 | Acc:0.9596 | F1:0.9341\n",
      "2022-04-29 22:11:33,993 INFO: val Loss:0.129 | Acc:0.9431 | F1:0.9290\n",
      "2022-04-29 22:11:34,654 INFO: -----------------SAVE:68epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 22:12:58,186 INFO: Epoch:[069/100]\n",
      "2022-04-29 22:12:58,187 INFO: Train Loss:0.157 | Acc:0.9624 | F1:0.9330\n",
      "2022-04-29 22:13:08,480 INFO: val Loss:0.180 | Acc:0.9655 | F1:0.9287\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 22:14:32,375 INFO: Epoch:[070/100]\n",
      "2022-04-29 22:14:32,376 INFO: Train Loss:0.150 | Acc:0.9657 | F1:0.9370\n",
      "2022-04-29 22:14:42,865 INFO: val Loss:0.218 | Acc:0.9431 | F1:0.9293\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 22:16:05,956 INFO: Epoch:[071/100]\n",
      "2022-04-29 22:16:05,957 INFO: Train Loss:0.165 | Acc:0.9642 | F1:0.9314\n",
      "2022-04-29 22:16:16,325 INFO: val Loss:0.185 | Acc:0.9736 | F1:0.9478\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 22:17:38,854 INFO: Epoch:[072/100]\n",
      "2022-04-29 22:17:38,855 INFO: Train Loss:0.147 | Acc:0.9640 | F1:0.9414\n",
      "2022-04-29 22:17:49,369 INFO: val Loss:0.197 | Acc:0.9574 | F1:0.9339\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 22:19:11,114 INFO: Epoch:[073/100]\n",
      "2022-04-29 22:19:11,115 INFO: Train Loss:0.118 | Acc:0.9726 | F1:0.9501\n",
      "2022-04-29 22:19:21,781 INFO: val Loss:0.154 | Acc:0.9787 | F1:0.9409\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 22:20:45,609 INFO: Epoch:[074/100]\n",
      "2022-04-29 22:20:45,610 INFO: Train Loss:0.132 | Acc:0.9721 | F1:0.9470\n",
      "2022-04-29 22:20:55,974 INFO: val Loss:0.196 | Acc:0.9726 | F1:0.9288\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 22:22:17,753 INFO: Epoch:[075/100]\n",
      "2022-04-29 22:22:17,753 INFO: Train Loss:0.106 | Acc:0.9728 | F1:0.9561\n",
      "2022-04-29 22:22:28,131 INFO: val Loss:0.139 | Acc:0.9695 | F1:0.9329\n",
      "2022-04-29 22:22:28,132 INFO: \n",
      "Best Val Epoch:68 | Val Loss:0.1286 | Val Acc:0.9431 | Val F1:0.9290\n",
      "2022-04-29 22:22:28,133 INFO: Total Process time:117.415Minute\n",
      "2022-04-29 22:22:28,138 INFO: {'exp_num': '3', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3940\n",
      "Dataset size:985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 22:22:28,546 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 22:23:52,674 INFO: Epoch:[001/100]\n",
      "2022-04-29 22:23:52,675 INFO: Train Loss:4.513 | Acc:0.0099 | F1:0.0025\n",
      "2022-04-29 22:24:03,294 INFO: val Loss:4.503 | Acc:0.0497 | F1:0.0154\n",
      "2022-04-29 22:24:03,911 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 22:25:25,274 INFO: Epoch:[002/100]\n",
      "2022-04-29 22:25:25,274 INFO: Train Loss:4.507 | Acc:0.0099 | F1:0.0030\n",
      "2022-04-29 22:25:35,752 INFO: val Loss:4.485 | Acc:0.0508 | F1:0.0086\n",
      "2022-04-29 22:25:36,380 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 22:26:59,786 INFO: Epoch:[003/100]\n",
      "2022-04-29 22:26:59,787 INFO: Train Loss:4.504 | Acc:0.0107 | F1:0.0033\n",
      "2022-04-29 22:27:10,374 INFO: val Loss:4.471 | Acc:0.0497 | F1:0.0128\n",
      "2022-04-29 22:27:11,066 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.91it/s]\n",
      "2022-04-29 22:28:35,887 INFO: Epoch:[004/100]\n",
      "2022-04-29 22:28:35,887 INFO: Train Loss:4.463 | Acc:0.0282 | F1:0.0115\n",
      "2022-04-29 22:28:46,472 INFO: val Loss:4.331 | Acc:0.0690 | F1:0.0205\n",
      "2022-04-29 22:28:47,127 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 22:30:10,988 INFO: Epoch:[005/100]\n",
      "2022-04-29 22:30:10,989 INFO: Train Loss:4.379 | Acc:0.0685 | F1:0.0196\n",
      "2022-04-29 22:30:21,559 INFO: val Loss:4.182 | Acc:0.3330 | F1:0.1072\n",
      "2022-04-29 22:30:22,221 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 22:31:43,388 INFO: Epoch:[006/100]\n",
      "2022-04-29 22:31:43,389 INFO: Train Loss:4.271 | Acc:0.2365 | F1:0.0707\n",
      "2022-04-29 22:31:53,715 INFO: val Loss:4.002 | Acc:0.5685 | F1:0.1521\n",
      "2022-04-29 22:31:54,337 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 22:33:18,074 INFO: Epoch:[007/100]\n",
      "2022-04-29 22:33:18,075 INFO: Train Loss:4.154 | Acc:0.4003 | F1:0.1114\n",
      "2022-04-29 22:33:28,488 INFO: val Loss:3.800 | Acc:0.6274 | F1:0.1636\n",
      "2022-04-29 22:33:29,202 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 22:34:53,286 INFO: Epoch:[008/100]\n",
      "2022-04-29 22:34:53,286 INFO: Train Loss:3.986 | Acc:0.5381 | F1:0.1508\n",
      "2022-04-29 22:35:03,724 INFO: val Loss:3.582 | Acc:0.7076 | F1:0.1757\n",
      "2022-04-29 22:35:04,354 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 22:36:27,831 INFO: Epoch:[009/100]\n",
      "2022-04-29 22:36:27,832 INFO: Train Loss:3.779 | Acc:0.6165 | F1:0.1718\n",
      "2022-04-29 22:36:38,242 INFO: val Loss:3.268 | Acc:0.7289 | F1:0.1622\n",
      "2022-04-29 22:36:38,897 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 22:38:02,213 INFO: Epoch:[010/100]\n",
      "2022-04-29 22:38:02,213 INFO: Train Loss:3.535 | Acc:0.6711 | F1:0.1861\n",
      "2022-04-29 22:38:12,669 INFO: val Loss:3.109 | Acc:0.7431 | F1:0.1660\n",
      "2022-04-29 22:38:13,343 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 22:39:37,253 INFO: Epoch:[011/100]\n",
      "2022-04-29 22:39:37,254 INFO: Train Loss:3.296 | Acc:0.6787 | F1:0.1895\n",
      "2022-04-29 22:39:47,589 INFO: val Loss:2.823 | Acc:0.7401 | F1:0.1716\n",
      "2022-04-29 22:39:48,243 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-29 22:41:09,964 INFO: Epoch:[012/100]\n",
      "2022-04-29 22:41:09,965 INFO: Train Loss:3.067 | Acc:0.6731 | F1:0.1937\n",
      "2022-04-29 22:41:20,516 INFO: val Loss:2.530 | Acc:0.5909 | F1:0.1501\n",
      "2022-04-29 22:41:21,135 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 22:42:44,275 INFO: Epoch:[013/100]\n",
      "2022-04-29 22:42:44,276 INFO: Train Loss:2.852 | Acc:0.6548 | F1:0.2050\n",
      "2022-04-29 22:42:54,784 INFO: val Loss:2.330 | Acc:0.6325 | F1:0.1824\n",
      "2022-04-29 22:42:55,399 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 22:44:19,039 INFO: Epoch:[014/100]\n",
      "2022-04-29 22:44:19,039 INFO: Train Loss:2.659 | Acc:0.5980 | F1:0.2231\n",
      "2022-04-29 22:44:29,508 INFO: val Loss:2.288 | Acc:0.6497 | F1:0.1807\n",
      "2022-04-29 22:44:30,029 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 22:45:53,935 INFO: Epoch:[015/100]\n",
      "2022-04-29 22:45:53,935 INFO: Train Loss:2.525 | Acc:0.6371 | F1:0.2364\n",
      "2022-04-29 22:46:04,618 INFO: val Loss:1.950 | Acc:0.6010 | F1:0.2130\n",
      "2022-04-29 22:46:05,296 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.91it/s]\n",
      "2022-04-29 22:47:30,163 INFO: Epoch:[016/100]\n",
      "2022-04-29 22:47:30,164 INFO: Train Loss:2.335 | Acc:0.6650 | F1:0.2689\n",
      "2022-04-29 22:47:40,678 INFO: val Loss:1.947 | Acc:0.6934 | F1:0.2484\n",
      "2022-04-29 22:47:41,289 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.01it/s]\n",
      "2022-04-29 22:49:03,288 INFO: Epoch:[017/100]\n",
      "2022-04-29 22:49:03,289 INFO: Train Loss:2.283 | Acc:0.6112 | F1:0.2433\n",
      "2022-04-29 22:49:13,562 INFO: val Loss:1.787 | Acc:0.7340 | F1:0.2617\n",
      "2022-04-29 22:49:14,174 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-29 22:50:38,717 INFO: Epoch:[018/100]\n",
      "2022-04-29 22:50:38,717 INFO: Train Loss:2.219 | Acc:0.6338 | F1:0.2713\n",
      "2022-04-29 22:50:49,286 INFO: val Loss:1.753 | Acc:0.6386 | F1:0.2467\n",
      "2022-04-29 22:50:49,908 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 22:52:13,404 INFO: Epoch:[019/100]\n",
      "2022-04-29 22:52:13,405 INFO: Train Loss:2.114 | Acc:0.6388 | F1:0.2798\n",
      "2022-04-29 22:52:23,886 INFO: val Loss:1.651 | Acc:0.6315 | F1:0.2752\n",
      "2022-04-29 22:52:24,492 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 22:53:47,533 INFO: Epoch:[020/100]\n",
      "2022-04-29 22:53:47,534 INFO: Train Loss:2.094 | Acc:0.6536 | F1:0.2882\n",
      "2022-04-29 22:53:57,951 INFO: val Loss:1.576 | Acc:0.7706 | F1:0.3095\n",
      "2022-04-29 22:53:58,592 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 22:55:22,939 INFO: Epoch:[021/100]\n",
      "2022-04-29 22:55:22,940 INFO: Train Loss:1.953 | Acc:0.6680 | F1:0.3314\n",
      "2022-04-29 22:55:33,470 INFO: val Loss:1.508 | Acc:0.7360 | F1:0.3494\n",
      "2022-04-29 22:55:34,081 INFO: -----------------SAVE:21epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 22:56:58,037 INFO: Epoch:[022/100]\n",
      "2022-04-29 22:56:58,038 INFO: Train Loss:1.871 | Acc:0.6690 | F1:0.3335\n",
      "2022-04-29 22:57:08,636 INFO: val Loss:1.494 | Acc:0.6619 | F1:0.3279\n",
      "2022-04-29 22:57:09,343 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 22:58:32,790 INFO: Epoch:[023/100]\n",
      "2022-04-29 22:58:32,791 INFO: Train Loss:1.840 | Acc:0.6645 | F1:0.3492\n",
      "2022-04-29 22:58:43,063 INFO: val Loss:1.313 | Acc:0.7299 | F1:0.3831\n",
      "2022-04-29 22:58:43,718 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:00:07,578 INFO: Epoch:[024/100]\n",
      "2022-04-29 23:00:07,579 INFO: Train Loss:1.740 | Acc:0.6883 | F1:0.3710\n",
      "2022-04-29 23:00:18,074 INFO: val Loss:1.297 | Acc:0.7269 | F1:0.3972\n",
      "2022-04-29 23:00:18,709 INFO: -----------------SAVE:24epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:01:42,323 INFO: Epoch:[025/100]\n",
      "2022-04-29 23:01:42,324 INFO: Train Loss:1.670 | Acc:0.6990 | F1:0.4104\n",
      "2022-04-29 23:01:52,530 INFO: val Loss:1.270 | Acc:0.7279 | F1:0.3860\n",
      "2022-04-29 23:01:53,184 INFO: -----------------SAVE:25epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 23:03:17,248 INFO: Epoch:[026/100]\n",
      "2022-04-29 23:03:17,248 INFO: Train Loss:1.578 | Acc:0.7145 | F1:0.4244\n",
      "2022-04-29 23:03:28,057 INFO: val Loss:1.147 | Acc:0.7655 | F1:0.4464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 23:03:28,703 INFO: -----------------SAVE:26epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:04:52,351 INFO: Epoch:[027/100]\n",
      "2022-04-29 23:04:52,352 INFO: Train Loss:1.470 | Acc:0.7294 | F1:0.4629\n",
      "2022-04-29 23:05:03,004 INFO: val Loss:1.186 | Acc:0.6386 | F1:0.4472\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.05it/s]\n",
      "2022-04-29 23:06:23,978 INFO: Epoch:[028/100]\n",
      "2022-04-29 23:06:23,979 INFO: Train Loss:1.420 | Acc:0.7203 | F1:0.4737\n",
      "2022-04-29 23:06:34,729 INFO: val Loss:0.999 | Acc:0.8173 | F1:0.5175\n",
      "2022-04-29 23:06:35,354 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 23:07:56,499 INFO: Epoch:[029/100]\n",
      "2022-04-29 23:07:56,499 INFO: Train Loss:1.344 | Acc:0.7510 | F1:0.5095\n",
      "2022-04-29 23:08:06,816 INFO: val Loss:1.119 | Acc:0.7726 | F1:0.5020\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.01it/s]\n",
      "2022-04-29 23:09:28,756 INFO: Epoch:[030/100]\n",
      "2022-04-29 23:09:28,757 INFO: Train Loss:1.294 | Acc:0.7396 | F1:0.5241\n",
      "2022-04-29 23:09:39,128 INFO: val Loss:0.984 | Acc:0.8345 | F1:0.5511\n",
      "2022-04-29 23:09:39,751 INFO: -----------------SAVE:30epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:11:03,350 INFO: Epoch:[031/100]\n",
      "2022-04-29 23:11:03,351 INFO: Train Loss:1.185 | Acc:0.7675 | F1:0.5596\n",
      "2022-04-29 23:11:13,762 INFO: val Loss:1.180 | Acc:0.7096 | F1:0.4897\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 23:12:36,274 INFO: Epoch:[032/100]\n",
      "2022-04-29 23:12:36,275 INFO: Train Loss:1.134 | Acc:0.7665 | F1:0.5790\n",
      "2022-04-29 23:12:46,668 INFO: val Loss:0.889 | Acc:0.7421 | F1:0.5644\n",
      "2022-04-29 23:12:47,306 INFO: -----------------SAVE:32epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:14:11,000 INFO: Epoch:[033/100]\n",
      "2022-04-29 23:14:11,001 INFO: Train Loss:1.088 | Acc:0.7777 | F1:0.5874\n",
      "2022-04-29 23:14:21,512 INFO: val Loss:0.827 | Acc:0.8132 | F1:0.5990\n",
      "2022-04-29 23:14:22,137 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 23:15:46,035 INFO: Epoch:[034/100]\n",
      "2022-04-29 23:15:46,036 INFO: Train Loss:1.061 | Acc:0.7954 | F1:0.6156\n",
      "2022-04-29 23:15:56,495 INFO: val Loss:0.803 | Acc:0.7990 | F1:0.6430\n",
      "2022-04-29 23:15:57,106 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 23:17:20,447 INFO: Epoch:[035/100]\n",
      "2022-04-29 23:17:20,447 INFO: Train Loss:0.950 | Acc:0.8089 | F1:0.6467\n",
      "2022-04-29 23:17:30,897 INFO: val Loss:0.734 | Acc:0.8640 | F1:0.6217\n",
      "2022-04-29 23:17:31,588 INFO: -----------------SAVE:35epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 23:18:55,519 INFO: Epoch:[036/100]\n",
      "2022-04-29 23:18:55,520 INFO: Train Loss:0.931 | Acc:0.8102 | F1:0.6535\n",
      "2022-04-29 23:19:05,842 INFO: val Loss:0.740 | Acc:0.8802 | F1:0.6751\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 23:20:29,150 INFO: Epoch:[037/100]\n",
      "2022-04-29 23:20:29,150 INFO: Train Loss:0.839 | Acc:0.8325 | F1:0.6870\n",
      "2022-04-29 23:20:39,703 INFO: val Loss:0.663 | Acc:0.8650 | F1:0.6576\n",
      "2022-04-29 23:20:40,327 INFO: -----------------SAVE:37epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-29 23:22:04,269 INFO: Epoch:[038/100]\n",
      "2022-04-29 23:22:04,270 INFO: Train Loss:0.790 | Acc:0.8416 | F1:0.7017\n",
      "2022-04-29 23:22:14,707 INFO: val Loss:0.545 | Acc:0.9005 | F1:0.7449\n",
      "2022-04-29 23:22:15,329 INFO: -----------------SAVE:38epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 23:23:38,792 INFO: Epoch:[039/100]\n",
      "2022-04-29 23:23:38,792 INFO: Train Loss:0.770 | Acc:0.8485 | F1:0.7116\n",
      "2022-04-29 23:23:49,199 INFO: val Loss:0.598 | Acc:0.8396 | F1:0.6967\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 23:25:11,557 INFO: Epoch:[040/100]\n",
      "2022-04-29 23:25:11,558 INFO: Train Loss:0.718 | Acc:0.8404 | F1:0.7262\n",
      "2022-04-29 23:25:21,819 INFO: val Loss:0.515 | Acc:0.9066 | F1:0.7518\n",
      "2022-04-29 23:25:22,471 INFO: -----------------SAVE:40epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:26:46,354 INFO: Epoch:[041/100]\n",
      "2022-04-29 23:26:46,354 INFO: Train Loss:0.713 | Acc:0.8553 | F1:0.7351\n",
      "2022-04-29 23:26:56,711 INFO: val Loss:0.659 | Acc:0.8924 | F1:0.7063\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 23:28:20,168 INFO: Epoch:[042/100]\n",
      "2022-04-29 23:28:20,168 INFO: Train Loss:0.605 | Acc:0.8629 | F1:0.7666\n",
      "2022-04-29 23:28:30,569 INFO: val Loss:0.569 | Acc:0.8538 | F1:0.7573\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.01it/s]\n",
      "2022-04-29 23:29:52,591 INFO: Epoch:[043/100]\n",
      "2022-04-29 23:29:52,592 INFO: Train Loss:0.612 | Acc:0.8619 | F1:0.7616\n",
      "2022-04-29 23:30:03,255 INFO: val Loss:0.452 | Acc:0.8234 | F1:0.7787\n",
      "2022-04-29 23:30:03,899 INFO: -----------------SAVE:43epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 23:31:27,027 INFO: Epoch:[044/100]\n",
      "2022-04-29 23:31:27,028 INFO: Train Loss:0.540 | Acc:0.8845 | F1:0.7900\n",
      "2022-04-29 23:31:37,647 INFO: val Loss:0.487 | Acc:0.8609 | F1:0.7961\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 23:33:00,048 INFO: Epoch:[045/100]\n",
      "2022-04-29 23:33:00,049 INFO: Train Loss:0.560 | Acc:0.8825 | F1:0.7919\n",
      "2022-04-29 23:33:10,643 INFO: val Loss:0.494 | Acc:0.8792 | F1:0.8217\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-29 23:34:31,923 INFO: Epoch:[046/100]\n",
      "2022-04-29 23:34:31,924 INFO: Train Loss:0.478 | Acc:0.8921 | F1:0.8158\n",
      "2022-04-29 23:34:42,382 INFO: val Loss:0.535 | Acc:0.8782 | F1:0.7697\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 23:36:06,636 INFO: Epoch:[047/100]\n",
      "2022-04-29 23:36:06,637 INFO: Train Loss:0.519 | Acc:0.8934 | F1:0.8162\n",
      "2022-04-29 23:36:17,119 INFO: val Loss:0.391 | Acc:0.8386 | F1:0.8089\n",
      "2022-04-29 23:36:17,746 INFO: -----------------SAVE:47epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 23:37:41,761 INFO: Epoch:[048/100]\n",
      "2022-04-29 23:37:41,762 INFO: Train Loss:0.454 | Acc:0.8868 | F1:0.8198\n",
      "2022-04-29 23:37:52,306 INFO: val Loss:0.362 | Acc:0.8832 | F1:0.8176\n",
      "2022-04-29 23:37:52,915 INFO: -----------------SAVE:48epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:39:16,757 INFO: Epoch:[049/100]\n",
      "2022-04-29 23:39:16,758 INFO: Train Loss:0.412 | Acc:0.8954 | F1:0.8347\n",
      "2022-04-29 23:39:27,189 INFO: val Loss:0.265 | Acc:0.9310 | F1:0.8817\n",
      "2022-04-29 23:39:27,804 INFO: -----------------SAVE:49epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-29 23:40:50,440 INFO: Epoch:[050/100]\n",
      "2022-04-29 23:40:50,440 INFO: Train Loss:0.419 | Acc:0.9000 | F1:0.8337\n",
      "2022-04-29 23:41:01,016 INFO: val Loss:0.360 | Acc:0.8853 | F1:0.8430\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-29 23:42:25,246 INFO: Epoch:[051/100]\n",
      "2022-04-29 23:42:25,247 INFO: Train Loss:0.355 | Acc:0.9137 | F1:0.8568\n",
      "2022-04-29 23:42:35,651 INFO: val Loss:0.361 | Acc:0.9208 | F1:0.8609\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.07it/s]\n",
      "2022-04-29 23:43:56,155 INFO: Epoch:[052/100]\n",
      "2022-04-29 23:43:56,156 INFO: Train Loss:0.358 | Acc:0.9173 | F1:0.8602\n",
      "2022-04-29 23:44:06,563 INFO: val Loss:0.380 | Acc:0.8680 | F1:0.8235\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.98it/s]\n",
      "2022-04-29 23:45:29,578 INFO: Epoch:[053/100]\n",
      "2022-04-29 23:45:29,579 INFO: Train Loss:0.386 | Acc:0.9099 | F1:0.8513\n",
      "2022-04-29 23:45:40,072 INFO: val Loss:0.262 | Acc:0.9492 | F1:0.8815\n",
      "2022-04-29 23:45:40,706 INFO: -----------------SAVE:53epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-29 23:47:04,065 INFO: Epoch:[054/100]\n",
      "2022-04-29 23:47:04,066 INFO: Train Loss:0.346 | Acc:0.9175 | F1:0.8685\n",
      "2022-04-29 23:47:14,480 INFO: val Loss:0.268 | Acc:0.9168 | F1:0.8748\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 23:48:36,845 INFO: Epoch:[055/100]\n",
      "2022-04-29 23:48:36,846 INFO: Train Loss:0.297 | Acc:0.9251 | F1:0.8693\n",
      "2022-04-29 23:48:47,311 INFO: val Loss:0.309 | Acc:0.9198 | F1:0.8810\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 23:50:10,473 INFO: Epoch:[056/100]\n",
      "2022-04-29 23:50:10,474 INFO: Train Loss:0.305 | Acc:0.9302 | F1:0.8780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 23:50:21,071 INFO: val Loss:0.254 | Acc:0.9614 | F1:0.9029\n",
      "2022-04-29 23:50:21,683 INFO: -----------------SAVE:56epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:51:45,429 INFO: Epoch:[057/100]\n",
      "2022-04-29 23:51:45,429 INFO: Train Loss:0.304 | Acc:0.9401 | F1:0.8941\n",
      "2022-04-29 23:51:55,885 INFO: val Loss:0.310 | Acc:0.9117 | F1:0.8729\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-29 23:53:18,149 INFO: Epoch:[058/100]\n",
      "2022-04-29 23:53:18,150 INFO: Train Loss:0.294 | Acc:0.9228 | F1:0.8835\n",
      "2022-04-29 23:53:28,758 INFO: val Loss:0.167 | Acc:0.9279 | F1:0.9068\n",
      "2022-04-29 23:53:29,420 INFO: -----------------SAVE:58epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-29 23:54:53,491 INFO: Epoch:[059/100]\n",
      "2022-04-29 23:54:53,492 INFO: Train Loss:0.256 | Acc:0.9249 | F1:0.8905\n",
      "2022-04-29 23:55:03,954 INFO: val Loss:0.300 | Acc:0.9431 | F1:0.9082\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-29 23:56:27,595 INFO: Epoch:[060/100]\n",
      "2022-04-29 23:56:27,596 INFO: Train Loss:0.269 | Acc:0.9264 | F1:0.8926\n",
      "2022-04-29 23:56:38,194 INFO: val Loss:0.321 | Acc:0.8731 | F1:0.8722\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-29 23:58:01,461 INFO: Epoch:[061/100]\n",
      "2022-04-29 23:58:01,461 INFO: Train Loss:0.240 | Acc:0.9406 | F1:0.9078\n",
      "2022-04-29 23:58:11,750 INFO: val Loss:0.230 | Acc:0.9452 | F1:0.8839\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.05it/s]\n",
      "2022-04-29 23:59:32,613 INFO: Epoch:[062/100]\n",
      "2022-04-29 23:59:32,614 INFO: Train Loss:0.248 | Acc:0.9332 | F1:0.8916\n",
      "2022-04-29 23:59:43,024 INFO: val Loss:0.244 | Acc:0.9310 | F1:0.8760\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 00:01:06,242 INFO: Epoch:[063/100]\n",
      "2022-04-30 00:01:06,243 INFO: Train Loss:0.237 | Acc:0.9429 | F1:0.9055\n",
      "2022-04-30 00:01:16,777 INFO: val Loss:0.207 | Acc:0.9533 | F1:0.8840\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-30 00:02:40,770 INFO: Epoch:[064/100]\n",
      "2022-04-30 00:02:40,771 INFO: Train Loss:0.192 | Acc:0.9528 | F1:0.9233\n",
      "2022-04-30 00:02:51,213 INFO: val Loss:0.182 | Acc:0.9553 | F1:0.9050\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 00:04:14,433 INFO: Epoch:[065/100]\n",
      "2022-04-30 00:04:14,434 INFO: Train Loss:0.205 | Acc:0.9444 | F1:0.9253\n",
      "2022-04-30 00:04:24,881 INFO: val Loss:0.186 | Acc:0.9208 | F1:0.9110\n",
      "2022-04-30 00:04:24,882 INFO: \n",
      "Best Val Epoch:58 | Val Loss:0.1670 | Val Acc:0.9279 | Val F1:0.9068\n",
      "2022-04-30 00:04:24,883 INFO: Total Process time:101.935Minute\n",
      "2022-04-30 00:04:24,886 INFO: {'exp_num': '4', 'data_path': './open', 'Kfold': 5, 'model_path': 'label_results/', 'image_type': 'train_1024', 'class_num': 88, 'model_name': 'regnety_040', 'drop_path_rate': 0.2, 'img_size': 512, 'batch_size': 16, 'epochs': 100, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 3, 'max_lr': 0.001, 'min_lr': 5e-05, 'tmax': 145, 'patience': 7, 'clipping': None, 'amp': True, 'multi_gpu': True, 'logging': False, 'num_workers': 4, 'seed': 42, 'step': 0, 'fold': 4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "Dataset size:3940\n",
      "Dataset size:985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:04:25,212 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:05:49,061 INFO: Epoch:[001/100]\n",
      "2022-04-30 00:05:49,062 INFO: Train Loss:4.509 | Acc:0.0129 | F1:0.0039\n",
      "2022-04-30 00:05:59,472 INFO: val Loss:4.494 | Acc:0.0406 | F1:0.0108\n",
      "2022-04-30 00:06:00,099 INFO: -----------------SAVE:1epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 00:07:23,582 INFO: Epoch:[002/100]\n",
      "2022-04-30 00:07:23,583 INFO: Train Loss:4.498 | Acc:0.0117 | F1:0.0033\n",
      "2022-04-30 00:07:34,346 INFO: val Loss:4.472 | Acc:0.0467 | F1:0.0084\n",
      "2022-04-30 00:07:34,993 INFO: -----------------SAVE:2epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:08:58,836 INFO: Epoch:[003/100]\n",
      "2022-04-30 00:08:58,836 INFO: Train Loss:4.500 | Acc:0.0124 | F1:0.0039\n",
      "2022-04-30 00:09:09,352 INFO: val Loss:4.463 | Acc:0.0599 | F1:0.0112\n",
      "2022-04-30 00:09:09,964 INFO: -----------------SAVE:3epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:10:33,592 INFO: Epoch:[004/100]\n",
      "2022-04-30 00:10:33,593 INFO: Train Loss:4.454 | Acc:0.0284 | F1:0.0091\n",
      "2022-04-30 00:10:44,095 INFO: val Loss:4.327 | Acc:0.0711 | F1:0.0236\n",
      "2022-04-30 00:10:44,719 INFO: -----------------SAVE:4epoch----------------\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.05it/s]\n",
      "2022-04-30 00:12:05,668 INFO: Epoch:[005/100]\n",
      "2022-04-30 00:12:05,669 INFO: Train Loss:4.375 | Acc:0.0741 | F1:0.0221\n",
      "2022-04-30 00:12:16,205 INFO: val Loss:4.184 | Acc:0.2518 | F1:0.0948\n",
      "2022-04-30 00:12:16,853 INFO: -----------------SAVE:5epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 00:13:41,096 INFO: Epoch:[006/100]\n",
      "2022-04-30 00:13:41,097 INFO: Train Loss:4.271 | Acc:0.2071 | F1:0.0708\n",
      "2022-04-30 00:13:51,492 INFO: val Loss:3.988 | Acc:0.4619 | F1:0.1331\n",
      "2022-04-30 00:13:52,213 INFO: -----------------SAVE:6epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 00:15:16,660 INFO: Epoch:[007/100]\n",
      "2022-04-30 00:15:16,661 INFO: Train Loss:4.142 | Acc:0.3622 | F1:0.1004\n",
      "2022-04-30 00:15:27,252 INFO: val Loss:3.798 | Acc:0.6447 | F1:0.1871\n",
      "2022-04-30 00:15:27,901 INFO: -----------------SAVE:7epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 00:16:52,188 INFO: Epoch:[008/100]\n",
      "2022-04-30 00:16:52,189 INFO: Train Loss:3.979 | Acc:0.5279 | F1:0.1475\n",
      "2022-04-30 00:17:02,762 INFO: val Loss:3.554 | Acc:0.7178 | F1:0.2025\n",
      "2022-04-30 00:17:03,387 INFO: -----------------SAVE:8epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-30 00:18:28,072 INFO: Epoch:[009/100]\n",
      "2022-04-30 00:18:28,072 INFO: Train Loss:3.772 | Acc:0.6332 | F1:0.1655\n",
      "2022-04-30 00:18:38,790 INFO: val Loss:3.278 | Acc:0.7523 | F1:0.1865\n",
      "2022-04-30 00:18:39,418 INFO: -----------------SAVE:9epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 00:20:02,641 INFO: Epoch:[010/100]\n",
      "2022-04-30 00:20:02,642 INFO: Train Loss:3.532 | Acc:0.6698 | F1:0.1841\n",
      "2022-04-30 00:20:13,180 INFO: val Loss:3.147 | Acc:0.7462 | F1:0.1895\n",
      "2022-04-30 00:20:13,796 INFO: -----------------SAVE:10epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 00:21:37,194 INFO: Epoch:[011/100]\n",
      "2022-04-30 00:21:37,195 INFO: Train Loss:3.305 | Acc:0.6886 | F1:0.1879\n",
      "2022-04-30 00:21:47,977 INFO: val Loss:2.755 | Acc:0.6822 | F1:0.1788\n",
      "2022-04-30 00:21:48,590 INFO: -----------------SAVE:11epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:23:12,244 INFO: Epoch:[012/100]\n",
      "2022-04-30 00:23:12,245 INFO: Train Loss:3.073 | Acc:0.6652 | F1:0.1880\n",
      "2022-04-30 00:23:22,888 INFO: val Loss:2.475 | Acc:0.7401 | F1:0.1932\n",
      "2022-04-30 00:23:23,504 INFO: -----------------SAVE:12epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 00:24:47,681 INFO: Epoch:[013/100]\n",
      "2022-04-30 00:24:47,681 INFO: Train Loss:2.842 | Acc:0.6530 | F1:0.2148\n",
      "2022-04-30 00:24:58,282 INFO: val Loss:2.401 | Acc:0.6893 | F1:0.1907\n",
      "2022-04-30 00:24:58,902 INFO: -----------------SAVE:13epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:26:22,633 INFO: Epoch:[014/100]\n",
      "2022-04-30 00:26:22,634 INFO: Train Loss:2.674 | Acc:0.6150 | F1:0.2027\n",
      "2022-04-30 00:26:33,268 INFO: val Loss:2.188 | Acc:0.6325 | F1:0.2037\n",
      "2022-04-30 00:26:33,880 INFO: -----------------SAVE:14epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:27:57,751 INFO: Epoch:[015/100]\n",
      "2022-04-30 00:27:57,751 INFO: Train Loss:2.574 | Acc:0.5944 | F1:0.2212\n",
      "2022-04-30 00:28:08,393 INFO: val Loss:1.997 | Acc:0.5909 | F1:0.2064\n",
      "2022-04-30 00:28:09,005 INFO: -----------------SAVE:15epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.01it/s]\n",
      "2022-04-30 00:29:31,004 INFO: Epoch:[016/100]\n",
      "2022-04-30 00:29:31,005 INFO: Train Loss:2.376 | Acc:0.6284 | F1:0.2410\n",
      "2022-04-30 00:29:41,335 INFO: val Loss:1.972 | Acc:0.6396 | F1:0.2291\n",
      "2022-04-30 00:29:41,954 INFO: -----------------SAVE:16epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-30 00:31:06,607 INFO: Epoch:[017/100]\n",
      "2022-04-30 00:31:06,608 INFO: Train Loss:2.277 | Acc:0.5926 | F1:0.2365\n",
      "2022-04-30 00:31:17,087 INFO: val Loss:1.840 | Acc:0.6315 | F1:0.2534\n",
      "2022-04-30 00:31:17,718 INFO: -----------------SAVE:17epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-30 00:32:39,378 INFO: Epoch:[018/100]\n",
      "2022-04-30 00:32:39,379 INFO: Train Loss:2.200 | Acc:0.6058 | F1:0.2627\n",
      "2022-04-30 00:32:50,073 INFO: val Loss:1.771 | Acc:0.5249 | F1:0.2369\n",
      "2022-04-30 00:32:50,726 INFO: -----------------SAVE:18epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 00:34:15,036 INFO: Epoch:[019/100]\n",
      "2022-04-30 00:34:15,037 INFO: Train Loss:2.138 | Acc:0.6195 | F1:0.2675\n",
      "2022-04-30 00:34:25,669 INFO: val Loss:1.666 | Acc:0.7350 | F1:0.3044\n",
      "2022-04-30 00:34:26,294 INFO: -----------------SAVE:19epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.02it/s]\n",
      "2022-04-30 00:35:48,077 INFO: Epoch:[020/100]\n",
      "2022-04-30 00:35:48,078 INFO: Train Loss:2.106 | Acc:0.6376 | F1:0.2860\n",
      "2022-04-30 00:35:58,750 INFO: val Loss:1.565 | Acc:0.7096 | F1:0.2893\n",
      "2022-04-30 00:35:59,384 INFO: -----------------SAVE:20epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-30 00:37:22,348 INFO: Epoch:[021/100]\n",
      "2022-04-30 00:37:22,349 INFO: Train Loss:1.991 | Acc:0.6190 | F1:0.3017\n",
      "2022-04-30 00:37:33,088 INFO: val Loss:1.600 | Acc:0.6112 | F1:0.2859\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-30 00:38:54,229 INFO: Epoch:[022/100]\n",
      "2022-04-30 00:38:54,230 INFO: Train Loss:1.896 | Acc:0.6523 | F1:0.3287\n",
      "2022-04-30 00:39:04,588 INFO: val Loss:1.472 | Acc:0.4914 | F1:0.2615\n",
      "2022-04-30 00:39:05,204 INFO: -----------------SAVE:22epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 00:40:28,360 INFO: Epoch:[023/100]\n",
      "2022-04-30 00:40:28,361 INFO: Train Loss:1.842 | Acc:0.6543 | F1:0.3457\n",
      "2022-04-30 00:40:38,845 INFO: val Loss:1.352 | Acc:0.7787 | F1:0.3842\n",
      "2022-04-30 00:40:39,460 INFO: -----------------SAVE:23epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-30 00:42:02,334 INFO: Epoch:[024/100]\n",
      "2022-04-30 00:42:02,335 INFO: Train Loss:1.750 | Acc:0.6668 | F1:0.3686\n",
      "2022-04-30 00:42:13,138 INFO: val Loss:1.416 | Acc:0.7421 | F1:0.3736\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 00:43:37,153 INFO: Epoch:[025/100]\n",
      "2022-04-30 00:43:37,153 INFO: Train Loss:1.700 | Acc:0.6853 | F1:0.3954\n",
      "2022-04-30 00:43:47,922 INFO: val Loss:1.267 | Acc:0.7442 | F1:0.3981\n",
      "2022-04-30 00:43:48,567 INFO: -----------------SAVE:25epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 00:45:12,646 INFO: Epoch:[026/100]\n",
      "2022-04-30 00:45:12,647 INFO: Train Loss:1.614 | Acc:0.6926 | F1:0.4210\n",
      "2022-04-30 00:45:23,329 INFO: val Loss:1.347 | Acc:0.7980 | F1:0.4052\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-30 00:46:47,337 INFO: Epoch:[027/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 00:46:47,337 INFO: Train Loss:1.532 | Acc:0.7063 | F1:0.4335\n",
      "2022-04-30 00:46:58,204 INFO: val Loss:1.177 | Acc:0.6954 | F1:0.4504\n",
      "2022-04-30 00:46:58,819 INFO: -----------------SAVE:27epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 00:48:22,154 INFO: Epoch:[028/100]\n",
      "2022-04-30 00:48:22,155 INFO: Train Loss:1.430 | Acc:0.7129 | F1:0.4540\n",
      "2022-04-30 00:48:32,862 INFO: val Loss:1.156 | Acc:0.7350 | F1:0.4434\n",
      "2022-04-30 00:48:33,547 INFO: -----------------SAVE:28epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 00:49:57,045 INFO: Epoch:[029/100]\n",
      "2022-04-30 00:49:57,046 INFO: Train Loss:1.369 | Acc:0.7294 | F1:0.4942\n",
      "2022-04-30 00:50:07,510 INFO: val Loss:1.025 | Acc:0.8426 | F1:0.5302\n",
      "2022-04-30 00:50:08,124 INFO: -----------------SAVE:29epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 00:51:31,637 INFO: Epoch:[030/100]\n",
      "2022-04-30 00:51:31,638 INFO: Train Loss:1.292 | Acc:0.7424 | F1:0.5118\n",
      "2022-04-30 00:51:42,240 INFO: val Loss:1.084 | Acc:0.7584 | F1:0.4918\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-30 00:53:06,718 INFO: Epoch:[031/100]\n",
      "2022-04-30 00:53:06,718 INFO: Train Loss:1.225 | Acc:0.7447 | F1:0.5337\n",
      "2022-04-30 00:53:17,234 INFO: val Loss:0.868 | Acc:0.8071 | F1:0.5718\n",
      "2022-04-30 00:53:17,851 INFO: -----------------SAVE:31epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:54:41,515 INFO: Epoch:[032/100]\n",
      "2022-04-30 00:54:41,516 INFO: Train Loss:1.155 | Acc:0.7541 | F1:0.5541\n",
      "2022-04-30 00:54:52,217 INFO: val Loss:0.921 | Acc:0.7635 | F1:0.5603\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 00:56:15,846 INFO: Epoch:[033/100]\n",
      "2022-04-30 00:56:15,847 INFO: Train Loss:1.119 | Acc:0.7586 | F1:0.5918\n",
      "2022-04-30 00:56:26,530 INFO: val Loss:0.863 | Acc:0.8203 | F1:0.6090\n",
      "2022-04-30 00:56:27,147 INFO: -----------------SAVE:33epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 00:57:50,271 INFO: Epoch:[034/100]\n",
      "2022-04-30 00:57:50,272 INFO: Train Loss:1.041 | Acc:0.7810 | F1:0.6167\n",
      "2022-04-30 00:58:00,792 INFO: val Loss:0.761 | Acc:0.8183 | F1:0.6335\n",
      "2022-04-30 00:58:01,389 INFO: -----------------SAVE:34epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 00:59:25,711 INFO: Epoch:[035/100]\n",
      "2022-04-30 00:59:25,711 INFO: Train Loss:0.979 | Acc:0.7832 | F1:0.6197\n",
      "2022-04-30 00:59:36,449 INFO: val Loss:0.782 | Acc:0.7726 | F1:0.6168\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 01:01:00,558 INFO: Epoch:[036/100]\n",
      "2022-04-30 01:01:00,558 INFO: Train Loss:0.929 | Acc:0.7947 | F1:0.6411\n",
      "2022-04-30 01:01:11,130 INFO: val Loss:0.816 | Acc:0.7482 | F1:0.6119\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-30 01:02:32,768 INFO: Epoch:[037/100]\n",
      "2022-04-30 01:02:32,768 INFO: Train Loss:0.918 | Acc:0.7947 | F1:0.6458\n",
      "2022-04-30 01:02:43,417 INFO: val Loss:0.672 | Acc:0.8650 | F1:0.6742\n",
      "2022-04-30 01:02:44,083 INFO: -----------------SAVE:37epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 01:04:07,476 INFO: Epoch:[038/100]\n",
      "2022-04-30 01:04:07,477 INFO: Train Loss:0.868 | Acc:0.8084 | F1:0.6548\n",
      "2022-04-30 01:04:18,032 INFO: val Loss:0.672 | Acc:0.8325 | F1:0.6905\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 01:05:41,306 INFO: Epoch:[039/100]\n",
      "2022-04-30 01:05:41,307 INFO: Train Loss:0.806 | Acc:0.8221 | F1:0.6969\n",
      "2022-04-30 01:05:51,835 INFO: val Loss:0.657 | Acc:0.9046 | F1:0.7159\n",
      "2022-04-30 01:05:52,514 INFO: -----------------SAVE:39epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 01:07:16,544 INFO: Epoch:[040/100]\n",
      "2022-04-30 01:07:16,544 INFO: Train Loss:0.772 | Acc:0.8228 | F1:0.6975\n",
      "2022-04-30 01:07:27,305 INFO: val Loss:0.594 | Acc:0.8020 | F1:0.7066\n",
      "2022-04-30 01:07:27,919 INFO: -----------------SAVE:40epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 01:08:51,988 INFO: Epoch:[041/100]\n",
      "2022-04-30 01:08:51,989 INFO: Train Loss:0.727 | Acc:0.8294 | F1:0.7088\n",
      "2022-04-30 01:09:02,770 INFO: val Loss:0.723 | Acc:0.8863 | F1:0.6934\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 01:10:26,176 INFO: Epoch:[042/100]\n",
      "2022-04-30 01:10:26,177 INFO: Train Loss:0.692 | Acc:0.8350 | F1:0.7105\n",
      "2022-04-30 01:10:36,803 INFO: val Loss:0.557 | Acc:0.7766 | F1:0.6827\n",
      "2022-04-30 01:10:37,425 INFO: -----------------SAVE:42epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-30 01:12:00,358 INFO: Epoch:[043/100]\n",
      "2022-04-30 01:12:00,359 INFO: Train Loss:0.674 | Acc:0.8310 | F1:0.7212\n",
      "2022-04-30 01:12:11,077 INFO: val Loss:0.568 | Acc:0.8234 | F1:0.7272\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-30 01:13:33,564 INFO: Epoch:[044/100]\n",
      "2022-04-30 01:13:33,564 INFO: Train Loss:0.627 | Acc:0.8439 | F1:0.7517\n",
      "2022-04-30 01:13:44,000 INFO: val Loss:0.543 | Acc:0.8832 | F1:0.7299\n",
      "2022-04-30 01:13:44,653 INFO: -----------------SAVE:44epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.01it/s]\n",
      "2022-04-30 01:15:06,645 INFO: Epoch:[045/100]\n",
      "2022-04-30 01:15:06,646 INFO: Train Loss:0.613 | Acc:0.8652 | F1:0.7525\n",
      "2022-04-30 01:15:17,059 INFO: val Loss:0.505 | Acc:0.9137 | F1:0.7806\n",
      "2022-04-30 01:15:17,725 INFO: -----------------SAVE:45epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.01it/s]\n",
      "2022-04-30 01:16:39,787 INFO: Epoch:[046/100]\n",
      "2022-04-30 01:16:39,788 INFO: Train Loss:0.584 | Acc:0.8591 | F1:0.7668\n",
      "2022-04-30 01:16:50,513 INFO: val Loss:0.464 | Acc:0.9147 | F1:0.7818\n",
      "2022-04-30 01:16:51,153 INFO: -----------------SAVE:46epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-30 01:18:13,511 INFO: Epoch:[047/100]\n",
      "2022-04-30 01:18:13,512 INFO: Train Loss:0.513 | Acc:0.8652 | F1:0.7850\n",
      "2022-04-30 01:18:24,230 INFO: val Loss:0.528 | Acc:0.8964 | F1:0.7843\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-30 01:19:48,155 INFO: Epoch:[048/100]\n",
      "2022-04-30 01:19:48,156 INFO: Train Loss:0.551 | Acc:0.8695 | F1:0.7786\n",
      "2022-04-30 01:19:58,678 INFO: val Loss:0.457 | Acc:0.9198 | F1:0.8254\n",
      "2022-04-30 01:19:59,297 INFO: -----------------SAVE:48epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 01:21:23,333 INFO: Epoch:[049/100]\n",
      "2022-04-30 01:21:23,334 INFO: Train Loss:0.497 | Acc:0.8756 | F1:0.8045\n",
      "2022-04-30 01:21:33,835 INFO: val Loss:0.367 | Acc:0.9137 | F1:0.8330\n",
      "2022-04-30 01:21:34,453 INFO: -----------------SAVE:49epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-30 01:22:58,927 INFO: Epoch:[050/100]\n",
      "2022-04-30 01:22:58,928 INFO: Train Loss:0.468 | Acc:0.8787 | F1:0.8159\n",
      "2022-04-30 01:23:09,186 INFO: val Loss:0.661 | Acc:0.8731 | F1:0.7299\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:24:32,821 INFO: Epoch:[051/100]\n",
      "2022-04-30 01:24:32,822 INFO: Train Loss:0.468 | Acc:0.8909 | F1:0.8182\n",
      "2022-04-30 01:24:43,300 INFO: val Loss:0.438 | Acc:0.9371 | F1:0.8335\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:26:06,964 INFO: Epoch:[052/100]\n",
      "2022-04-30 01:26:06,965 INFO: Train Loss:0.441 | Acc:0.8916 | F1:0.8186\n",
      "2022-04-30 01:26:17,735 INFO: val Loss:0.511 | Acc:0.8741 | F1:0.8057\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:27:41,570 INFO: Epoch:[053/100]\n",
      "2022-04-30 01:27:41,570 INFO: Train Loss:0.428 | Acc:0.8967 | F1:0.8274\n",
      "2022-04-30 01:27:52,298 INFO: val Loss:0.394 | Acc:0.9401 | F1:0.8566\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:29:16,034 INFO: Epoch:[054/100]\n",
      "2022-04-30 01:29:16,035 INFO: Train Loss:0.406 | Acc:0.9015 | F1:0.8321\n",
      "2022-04-30 01:29:26,641 INFO: val Loss:0.356 | Acc:0.9310 | F1:0.8272\n",
      "2022-04-30 01:29:27,291 INFO: -----------------SAVE:54epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-30 01:30:51,270 INFO: Epoch:[055/100]\n",
      "2022-04-30 01:30:51,270 INFO: Train Loss:0.350 | Acc:0.9114 | F1:0.8519\n",
      "2022-04-30 01:31:01,838 INFO: val Loss:0.440 | Acc:0.9127 | F1:0.8368\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.93it/s]\n",
      "2022-04-30 01:32:26,033 INFO: Epoch:[056/100]\n",
      "2022-04-30 01:32:26,033 INFO: Train Loss:0.338 | Acc:0.9246 | F1:0.8687\n",
      "2022-04-30 01:32:36,945 INFO: val Loss:0.381 | Acc:0.9208 | F1:0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 01:34:00,362 INFO: Epoch:[057/100]\n",
      "2022-04-30 01:34:00,363 INFO: Train Loss:0.326 | Acc:0.9261 | F1:0.8705\n",
      "2022-04-30 01:34:11,151 INFO: val Loss:0.370 | Acc:0.9198 | F1:0.8485\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.01it/s]\n",
      "2022-04-30 01:35:33,099 INFO: Epoch:[058/100]\n",
      "2022-04-30 01:35:33,099 INFO: Train Loss:0.306 | Acc:0.9203 | F1:0.8621\n",
      "2022-04-30 01:35:43,795 INFO: val Loss:0.423 | Acc:0.9025 | F1:0.8493\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 01:37:06,937 INFO: Epoch:[059/100]\n",
      "2022-04-30 01:37:06,937 INFO: Train Loss:0.296 | Acc:0.9294 | F1:0.8771\n",
      "2022-04-30 01:37:17,569 INFO: val Loss:0.457 | Acc:0.9523 | F1:0.8650\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:38:41,210 INFO: Epoch:[060/100]\n",
      "2022-04-30 01:38:41,211 INFO: Train Loss:0.302 | Acc:0.9261 | F1:0.8778\n",
      "2022-04-30 01:38:51,884 INFO: val Loss:0.273 | Acc:0.9513 | F1:0.8911\n",
      "2022-04-30 01:38:52,535 INFO: -----------------SAVE:60epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 01:40:15,914 INFO: Epoch:[061/100]\n",
      "2022-04-30 01:40:15,915 INFO: Train Loss:0.251 | Acc:0.9266 | F1:0.8803\n",
      "2022-04-30 01:40:26,608 INFO: val Loss:0.378 | Acc:0.9239 | F1:0.8727\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-30 01:41:49,554 INFO: Epoch:[062/100]\n",
      "2022-04-30 01:41:49,554 INFO: Train Loss:0.275 | Acc:0.9266 | F1:0.8788\n",
      "2022-04-30 01:42:00,249 INFO: val Loss:0.241 | Acc:0.9360 | F1:0.9043\n",
      "2022-04-30 01:42:00,907 INFO: -----------------SAVE:62epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.98it/s]\n",
      "2022-04-30 01:43:23,932 INFO: Epoch:[063/100]\n",
      "2022-04-30 01:43:23,933 INFO: Train Loss:0.233 | Acc:0.9376 | F1:0.9084\n",
      "2022-04-30 01:43:34,452 INFO: val Loss:0.287 | Acc:0.9421 | F1:0.8907\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 01:44:57,771 INFO: Epoch:[064/100]\n",
      "2022-04-30 01:44:57,771 INFO: Train Loss:0.239 | Acc:0.9421 | F1:0.9072\n",
      "2022-04-30 01:45:08,384 INFO: val Loss:0.314 | Acc:0.9574 | F1:0.8852\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 01:46:31,840 INFO: Epoch:[065/100]\n",
      "2022-04-30 01:46:31,842 INFO: Train Loss:0.187 | Acc:0.9510 | F1:0.9222\n",
      "2022-04-30 01:46:42,663 INFO: val Loss:0.376 | Acc:0.9411 | F1:0.8779\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.94it/s]\n",
      "2022-04-30 01:48:06,743 INFO: Epoch:[066/100]\n",
      "2022-04-30 01:48:06,744 INFO: Train Loss:0.186 | Acc:0.9523 | F1:0.9206\n",
      "2022-04-30 01:48:17,482 INFO: val Loss:0.279 | Acc:0.9604 | F1:0.9072\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:49:41,144 INFO: Epoch:[067/100]\n",
      "2022-04-30 01:49:41,144 INFO: Train Loss:0.177 | Acc:0.9622 | F1:0.9332\n",
      "2022-04-30 01:49:51,955 INFO: val Loss:0.221 | Acc:0.9645 | F1:0.9128\n",
      "2022-04-30 01:49:52,590 INFO: -----------------SAVE:67epoch----------------\n",
      "100%|██████████| 247/247 [01:24<00:00,  2.92it/s]\n",
      "2022-04-30 01:51:17,211 INFO: Epoch:[068/100]\n",
      "2022-04-30 01:51:17,212 INFO: Train Loss:0.182 | Acc:0.9546 | F1:0.9140\n",
      "2022-04-30 01:51:27,836 INFO: val Loss:0.336 | Acc:0.9685 | F1:0.9094\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-30 01:52:50,801 INFO: Epoch:[069/100]\n",
      "2022-04-30 01:52:50,801 INFO: Train Loss:0.165 | Acc:0.9589 | F1:0.9338\n",
      "2022-04-30 01:53:01,456 INFO: val Loss:0.306 | Acc:0.9350 | F1:0.8892\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-30 01:54:24,183 INFO: Epoch:[070/100]\n",
      "2022-04-30 01:54:24,184 INFO: Train Loss:0.151 | Acc:0.9660 | F1:0.9425\n",
      "2022-04-30 01:54:34,714 INFO: val Loss:0.243 | Acc:0.9543 | F1:0.9018\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:55:58,406 INFO: Epoch:[071/100]\n",
      "2022-04-30 01:55:58,407 INFO: Train Loss:0.164 | Acc:0.9622 | F1:0.9320\n",
      "2022-04-30 01:56:09,081 INFO: val Loss:0.254 | Acc:0.9695 | F1:0.9189\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 01:57:32,932 INFO: Epoch:[072/100]\n",
      "2022-04-30 01:57:32,933 INFO: Train Loss:0.146 | Acc:0.9652 | F1:0.9393\n",
      "2022-04-30 01:57:43,585 INFO: val Loss:0.296 | Acc:0.9645 | F1:0.9120\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 01:59:06,632 INFO: Epoch:[073/100]\n",
      "2022-04-30 01:59:06,633 INFO: Train Loss:0.135 | Acc:0.9627 | F1:0.9422\n",
      "2022-04-30 01:59:17,115 INFO: val Loss:0.172 | Acc:0.9736 | F1:0.9446\n",
      "2022-04-30 01:59:17,780 INFO: -----------------SAVE:73epoch----------------\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 02:00:41,562 INFO: Epoch:[074/100]\n",
      "2022-04-30 02:00:41,562 INFO: Train Loss:0.108 | Acc:0.9787 | F1:0.9598\n",
      "2022-04-30 02:00:52,248 INFO: val Loss:0.351 | Acc:0.9635 | F1:0.8884\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.96it/s]\n",
      "2022-04-30 02:02:15,750 INFO: Epoch:[075/100]\n",
      "2022-04-30 02:02:15,751 INFO: Train Loss:0.114 | Acc:0.9728 | F1:0.9515\n",
      "2022-04-30 02:02:26,470 INFO: val Loss:0.280 | Acc:0.9716 | F1:0.9190\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.97it/s]\n",
      "2022-04-30 02:03:49,657 INFO: Epoch:[076/100]\n",
      "2022-04-30 02:03:49,657 INFO: Train Loss:0.110 | Acc:0.9779 | F1:0.9571\n",
      "2022-04-30 02:04:00,195 INFO: val Loss:0.165 | Acc:0.9838 | F1:0.9492\n",
      "2022-04-30 02:04:00,799 INFO: -----------------SAVE:76epoch----------------\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.98it/s]\n",
      "2022-04-30 02:05:23,690 INFO: Epoch:[077/100]\n",
      "2022-04-30 02:05:23,690 INFO: Train Loss:0.088 | Acc:0.9794 | F1:0.9668\n",
      "2022-04-30 02:05:34,397 INFO: val Loss:0.130 | Acc:0.9756 | F1:0.9461\n",
      "2022-04-30 02:05:35,014 INFO: -----------------SAVE:77epoch----------------\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.03it/s]\n",
      "2022-04-30 02:06:56,576 INFO: Epoch:[078/100]\n",
      "2022-04-30 02:06:56,576 INFO: Train Loss:0.099 | Acc:0.9810 | F1:0.9647\n",
      "2022-04-30 02:07:07,193 INFO: val Loss:0.198 | Acc:0.9482 | F1:0.9278\n",
      "100%|██████████| 247/247 [01:22<00:00,  2.99it/s]\n",
      "2022-04-30 02:08:29,749 INFO: Epoch:[079/100]\n",
      "2022-04-30 02:08:29,750 INFO: Train Loss:0.089 | Acc:0.9825 | F1:0.9661\n",
      "2022-04-30 02:08:40,502 INFO: val Loss:0.197 | Acc:0.9726 | F1:0.9272\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.95it/s]\n",
      "2022-04-30 02:10:04,261 INFO: Epoch:[080/100]\n",
      "2022-04-30 02:10:04,261 INFO: Train Loss:0.105 | Acc:0.9751 | F1:0.9603\n",
      "2022-04-30 02:10:15,082 INFO: val Loss:0.202 | Acc:0.9635 | F1:0.9158\n",
      "100%|██████████| 247/247 [01:21<00:00,  3.04it/s]\n",
      "2022-04-30 02:11:36,320 INFO: Epoch:[081/100]\n",
      "2022-04-30 02:11:36,321 INFO: Train Loss:0.088 | Acc:0.9805 | F1:0.9633\n",
      "2022-04-30 02:11:46,935 INFO: val Loss:0.295 | Acc:0.9340 | F1:0.9155\n",
      "100%|██████████| 247/247 [01:20<00:00,  3.07it/s]\n",
      "2022-04-30 02:13:07,426 INFO: Epoch:[082/100]\n",
      "2022-04-30 02:13:07,427 INFO: Train Loss:0.064 | Acc:0.9835 | F1:0.9711\n",
      "2022-04-30 02:13:18,117 INFO: val Loss:0.184 | Acc:0.9756 | F1:0.9446\n",
      "100%|██████████| 247/247 [01:23<00:00,  2.94it/s]\n",
      "2022-04-30 02:14:42,106 INFO: Epoch:[083/100]\n",
      "2022-04-30 02:14:42,106 INFO: Train Loss:0.061 | Acc:0.9855 | F1:0.9777\n",
      "2022-04-30 02:14:52,809 INFO: val Loss:0.188 | Acc:0.9787 | F1:0.9462\n",
      "100%|██████████| 247/247 [01:22<00:00,  3.00it/s]\n",
      "2022-04-30 02:16:15,237 INFO: Epoch:[084/100]\n",
      "2022-04-30 02:16:15,238 INFO: Train Loss:0.062 | Acc:0.9911 | F1:0.9817\n",
      "2022-04-30 02:16:25,777 INFO: val Loss:0.248 | Acc:0.9695 | F1:0.9331\n",
      "2022-04-30 02:16:25,778 INFO: \n",
      "Best Val Epoch:77 | Val Loss:0.1297 | Val Acc:0.9756 | Val F1:0.9461\n",
      "2022-04-30 02:16:25,779 INFO: Total Process time:132.006Minute\n"
     ]
    }
   ],
   "source": [
    "args.step = 0\n",
    "models_path = []\n",
    "for s_fold in range(5): # 5fold\n",
    "    args.fold = s_fold\n",
    "    args.exp_num = str(s_fold)\n",
    "    save_path = main(args)\n",
    "    models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size:2154\n"
     ]
    }
   ],
   "source": [
    "img_size = 512\n",
    "\n",
    "test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "test_dataset = Test_dataset(df_test, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_path = ['./class_results/000', './class_results/001', './class_results/002', './class_results/003', './class_results/004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 20:27:39,498 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:11<00:00,  2.11s/it]\n",
      "2022-05-01 20:28:52,384 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:08<00:00,  2.03s/it]\n",
      "2022-05-01 20:30:02,320 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:08<00:00,  2.03s/it]\n",
      "2022-05-01 20:31:12,189 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:08<00:00,  2.02s/it]\n",
      "2022-05-01 20:32:21,907 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth)\n",
      "100%|██████████| 34/34 [01:09<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "ensemble = ensemble_5fold(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62,\n",
       " 28,\n",
       " 69,\n",
       " 64,\n",
       " 63,\n",
       " 50,\n",
       " 17,\n",
       " 32,\n",
       " 40,\n",
       " 39,\n",
       " 0,\n",
       " 76,\n",
       " 85,\n",
       " 60,\n",
       " 62,\n",
       " 2,\n",
       " 25,\n",
       " 22,\n",
       " 21,\n",
       " 47,\n",
       " 66,\n",
       " 83,\n",
       " 0,\n",
       " 58,\n",
       " 51,\n",
       " 55,\n",
       " 51,\n",
       " 60,\n",
       " 40,\n",
       " 78,\n",
       " 55,\n",
       " 5,\n",
       " 59,\n",
       " 83,\n",
       " 84,\n",
       " 82,\n",
       " 40,\n",
       " 9,\n",
       " 37,\n",
       " 44,\n",
       " 45,\n",
       " 10,\n",
       " 60,\n",
       " 14,\n",
       " 44,\n",
       " 63,\n",
       " 68,\n",
       " 17,\n",
       " 84,\n",
       " 59,\n",
       " 9,\n",
       " 72,\n",
       " 31,\n",
       " 35,\n",
       " 84,\n",
       " 46,\n",
       " 20,\n",
       " 23,\n",
       " 38,\n",
       " 15,\n",
       " 72,\n",
       " 51,\n",
       " 18,\n",
       " 80,\n",
       " 13,\n",
       " 36,\n",
       " 72,\n",
       " 13,\n",
       " 53,\n",
       " 43,\n",
       " 25,\n",
       " 76,\n",
       " 32,\n",
       " 61,\n",
       " 41,\n",
       " 84,\n",
       " 9,\n",
       " 40,\n",
       " 5,\n",
       " 9,\n",
       " 50,\n",
       " 76,\n",
       " 57,\n",
       " 77,\n",
       " 72,\n",
       " 1,\n",
       " 87,\n",
       " 22,\n",
       " 38,\n",
       " 83,\n",
       " 36,\n",
       " 84,\n",
       " 5,\n",
       " 42,\n",
       " 55,\n",
       " 65,\n",
       " 72,\n",
       " 72,\n",
       " 45,\n",
       " 45,\n",
       " 9,\n",
       " 21,\n",
       " 52,\n",
       " 17,\n",
       " 76,\n",
       " 1,\n",
       " 87,\n",
       " 49,\n",
       " 72,\n",
       " 34,\n",
       " 36,\n",
       " 58,\n",
       " 69,\n",
       " 72,\n",
       " 84,\n",
       " 15,\n",
       " 28,\n",
       " 52,\n",
       " 15,\n",
       " 41,\n",
       " 55,\n",
       " 19,\n",
       " 45,\n",
       " 72,\n",
       " 77,\n",
       " 72,\n",
       " 61,\n",
       " 72,\n",
       " 54,\n",
       " 55,\n",
       " 28,\n",
       " 46,\n",
       " 50,\n",
       " 42,\n",
       " 15,\n",
       " 25,\n",
       " 57,\n",
       " 2,\n",
       " 52,\n",
       " 78,\n",
       " 73,\n",
       " 33,\n",
       " 7,\n",
       " 9,\n",
       " 84,\n",
       " 35,\n",
       " 23,\n",
       " 84,\n",
       " 85,\n",
       " 76,\n",
       " 84,\n",
       " 79,\n",
       " 72,\n",
       " 52,\n",
       " 83,\n",
       " 63,\n",
       " 68,\n",
       " 63,\n",
       " 33,\n",
       " 45,\n",
       " 57,\n",
       " 60,\n",
       " 57,\n",
       " 11,\n",
       " 24,\n",
       " 38,\n",
       " 49,\n",
       " 74,\n",
       " 83,\n",
       " 10,\n",
       " 21,\n",
       " 24,\n",
       " 56,\n",
       " 39,\n",
       " 9,\n",
       " 50,\n",
       " 55,\n",
       " 86,\n",
       " 15,\n",
       " 84,\n",
       " 40,\n",
       " 27,\n",
       " 3,\n",
       " 11,\n",
       " 51,\n",
       " 20,\n",
       " 62,\n",
       " 45,\n",
       " 79,\n",
       " 70,\n",
       " 50,\n",
       " 18,\n",
       " 84,\n",
       " 32,\n",
       " 50,\n",
       " 57,\n",
       " 81,\n",
       " 85,\n",
       " 84,\n",
       " 33,\n",
       " 72,\n",
       " 12,\n",
       " 58,\n",
       " 1,\n",
       " 52,\n",
       " 40,\n",
       " 47,\n",
       " 52,\n",
       " 33,\n",
       " 25,\n",
       " 41,\n",
       " 15,\n",
       " 55,\n",
       " 76,\n",
       " 48,\n",
       " 20,\n",
       " 14,\n",
       " 49,\n",
       " 72,\n",
       " 1,\n",
       " 65,\n",
       " 28,\n",
       " 63,\n",
       " 1,\n",
       " 16,\n",
       " 43,\n",
       " 51,\n",
       " 36,\n",
       " 5,\n",
       " 72,\n",
       " 52,\n",
       " 55,\n",
       " 81,\n",
       " 49,\n",
       " 80,\n",
       " 14,\n",
       " 72,\n",
       " 45,\n",
       " 32,\n",
       " 14,\n",
       " 45,\n",
       " 34,\n",
       " 85,\n",
       " 59,\n",
       " 77,\n",
       " 41,\n",
       " 63,\n",
       " 64,\n",
       " 79,\n",
       " 9,\n",
       " 69,\n",
       " 64,\n",
       " 4,\n",
       " 15,\n",
       " 44,\n",
       " 45,\n",
       " 45,\n",
       " 16,\n",
       " 86,\n",
       " 33,\n",
       " 31,\n",
       " 59,\n",
       " 33,\n",
       " 32,\n",
       " 41,\n",
       " 40,\n",
       " 76,\n",
       " 55,\n",
       " 44,\n",
       " 9,\n",
       " 21,\n",
       " 55,\n",
       " 77,\n",
       " 72,\n",
       " 9,\n",
       " 15,\n",
       " 47,\n",
       " 15,\n",
       " 69,\n",
       " 87,\n",
       " 48,\n",
       " 36,\n",
       " 72,\n",
       " 53,\n",
       " 52,\n",
       " 55,\n",
       " 30,\n",
       " 63,\n",
       " 63,\n",
       " 0,\n",
       " 22,\n",
       " 1,\n",
       " 38,\n",
       " 37,\n",
       " 9,\n",
       " 87,\n",
       " 63,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 58,\n",
       " 41,\n",
       " 83,\n",
       " 72,\n",
       " 57,\n",
       " 51,\n",
       " 68,\n",
       " 28,\n",
       " 36,\n",
       " 56,\n",
       " 59,\n",
       " 72,\n",
       " 3,\n",
       " 46,\n",
       " 76,\n",
       " 73,\n",
       " 59,\n",
       " 76,\n",
       " 76,\n",
       " 63,\n",
       " 9,\n",
       " 58,\n",
       " 46,\n",
       " 7,\n",
       " 40,\n",
       " 15,\n",
       " 55,\n",
       " 56,\n",
       " 50,\n",
       " 72,\n",
       " 58,\n",
       " 18,\n",
       " 59,\n",
       " 24,\n",
       " 19,\n",
       " 3,\n",
       " 63,\n",
       " 43,\n",
       " 58,\n",
       " 4,\n",
       " 76,\n",
       " 55,\n",
       " 42,\n",
       " 63,\n",
       " 59,\n",
       " 39,\n",
       " 44,\n",
       " 42,\n",
       " 33,\n",
       " 53,\n",
       " 24,\n",
       " 5,\n",
       " 9,\n",
       " 17,\n",
       " 21,\n",
       " 16,\n",
       " 9,\n",
       " 52,\n",
       " 3,\n",
       " 67,\n",
       " 70,\n",
       " 13,\n",
       " 38,\n",
       " 19,\n",
       " 72,\n",
       " 80,\n",
       " 75,\n",
       " 7,\n",
       " 5,\n",
       " 37,\n",
       " 33,\n",
       " 55,\n",
       " 9,\n",
       " 3,\n",
       " 33,\n",
       " 69,\n",
       " 77,\n",
       " 56,\n",
       " 14,\n",
       " 28,\n",
       " 46,\n",
       " 55,\n",
       " 74,\n",
       " 62,\n",
       " 15,\n",
       " 28,\n",
       " 42,\n",
       " 15,\n",
       " 32,\n",
       " 55,\n",
       " 40,\n",
       " 43,\n",
       " 1,\n",
       " 37,\n",
       " 15,\n",
       " 69,\n",
       " 9,\n",
       " 2,\n",
       " 35,\n",
       " 63,\n",
       " 78,\n",
       " 64,\n",
       " 72,\n",
       " 82,\n",
       " 2,\n",
       " 63,\n",
       " 50,\n",
       " 17,\n",
       " 40,\n",
       " 51,\n",
       " 9,\n",
       " 16,\n",
       " 50,\n",
       " 9,\n",
       " 1,\n",
       " 72,\n",
       " 40,\n",
       " 72,\n",
       " 32,\n",
       " 69,\n",
       " 42,\n",
       " 84,\n",
       " 42,\n",
       " 24,\n",
       " 19,\n",
       " 50,\n",
       " 87,\n",
       " 87,\n",
       " 41,\n",
       " 71,\n",
       " 1,\n",
       " 84,\n",
       " 9,\n",
       " 1,\n",
       " 63,\n",
       " 31,\n",
       " 79,\n",
       " 51,\n",
       " 24,\n",
       " 25,\n",
       " 47,\n",
       " 65,\n",
       " 72,\n",
       " 56,\n",
       " 73,\n",
       " 70,\n",
       " 24,\n",
       " 86,\n",
       " 62,\n",
       " 38,\n",
       " 77,\n",
       " 84,\n",
       " 1,\n",
       " 24,\n",
       " 63,\n",
       " 33,\n",
       " 59,\n",
       " 33,\n",
       " 22,\n",
       " 44,\n",
       " 9,\n",
       " 21,\n",
       " 60,\n",
       " 41,\n",
       " 24,\n",
       " 14,\n",
       " 13,\n",
       " 44,\n",
       " 59,\n",
       " 42,\n",
       " 53,\n",
       " 42,\n",
       " 63,\n",
       " 3,\n",
       " 56,\n",
       " 9,\n",
       " 76,\n",
       " 72,\n",
       " 60,\n",
       " 60,\n",
       " 49,\n",
       " 59,\n",
       " 22,\n",
       " 5,\n",
       " 59,\n",
       " 59,\n",
       " 9,\n",
       " 28,\n",
       " 34,\n",
       " 51,\n",
       " 20,\n",
       " 6,\n",
       " 7,\n",
       " 55,\n",
       " 33,\n",
       " 18,\n",
       " 70,\n",
       " 68,\n",
       " 7,\n",
       " 86,\n",
       " 56,\n",
       " 65,\n",
       " 46,\n",
       " 72,\n",
       " 4,\n",
       " 63,\n",
       " 38,\n",
       " 78,\n",
       " 68,\n",
       " 41,\n",
       " 16,\n",
       " 56,\n",
       " 73,\n",
       " 72,\n",
       " 72,\n",
       " 84,\n",
       " 9,\n",
       " 42,\n",
       " 38,\n",
       " 5,\n",
       " 64,\n",
       " 53,\n",
       " 76,\n",
       " 45,\n",
       " 52,\n",
       " 82,\n",
       " 5,\n",
       " 76,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 34,\n",
       " 33,\n",
       " 34,\n",
       " 28,\n",
       " 55,\n",
       " 86,\n",
       " 79,\n",
       " 49,\n",
       " 67,\n",
       " 9,\n",
       " 76,\n",
       " 26,\n",
       " 59,\n",
       " 28,\n",
       " 87,\n",
       " 72,\n",
       " 63,\n",
       " 33,\n",
       " 54,\n",
       " 9,\n",
       " 13,\n",
       " 47,\n",
       " 52,\n",
       " 81,\n",
       " 62,\n",
       " 55,\n",
       " 18,\n",
       " 81,\n",
       " 56,\n",
       " 39,\n",
       " 15,\n",
       " 40,\n",
       " 23,\n",
       " 9,\n",
       " 33,\n",
       " 28,\n",
       " 28,\n",
       " 33,\n",
       " 59,\n",
       " 33,\n",
       " 79,\n",
       " 33,\n",
       " 51,\n",
       " 11,\n",
       " 77,\n",
       " 86,\n",
       " 23,\n",
       " 27,\n",
       " 82,\n",
       " 49,\n",
       " 42,\n",
       " 63,\n",
       " 20,\n",
       " 70,\n",
       " 72,\n",
       " 72,\n",
       " 24,\n",
       " 38,\n",
       " 72,\n",
       " 59,\n",
       " 32,\n",
       " 5,\n",
       " 86,\n",
       " 87,\n",
       " 42,\n",
       " 23,\n",
       " 63,\n",
       " 86,\n",
       " 33,\n",
       " 40,\n",
       " 33,\n",
       " 34,\n",
       " 7,\n",
       " 50,\n",
       " 9,\n",
       " 52,\n",
       " 9,\n",
       " 70,\n",
       " 51,\n",
       " 42,\n",
       " 25,\n",
       " 9,\n",
       " 23,\n",
       " 33,\n",
       " 16,\n",
       " 52,\n",
       " 86,\n",
       " 24,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 87,\n",
       " 62,\n",
       " 0,\n",
       " 22,\n",
       " 42,\n",
       " 68,\n",
       " 33,\n",
       " 1,\n",
       " 40,\n",
       " 67,\n",
       " 55,\n",
       " 58,\n",
       " 83,\n",
       " 40,\n",
       " 11,\n",
       " 15,\n",
       " 23,\n",
       " 41,\n",
       " 52,\n",
       " 38,\n",
       " 72,\n",
       " 0,\n",
       " 21,\n",
       " 83,\n",
       " 9,\n",
       " 37,\n",
       " 15,\n",
       " 86,\n",
       " 77,\n",
       " 35,\n",
       " 21,\n",
       " 59,\n",
       " 29,\n",
       " 79,\n",
       " 58,\n",
       " 13,\n",
       " 15,\n",
       " 40,\n",
       " 72,\n",
       " 8,\n",
       " 21,\n",
       " 72,\n",
       " 17,\n",
       " 28,\n",
       " 70,\n",
       " 34,\n",
       " 66,\n",
       " 5,\n",
       " 33,\n",
       " 50,\n",
       " 3,\n",
       " 54,\n",
       " 54,\n",
       " 0,\n",
       " 76,\n",
       " 51,\n",
       " 9,\n",
       " 63,\n",
       " 40,\n",
       " 38,\n",
       " 65,\n",
       " 52,\n",
       " 8,\n",
       " 63,\n",
       " 63,\n",
       " 33,\n",
       " 28,\n",
       " 40,\n",
       " 42,\n",
       " 10,\n",
       " 56,\n",
       " 47,\n",
       " 27,\n",
       " 5,\n",
       " 33,\n",
       " 85,\n",
       " 63,\n",
       " 2,\n",
       " 10,\n",
       " 41,\n",
       " 32,\n",
       " 59,\n",
       " 1,\n",
       " 19,\n",
       " 55,\n",
       " 71,\n",
       " 8,\n",
       " 76,\n",
       " 48,\n",
       " 52,\n",
       " 58,\n",
       " 30,\n",
       " 24,\n",
       " 79,\n",
       " 19,\n",
       " 41,\n",
       " 33,\n",
       " 87,\n",
       " 18,\n",
       " 72,\n",
       " 56,\n",
       " 73,\n",
       " 82,\n",
       " 72,\n",
       " 50,\n",
       " 33,\n",
       " 52,\n",
       " 4,\n",
       " 33,\n",
       " 3,\n",
       " 49,\n",
       " 50,\n",
       " 33,\n",
       " 42,\n",
       " 21,\n",
       " 52,\n",
       " 14,\n",
       " 79,\n",
       " 33,\n",
       " 24,\n",
       " 47,\n",
       " 50,\n",
       " 63,\n",
       " 24,\n",
       " 3,\n",
       " 50,\n",
       " 48,\n",
       " 13,\n",
       " 45,\n",
       " 9,\n",
       " 44,\n",
       " 72,\n",
       " 14,\n",
       " 65,\n",
       " 87,\n",
       " 3,\n",
       " 52,\n",
       " 15,\n",
       " 55,\n",
       " 34,\n",
       " 10,\n",
       " 51,\n",
       " 3,\n",
       " 15,\n",
       " 25,\n",
       " 82,\n",
       " 45,\n",
       " 47,\n",
       " 74,\n",
       " 23,\n",
       " 75,\n",
       " 83,\n",
       " 34,\n",
       " 59,\n",
       " 48,\n",
       " 55,\n",
       " 0,\n",
       " 13,\n",
       " 72,\n",
       " 36,\n",
       " 55,\n",
       " 75,\n",
       " 9,\n",
       " 40,\n",
       " 68,\n",
       " 68,\n",
       " 57,\n",
       " 8,\n",
       " 85,\n",
       " 21,\n",
       " 67,\n",
       " 68,\n",
       " 84,\n",
       " 31,\n",
       " 72,\n",
       " 85,\n",
       " 55,\n",
       " 50,\n",
       " 87,\n",
       " 31,\n",
       " 66,\n",
       " 43,\n",
       " 64,\n",
       " 76,\n",
       " 51,\n",
       " 0,\n",
       " 39,\n",
       " 86,\n",
       " 9,\n",
       " 15,\n",
       " 17,\n",
       " 25,\n",
       " 68,\n",
       " 76,\n",
       " 55,\n",
       " 0,\n",
       " 61,\n",
       " 55,\n",
       " 51,\n",
       " 35,\n",
       " 9,\n",
       " 45,\n",
       " 59,\n",
       " 9,\n",
       " 15,\n",
       " 9,\n",
       " 31,\n",
       " 72,\n",
       " 23,\n",
       " 25,\n",
       " 9,\n",
       " 14,\n",
       " 84,\n",
       " 63,\n",
       " 16,\n",
       " 57,\n",
       " 24,\n",
       " 52,\n",
       " 34,\n",
       " 0,\n",
       " 55,\n",
       " 56,\n",
       " 61,\n",
       " 70,\n",
       " 36,\n",
       " 2,\n",
       " 65,\n",
       " 15,\n",
       " 65,\n",
       " 78,\n",
       " 55,\n",
       " 14,\n",
       " 72,\n",
       " 13,\n",
       " 87,\n",
       " 32,\n",
       " 72,\n",
       " 80,\n",
       " 5,\n",
       " 33,\n",
       " 45,\n",
       " 55,\n",
       " 20,\n",
       " 72,\n",
       " 67,\n",
       " 8,\n",
       " 84,\n",
       " 9,\n",
       " 21,\n",
       " 41,\n",
       " 52,\n",
       " 51,\n",
       " 47,\n",
       " 21,\n",
       " 60,\n",
       " 77,\n",
       " 40,\n",
       " 40,\n",
       " 33,\n",
       " 25,\n",
       " 18,\n",
       " 16,\n",
       " 72,\n",
       " 63,\n",
       " 57,\n",
       " 47,\n",
       " 40,\n",
       " 44,\n",
       " 72,\n",
       " 84,\n",
       " 40,\n",
       " 12,\n",
       " 37,\n",
       " 59,\n",
       " 51,\n",
       " 55,\n",
       " 12,\n",
       " 24,\n",
       " 71,\n",
       " 38,\n",
       " 55,\n",
       " 52,\n",
       " 14,\n",
       " 80,\n",
       " 38,\n",
       " 13,\n",
       " 58,\n",
       " 45,\n",
       " 62,\n",
       " 51,\n",
       " 81,\n",
       " 18,\n",
       " 39,\n",
       " 60,\n",
       " 44,\n",
       " 60,\n",
       " 65,\n",
       " 70,\n",
       " 33,\n",
       " 55,\n",
       " 66,\n",
       " 84,\n",
       " 44,\n",
       " 39,\n",
       " 4,\n",
       " 68,\n",
       " 9,\n",
       " 68,\n",
       " 63,\n",
       " 5,\n",
       " 18,\n",
       " 42,\n",
       " 15,\n",
       " 52,\n",
       " 65,\n",
       " 37,\n",
       " 19,\n",
       " 40,\n",
       " 24,\n",
       " 38,\n",
       " 58,\n",
       " 56,\n",
       " 2,\n",
       " 38,\n",
       " 39,\n",
       " 14,\n",
       " 87,\n",
       " 34,\n",
       " 17,\n",
       " 46,\n",
       " 63,\n",
       " 30,\n",
       " 56,\n",
       " 24,\n",
       " 16,\n",
       " 72,\n",
       " 17,\n",
       " 77,\n",
       " 84,\n",
       " 57,\n",
       " 50,\n",
       " 34,\n",
       " 33,\n",
       " 6,\n",
       " 58,\n",
       " 35,\n",
       " 5,\n",
       " 33,\n",
       " 21,\n",
       " 19,\n",
       " 1,\n",
       " 9,\n",
       " 33,\n",
       " 61,\n",
       " 53,\n",
       " 40,\n",
       " 57,\n",
       " 55,\n",
       " 40,\n",
       " 23,\n",
       " 59,\n",
       " 72,\n",
       " 49,\n",
       " 18,\n",
       " 28,\n",
       " 45,\n",
       " 55,\n",
       " 55,\n",
       " 47,\n",
       " 58,\n",
       " 15,\n",
       " 11,\n",
       " 9,\n",
       " 3,\n",
       " 82,\n",
       " 3,\n",
       " 38,\n",
       " 55,\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_pred = ensemble.argmax(axis=1).tolist()\n",
    "f_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(\"./open/train_df_add2.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "\n",
    "f_result = [label_decoder[result] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tile-glue_strip',\n",
       " 'grid-good',\n",
       " 'transistor-bent_lead',\n",
       " 'tile-gray_stroke',\n",
       " 'tile-good',\n",
       " 'pill-crack',\n",
       " 'capsule-scratch',\n",
       " 'hazelnut-cut',\n",
       " 'leather-good',\n",
       " 'leather-glue',\n",
       " 'bottle-broken_large',\n",
       " 'wood-good',\n",
       " 'zipper-rough',\n",
       " 'screw-thread_top',\n",
       " 'tile-glue_strip',\n",
       " 'bottle-contamination',\n",
       " 'grid-bent',\n",
       " 'carpet-hole',\n",
       " 'carpet-good',\n",
       " 'pill-color',\n",
       " 'tile-rough',\n",
       " 'zipper-fabric_interior',\n",
       " 'bottle-broken_large',\n",
       " 'screw-scratch_neck',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-thread_top',\n",
       " 'leather-good',\n",
       " 'wood-liquid',\n",
       " 'screw-good',\n",
       " 'cable-cable_swap',\n",
       " 'screw-thread_side',\n",
       " 'zipper-fabric_interior',\n",
       " 'zipper-good',\n",
       " 'zipper-fabric_border',\n",
       " 'leather-good',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-good',\n",
       " 'cable-missing_cable',\n",
       " 'screw-thread_top',\n",
       " 'capsule-faulty_imprint',\n",
       " 'metal_nut-flip',\n",
       " 'tile-good',\n",
       " 'toothbrush-good',\n",
       " 'capsule-scratch',\n",
       " 'zipper-good',\n",
       " 'screw-thread_side',\n",
       " 'cable-good',\n",
       " 'transistor-good',\n",
       " 'hazelnut-crack',\n",
       " 'hazelnut-print',\n",
       " 'zipper-good',\n",
       " 'metal_nut-scratch',\n",
       " 'carpet-cut',\n",
       " 'carpet-metal_contamination',\n",
       " 'leather-fold',\n",
       " 'capsule-good',\n",
       " 'transistor-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'capsule-squeeze',\n",
       " 'zipper-broken_teeth',\n",
       " 'capsule-crack',\n",
       " 'leather-color',\n",
       " 'transistor-good',\n",
       " 'capsule-crack',\n",
       " 'pill-pill_type',\n",
       " 'metal_nut-color',\n",
       " 'grid-bent',\n",
       " 'wood-good',\n",
       " 'hazelnut-cut',\n",
       " 'tile-crack',\n",
       " 'leather-poke',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'leather-good',\n",
       " 'cable-cable_swap',\n",
       " 'cable-good',\n",
       " 'pill-crack',\n",
       " 'wood-good',\n",
       " 'screw-scratch_head',\n",
       " 'wood-hole',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'carpet-hole',\n",
       " 'leather-fold',\n",
       " 'zipper-fabric_interior',\n",
       " 'leather-color',\n",
       " 'zipper-good',\n",
       " 'cable-cable_swap',\n",
       " 'metal_nut-bent',\n",
       " 'screw-good',\n",
       " 'tile-oil',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'metal_nut-good',\n",
       " 'metal_nut-good',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'pill-good',\n",
       " 'capsule-scratch',\n",
       " 'wood-good',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'pill-contamination',\n",
       " 'transistor-good',\n",
       " 'hazelnut-hole',\n",
       " 'leather-color',\n",
       " 'screw-scratch_neck',\n",
       " 'transistor-bent_lead',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'pill-good',\n",
       " 'capsule-good',\n",
       " 'leather-poke',\n",
       " 'screw-good',\n",
       " 'carpet-color',\n",
       " 'metal_nut-good',\n",
       " 'transistor-good',\n",
       " 'wood-hole',\n",
       " 'transistor-good',\n",
       " 'tile-crack',\n",
       " 'transistor-good',\n",
       " 'pill-scratch',\n",
       " 'screw-good',\n",
       " 'grid-good',\n",
       " 'metal_nut-scratch',\n",
       " 'pill-crack',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'grid-bent',\n",
       " 'screw-scratch_head',\n",
       " 'bottle-contamination',\n",
       " 'pill-good',\n",
       " 'wood-liquid',\n",
       " 'transistor-misplaced',\n",
       " 'hazelnut-good',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'cable-good',\n",
       " 'zipper-good',\n",
       " 'hazelnut-print',\n",
       " 'carpet-metal_contamination',\n",
       " 'zipper-good',\n",
       " 'zipper-rough',\n",
       " 'wood-good',\n",
       " 'zipper-good',\n",
       " 'wood-scratch',\n",
       " 'transistor-good',\n",
       " 'pill-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'tile-good',\n",
       " 'toothbrush-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-scratch_head',\n",
       " 'screw-thread_top',\n",
       " 'screw-scratch_head',\n",
       " 'cable-missing_wire',\n",
       " 'carpet-thread',\n",
       " 'leather-fold',\n",
       " 'pill-contamination',\n",
       " 'wood-color',\n",
       " 'zipper-fabric_interior',\n",
       " 'cable-missing_cable',\n",
       " 'carpet-good',\n",
       " 'carpet-thread',\n",
       " 'screw-manipulated_front',\n",
       " 'leather-glue',\n",
       " 'cable-good',\n",
       " 'pill-crack',\n",
       " 'screw-good',\n",
       " 'zipper-split_teeth',\n",
       " 'capsule-good',\n",
       " 'zipper-good',\n",
       " 'leather-good',\n",
       " 'grid-glue',\n",
       " 'bottle-good',\n",
       " 'cable-missing_wire',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-cut',\n",
       " 'tile-glue_strip',\n",
       " 'metal_nut-good',\n",
       " 'wood-scratch',\n",
       " 'transistor-cut_lead',\n",
       " 'pill-crack',\n",
       " 'capsule-squeeze',\n",
       " 'zipper-good',\n",
       " 'hazelnut-cut',\n",
       " 'pill-crack',\n",
       " 'screw-scratch_head',\n",
       " 'zipper-combined',\n",
       " 'zipper-rough',\n",
       " 'zipper-good',\n",
       " 'hazelnut-good',\n",
       " 'transistor-good',\n",
       " 'cable-poke_insulation',\n",
       " 'screw-scratch_neck',\n",
       " 'bottle-broken_small',\n",
       " 'pill-good',\n",
       " 'leather-good',\n",
       " 'pill-color',\n",
       " 'pill-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-bent',\n",
       " 'leather-poke',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'wood-good',\n",
       " 'pill-combined',\n",
       " 'carpet-cut',\n",
       " 'capsule-faulty_imprint',\n",
       " 'pill-contamination',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_small',\n",
       " 'tile-oil',\n",
       " 'grid-good',\n",
       " 'tile-good',\n",
       " 'bottle-broken_small',\n",
       " 'capsule-poke',\n",
       " 'metal_nut-color',\n",
       " 'pill-faulty_imprint',\n",
       " 'leather-color',\n",
       " 'cable-cable_swap',\n",
       " 'transistor-good',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'zipper-combined',\n",
       " 'pill-contamination',\n",
       " 'zipper-broken_teeth',\n",
       " 'capsule-faulty_imprint',\n",
       " 'transistor-good',\n",
       " 'metal_nut-good',\n",
       " 'hazelnut-cut',\n",
       " 'capsule-faulty_imprint',\n",
       " 'metal_nut-good',\n",
       " 'hazelnut-hole',\n",
       " 'zipper-rough',\n",
       " 'screw-thread_side',\n",
       " 'wood-hole',\n",
       " 'leather-poke',\n",
       " 'tile-good',\n",
       " 'tile-gray_stroke',\n",
       " 'wood-scratch',\n",
       " 'cable-good',\n",
       " 'transistor-bent_lead',\n",
       " 'tile-gray_stroke',\n",
       " 'cable-bent_wire',\n",
       " 'capsule-good',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-good',\n",
       " 'metal_nut-good',\n",
       " 'capsule-poke',\n",
       " 'zipper-split_teeth',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-crack',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-cut',\n",
       " 'leather-poke',\n",
       " 'leather-good',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'metal_nut-flip',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'screw-good',\n",
       " 'wood-hole',\n",
       " 'transistor-good',\n",
       " 'cable-good',\n",
       " 'capsule-good',\n",
       " 'pill-color',\n",
       " 'capsule-good',\n",
       " 'transistor-bent_lead',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'pill-combined',\n",
       " 'leather-color',\n",
       " 'transistor-good',\n",
       " 'pill-pill_type',\n",
       " 'pill-good',\n",
       " 'screw-good',\n",
       " 'grid-thread',\n",
       " 'tile-good',\n",
       " 'tile-good',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-hole',\n",
       " 'bottle-broken_small',\n",
       " 'leather-fold',\n",
       " 'leather-cut',\n",
       " 'cable-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'tile-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_neck',\n",
       " 'leather-poke',\n",
       " 'zipper-fabric_interior',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-faulty_imprint',\n",
       " 'toothbrush-good',\n",
       " 'grid-good',\n",
       " 'leather-color',\n",
       " 'screw-manipulated_front',\n",
       " 'screw-thread_side',\n",
       " 'transistor-good',\n",
       " 'bottle-good',\n",
       " 'metal_nut-scratch',\n",
       " 'wood-good',\n",
       " 'transistor-misplaced',\n",
       " 'screw-thread_side',\n",
       " 'wood-good',\n",
       " 'wood-good',\n",
       " 'tile-good',\n",
       " 'cable-good',\n",
       " 'screw-scratch_neck',\n",
       " 'metal_nut-scratch',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'leather-good',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'screw-manipulated_front',\n",
       " 'pill-crack',\n",
       " 'transistor-good',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-squeeze',\n",
       " 'screw-thread_side',\n",
       " 'carpet-thread',\n",
       " 'carpet-color',\n",
       " 'bottle-good',\n",
       " 'tile-good',\n",
       " 'metal_nut-color',\n",
       " 'screw-scratch_neck',\n",
       " 'cable-bent_wire',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'screw-thread_side',\n",
       " 'leather-glue',\n",
       " 'metal_nut-flip',\n",
       " 'metal_nut-bent',\n",
       " 'hazelnut-good',\n",
       " 'pill-pill_type',\n",
       " 'carpet-thread',\n",
       " 'cable-cable_swap',\n",
       " 'cable-good',\n",
       " 'capsule-scratch',\n",
       " 'carpet-good',\n",
       " 'capsule-poke',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'bottle-good',\n",
       " 'toothbrush-defective',\n",
       " 'transistor-cut_lead',\n",
       " 'capsule-crack',\n",
       " 'leather-fold',\n",
       " 'carpet-color',\n",
       " 'transistor-good',\n",
       " 'zipper-broken_teeth',\n",
       " 'wood-combined',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'cable-cable_swap',\n",
       " 'leather-cut',\n",
       " 'hazelnut-good',\n",
       " 'screw-good',\n",
       " 'cable-good',\n",
       " 'bottle-good',\n",
       " 'hazelnut-good',\n",
       " 'transistor-bent_lead',\n",
       " 'wood-hole',\n",
       " 'screw-manipulated_front',\n",
       " 'capsule-faulty_imprint',\n",
       " 'grid-good',\n",
       " 'metal_nut-scratch',\n",
       " 'screw-good',\n",
       " 'wood-color',\n",
       " 'tile-glue_strip',\n",
       " 'capsule-good',\n",
       " 'grid-good',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'hazelnut-cut',\n",
       " 'screw-good',\n",
       " 'leather-good',\n",
       " 'metal_nut-color',\n",
       " 'bottle-broken_small',\n",
       " 'leather-cut',\n",
       " 'capsule-good',\n",
       " 'transistor-bent_lead',\n",
       " 'cable-good',\n",
       " 'bottle-contamination',\n",
       " 'hazelnut-print',\n",
       " 'tile-good',\n",
       " 'wood-liquid',\n",
       " 'tile-gray_stroke',\n",
       " 'transistor-good',\n",
       " 'zipper-fabric_border',\n",
       " 'bottle-contamination',\n",
       " 'tile-good',\n",
       " 'pill-crack',\n",
       " 'capsule-scratch',\n",
       " 'leather-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'cable-good',\n",
       " 'capsule-poke',\n",
       " 'pill-crack',\n",
       " 'cable-good',\n",
       " 'bottle-broken_small',\n",
       " 'transistor-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'hazelnut-cut',\n",
       " 'transistor-bent_lead',\n",
       " 'metal_nut-bent',\n",
       " 'zipper-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-thread',\n",
       " 'carpet-color',\n",
       " 'pill-crack',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'leather-poke',\n",
       " 'transistor-damaged_case',\n",
       " 'bottle-broken_small',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'bottle-broken_small',\n",
       " 'tile-good',\n",
       " 'hazelnut-crack',\n",
       " 'wood-scratch',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-thread',\n",
       " 'grid-bent',\n",
       " 'pill-color',\n",
       " 'tile-oil',\n",
       " 'transistor-good',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-misplaced',\n",
       " 'transistor-cut_lead',\n",
       " 'carpet-thread',\n",
       " 'zipper-split_teeth',\n",
       " 'tile-glue_strip',\n",
       " 'leather-fold',\n",
       " 'wood-hole',\n",
       " 'zipper-good',\n",
       " 'bottle-broken_small',\n",
       " 'carpet-thread',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'carpet-hole',\n",
       " 'metal_nut-flip',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'screw-thread_top',\n",
       " 'leather-poke',\n",
       " 'carpet-thread',\n",
       " 'capsule-faulty_imprint',\n",
       " 'capsule-crack',\n",
       " 'metal_nut-flip',\n",
       " 'screw-thread_side',\n",
       " 'metal_nut-bent',\n",
       " 'pill-pill_type',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'bottle-good',\n",
       " 'screw-manipulated_front',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'transistor-good',\n",
       " 'screw-thread_top',\n",
       " 'screw-thread_top',\n",
       " 'pill-contamination',\n",
       " 'screw-thread_side',\n",
       " 'carpet-hole',\n",
       " 'cable-cable_swap',\n",
       " 'screw-thread_side',\n",
       " 'screw-thread_side',\n",
       " 'cable-good',\n",
       " 'grid-good',\n",
       " 'hazelnut-hole',\n",
       " 'pill-faulty_imprint',\n",
       " 'carpet-cut',\n",
       " 'cable-combined',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'screw-good',\n",
       " 'hazelnut-good',\n",
       " 'capsule-squeeze',\n",
       " 'transistor-cut_lead',\n",
       " 'toothbrush-good',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'zipper-split_teeth',\n",
       " 'screw-manipulated_front',\n",
       " 'tile-oil',\n",
       " 'metal_nut-scratch',\n",
       " 'transistor-good',\n",
       " 'cable-bent_wire',\n",
       " 'tile-good',\n",
       " 'leather-fold',\n",
       " 'wood-liquid',\n",
       " 'toothbrush-good',\n",
       " 'leather-poke',\n",
       " 'capsule-poke',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-misplaced',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-bent',\n",
       " 'leather-fold',\n",
       " 'cable-cable_swap',\n",
       " 'tile-gray_stroke',\n",
       " 'pill-pill_type',\n",
       " 'wood-good',\n",
       " 'metal_nut-good',\n",
       " 'pill-good',\n",
       " 'zipper-fabric_border',\n",
       " 'cable-cable_swap',\n",
       " 'wood-good',\n",
       " 'capsule-good',\n",
       " 'carpet-cut',\n",
       " 'capsule-good',\n",
       " 'hazelnut-hole',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-hole',\n",
       " 'grid-good',\n",
       " 'screw-good',\n",
       " 'zipper-split_teeth',\n",
       " 'wood-scratch',\n",
       " 'pill-contamination',\n",
       " 'toothbrush-defective',\n",
       " 'cable-good',\n",
       " 'wood-good',\n",
       " 'grid-broken',\n",
       " 'screw-thread_side',\n",
       " 'grid-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'transistor-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'pill-scratch',\n",
       " 'cable-good',\n",
       " 'capsule-crack',\n",
       " 'pill-color',\n",
       " 'pill-good',\n",
       " 'zipper-combined',\n",
       " 'tile-glue_strip',\n",
       " 'screw-good',\n",
       " 'capsule-squeeze',\n",
       " 'zipper-combined',\n",
       " 'screw-manipulated_front',\n",
       " 'leather-glue',\n",
       " 'capsule-good',\n",
       " 'leather-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-good',\n",
       " 'grid-good',\n",
       " 'hazelnut-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-good',\n",
       " 'wood-scratch',\n",
       " 'hazelnut-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'cable-missing_wire',\n",
       " 'wood-hole',\n",
       " 'zipper-split_teeth',\n",
       " 'carpet-metal_contamination',\n",
       " 'grid-glue',\n",
       " 'zipper-fabric_border',\n",
       " 'pill-contamination',\n",
       " 'metal_nut-bent',\n",
       " 'tile-good',\n",
       " 'carpet-cut',\n",
       " 'transistor-cut_lead',\n",
       " 'transistor-good',\n",
       " 'transistor-good',\n",
       " 'carpet-thread',\n",
       " 'leather-fold',\n",
       " 'transistor-good',\n",
       " 'screw-thread_side',\n",
       " 'hazelnut-cut',\n",
       " 'cable-cable_swap',\n",
       " 'zipper-split_teeth',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-metal_contamination',\n",
       " 'tile-good',\n",
       " 'zipper-split_teeth',\n",
       " 'hazelnut-good',\n",
       " 'leather-good',\n",
       " 'hazelnut-good',\n",
       " 'hazelnut-hole',\n",
       " 'cable-cut_inner_insulation',\n",
       " 'pill-crack',\n",
       " 'cable-good',\n",
       " 'pill-good',\n",
       " 'cable-good',\n",
       " 'transistor-cut_lead',\n",
       " 'pill-faulty_imprint',\n",
       " 'metal_nut-bent',\n",
       " 'grid-bent',\n",
       " 'cable-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'hazelnut-good',\n",
       " 'capsule-poke',\n",
       " 'pill-good',\n",
       " 'zipper-split_teeth',\n",
       " 'carpet-thread',\n",
       " 'cable-bent_wire',\n",
       " 'cable-cable_swap',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'tile-glue_strip',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-hole',\n",
       " 'metal_nut-bent',\n",
       " 'toothbrush-good',\n",
       " 'hazelnut-good',\n",
       " 'bottle-broken_small',\n",
       " 'leather-good',\n",
       " 'toothbrush-defective',\n",
       " 'screw-good',\n",
       " 'screw-scratch_neck',\n",
       " 'zipper-fabric_interior',\n",
       " 'leather-good',\n",
       " 'cable-missing_wire',\n",
       " 'capsule-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'leather-poke',\n",
       " 'pill-good',\n",
       " 'leather-fold',\n",
       " 'transistor-good',\n",
       " 'bottle-broken_large',\n",
       " 'carpet-good',\n",
       " 'zipper-fabric_interior',\n",
       " 'cable-good',\n",
       " 'leather-cut',\n",
       " 'capsule-good',\n",
       " 'zipper-split_teeth',\n",
       " 'wood-hole',\n",
       " 'hazelnut-print',\n",
       " 'carpet-good',\n",
       " 'screw-thread_side',\n",
       " 'grid-metal_contamination',\n",
       " 'wood-scratch',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-crack',\n",
       " 'capsule-good',\n",
       " 'leather-good',\n",
       " 'transistor-good',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'carpet-good',\n",
       " 'transistor-good',\n",
       " 'capsule-scratch',\n",
       " 'grid-good',\n",
       " 'transistor-cut_lead',\n",
       " 'hazelnut-hole',\n",
       " 'tile-rough',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'pill-crack',\n",
       " 'bottle-good',\n",
       " 'pill-scratch',\n",
       " 'pill-scratch',\n",
       " 'bottle-broken_large',\n",
       " 'wood-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'cable-good',\n",
       " 'tile-good',\n",
       " 'leather-good',\n",
       " 'leather-fold',\n",
       " 'tile-oil',\n",
       " 'pill-good',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'tile-good',\n",
       " 'tile-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-good',\n",
       " 'leather-good',\n",
       " 'metal_nut-bent',\n",
       " 'cable-missing_cable',\n",
       " 'screw-manipulated_front',\n",
       " 'pill-color',\n",
       " 'grid-glue',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'zipper-rough',\n",
       " 'tile-good',\n",
       " 'bottle-contamination',\n",
       " 'cable-missing_cable',\n",
       " 'leather-poke',\n",
       " 'hazelnut-cut',\n",
       " 'screw-thread_side',\n",
       " 'bottle-broken_small',\n",
       " 'carpet-color',\n",
       " 'screw-good',\n",
       " 'transistor-damaged_case',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'wood-good',\n",
       " 'pill-combined',\n",
       " 'pill-good',\n",
       " 'screw-scratch_neck',\n",
       " 'grid-thread',\n",
       " 'carpet-thread',\n",
       " 'wood-scratch',\n",
       " 'carpet-color',\n",
       " 'leather-poke',\n",
       " 'hazelnut-good',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'capsule-squeeze',\n",
       " 'transistor-good',\n",
       " 'screw-manipulated_front',\n",
       " 'transistor-misplaced',\n",
       " 'zipper-fabric_border',\n",
       " 'transistor-good',\n",
       " 'pill-crack',\n",
       " 'hazelnut-good',\n",
       " 'pill-good',\n",
       " 'cable-bent_wire',\n",
       " 'hazelnut-good',\n",
       " 'bottle-good',\n",
       " 'pill-contamination',\n",
       " 'pill-crack',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-bent',\n",
       " 'carpet-good',\n",
       " 'pill-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'wood-scratch',\n",
       " 'hazelnut-good',\n",
       " 'carpet-thread',\n",
       " 'pill-color',\n",
       " 'pill-crack',\n",
       " 'tile-good',\n",
       " 'carpet-thread',\n",
       " 'bottle-good',\n",
       " 'pill-crack',\n",
       " 'pill-combined',\n",
       " 'capsule-crack',\n",
       " 'metal_nut-good',\n",
       " 'cable-good',\n",
       " 'metal_nut-flip',\n",
       " 'transistor-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'tile-oil',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'bottle-good',\n",
       " 'pill-good',\n",
       " 'capsule-good',\n",
       " 'screw-good',\n",
       " 'hazelnut-hole',\n",
       " 'cable-missing_cable',\n",
       " 'pill-faulty_imprint',\n",
       " 'bottle-good',\n",
       " 'capsule-good',\n",
       " 'grid-bent',\n",
       " 'zipper-fabric_border',\n",
       " 'metal_nut-good',\n",
       " 'pill-color',\n",
       " 'wood-color',\n",
       " 'carpet-metal_contamination',\n",
       " 'wood-combined',\n",
       " 'zipper-fabric_interior',\n",
       " 'hazelnut-hole',\n",
       " 'screw-thread_side',\n",
       " 'pill-combined',\n",
       " 'screw-good',\n",
       " 'bottle-broken_large',\n",
       " 'capsule-crack',\n",
       " 'transistor-good',\n",
       " 'leather-color',\n",
       " 'screw-good',\n",
       " 'wood-combined',\n",
       " 'cable-good',\n",
       " 'leather-good',\n",
       " 'toothbrush-good',\n",
       " 'toothbrush-good',\n",
       " 'screw-scratch_head',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'zipper-rough',\n",
       " 'carpet-good',\n",
       " 'toothbrush-defective',\n",
       " 'toothbrush-good',\n",
       " 'zipper-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'zipper-rough',\n",
       " 'screw-good',\n",
       " 'pill-crack',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-crack',\n",
       " 'tile-rough',\n",
       " 'metal_nut-color',\n",
       " 'tile-gray_stroke',\n",
       " 'wood-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'bottle-broken_large',\n",
       " 'leather-glue',\n",
       " 'zipper-split_teeth',\n",
       " 'cable-good',\n",
       " 'capsule-good',\n",
       " 'capsule-scratch',\n",
       " 'grid-bent',\n",
       " 'toothbrush-good',\n",
       " 'wood-good',\n",
       " 'screw-good',\n",
       " 'bottle-broken_large',\n",
       " 'tile-crack',\n",
       " 'screw-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'hazelnut-print',\n",
       " 'cable-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-thread_side',\n",
       " 'cable-good',\n",
       " 'capsule-good',\n",
       " 'cable-good',\n",
       " 'hazelnut-crack',\n",
       " 'transistor-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'grid-bent',\n",
       " 'cable-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-good',\n",
       " 'tile-good',\n",
       " 'capsule-poke',\n",
       " 'screw-scratch_head',\n",
       " 'carpet-thread',\n",
       " 'pill-good',\n",
       " 'hazelnut-hole',\n",
       " 'bottle-broken_large',\n",
       " 'screw-good',\n",
       " 'screw-manipulated_front',\n",
       " 'tile-crack',\n",
       " 'transistor-cut_lead',\n",
       " 'leather-color',\n",
       " 'bottle-contamination',\n",
       " 'tile-oil',\n",
       " 'capsule-good',\n",
       " 'tile-oil',\n",
       " 'wood-liquid',\n",
       " 'screw-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'transistor-good',\n",
       " 'capsule-crack',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-cut',\n",
       " 'transistor-good',\n",
       " 'zipper-broken_teeth',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'carpet-cut',\n",
       " 'transistor-good',\n",
       " 'toothbrush-defective',\n",
       " 'cable-cut_outer_insulation',\n",
       " 'zipper-good',\n",
       " 'cable-good',\n",
       " 'carpet-good',\n",
       " 'leather-poke',\n",
       " 'pill-good',\n",
       " 'pill-faulty_imprint',\n",
       " 'pill-color',\n",
       " 'carpet-good',\n",
       " 'screw-thread_top',\n",
       " 'wood-hole',\n",
       " 'leather-good',\n",
       " 'leather-good',\n",
       " 'hazelnut-good',\n",
       " 'grid-bent',\n",
       " 'capsule-squeeze',\n",
       " 'capsule-poke',\n",
       " 'transistor-good',\n",
       " 'tile-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-color',\n",
       " 'leather-good',\n",
       " 'metal_nut-flip',\n",
       " 'transistor-good',\n",
       " 'zipper-good',\n",
       " 'leather-good',\n",
       " 'cable-poke_insulation',\n",
       " 'leather-cut',\n",
       " 'screw-thread_side',\n",
       " 'pill-faulty_imprint',\n",
       " 'screw-good',\n",
       " 'cable-poke_insulation',\n",
       " 'carpet-thread',\n",
       " 'transistor-damaged_case',\n",
       " 'leather-fold',\n",
       " 'screw-good',\n",
       " 'pill-good',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-broken_teeth',\n",
       " 'leather-fold',\n",
       " 'capsule-crack',\n",
       " 'screw-scratch_neck',\n",
       " 'metal_nut-good',\n",
       " 'tile-glue_strip',\n",
       " 'pill-faulty_imprint',\n",
       " 'zipper-combined',\n",
       " 'capsule-squeeze',\n",
       " 'leather-glue',\n",
       " 'screw-thread_top',\n",
       " 'metal_nut-flip',\n",
       " 'screw-thread_top',\n",
       " 'tile-oil',\n",
       " 'transistor-cut_lead',\n",
       " 'hazelnut-good',\n",
       " 'screw-good',\n",
       " 'tile-rough',\n",
       " 'zipper-good',\n",
       " 'metal_nut-flip',\n",
       " 'leather-glue',\n",
       " 'cable-bent_wire',\n",
       " 'toothbrush-good',\n",
       " 'cable-good',\n",
       " 'toothbrush-good',\n",
       " 'tile-good',\n",
       " 'cable-cable_swap',\n",
       " 'capsule-squeeze',\n",
       " 'metal_nut-bent',\n",
       " 'capsule-good',\n",
       " 'pill-good',\n",
       " 'tile-oil',\n",
       " 'leather-cut',\n",
       " 'carpet-color',\n",
       " 'leather-good',\n",
       " 'carpet-thread',\n",
       " 'leather-fold',\n",
       " 'screw-scratch_neck',\n",
       " 'screw-manipulated_front',\n",
       " 'bottle-contamination',\n",
       " 'leather-fold',\n",
       " 'leather-glue',\n",
       " 'capsule-faulty_imprint',\n",
       " 'zipper-squeezed_teeth',\n",
       " 'hazelnut-hole',\n",
       " 'capsule-scratch',\n",
       " 'metal_nut-scratch',\n",
       " 'tile-good',\n",
       " 'grid-thread',\n",
       " 'screw-manipulated_front',\n",
       " 'carpet-thread',\n",
       " 'capsule-poke',\n",
       " 'transistor-good',\n",
       " 'capsule-scratch',\n",
       " 'wood-hole',\n",
       " 'zipper-good',\n",
       " 'screw-scratch_head',\n",
       " 'pill-crack',\n",
       " 'hazelnut-hole',\n",
       " 'hazelnut-good',\n",
       " 'cable-combined',\n",
       " 'screw-scratch_neck',\n",
       " 'hazelnut-print',\n",
       " 'cable-cable_swap',\n",
       " 'hazelnut-good',\n",
       " 'carpet-good',\n",
       " 'carpet-color',\n",
       " 'bottle-broken_small',\n",
       " 'cable-good',\n",
       " 'hazelnut-good',\n",
       " 'tile-crack',\n",
       " 'pill-pill_type',\n",
       " 'leather-good',\n",
       " 'screw-scratch_head',\n",
       " 'screw-good',\n",
       " 'leather-good',\n",
       " 'carpet-metal_contamination',\n",
       " 'screw-thread_side',\n",
       " 'transistor-good',\n",
       " 'pill-contamination',\n",
       " 'capsule-squeeze',\n",
       " 'grid-good',\n",
       " 'metal_nut-good',\n",
       " 'screw-good',\n",
       " 'screw-good',\n",
       " 'pill-color',\n",
       " 'screw-scratch_neck',\n",
       " 'capsule-good',\n",
       " 'cable-missing_wire',\n",
       " 'cable-good',\n",
       " 'bottle-good',\n",
       " 'zipper-fabric_border',\n",
       " 'bottle-good',\n",
       " 'leather-fold',\n",
       " 'screw-good',\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tile-glue_strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>transistor-bent_lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tile-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>2149</td>\n",
       "      <td>tile-gray_stroke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2150</th>\n",
       "      <td>2150</td>\n",
       "      <td>screw-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>2151</td>\n",
       "      <td>grid-good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2152</td>\n",
       "      <td>cable-poke_insulation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2153</th>\n",
       "      <td>2153</td>\n",
       "      <td>zipper-good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2154 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                  label\n",
       "0         0        tile-glue_strip\n",
       "1         1              grid-good\n",
       "2         2   transistor-bent_lead\n",
       "3         3       tile-gray_stroke\n",
       "4         4              tile-good\n",
       "...     ...                    ...\n",
       "2149   2149       tile-gray_stroke\n",
       "2150   2150             screw-good\n",
       "2151   2151              grid-good\n",
       "2152   2152  cable-poke_insulation\n",
       "2153   2153            zipper-good\n",
       "\n",
       "[2154 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"./open/sample_submission.csv\")\n",
    "\n",
    "submission[\"label\"] = f_result\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"./submission/label_result_add_0502.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
